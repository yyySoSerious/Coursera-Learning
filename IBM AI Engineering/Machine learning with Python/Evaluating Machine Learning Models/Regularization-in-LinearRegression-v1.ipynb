{"cells":[{"cell_type":"markdown","id":"ed75b5e1-7878-4d9e-a9d7-44833a97cf1f","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Regularization in Linear Regression\n","Estimated time needed: **30** minutes\n","    \n","\n","## Objectives\n","\n","\n","After completing this lab you will be able to:\n","\n","* Implement, evaluate, and compare the performance of three regularization techniques for linear regression\n","* Analyze the effect of simple linear regularization when modelling on noisy data with and without outliers\n","* Use Lasso regularization to reduce the number of features for subsequent multiple linear regression modelling\n"]},{"cell_type":"markdown","id":"9dc47125-6734-4b38-89cb-97e5b3298a34","metadata":{},"outputs":[],"source":["Before you begin the lab, ensure the availability of all the required libraries by executing the cell below.\n"]},{"cell_type":"code","id":"86ba6118-4e3e-4710-8674-28ed19c76c5e","metadata":{},"outputs":[],"source":["!pip install numpy\n!pip install pandas\n!pip install scikit-learn\n!pip install matplotlib"]},{"cell_type":"markdown","id":"05c11d25-822d-4fa8-9bb4-a1558904cb86","metadata":{},"outputs":[],"source":["### Import the required libraries\n"]},{"cell_type":"code","id":"23341155-3ddc-43f0-a6c2-33ba3852c750","metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score"]},{"cell_type":"markdown","id":"e357b457-ca07-4a26-87e4-5e12840fc211","metadata":{},"outputs":[],"source":["### Define a function to display evaluation metrics \n","We'll be evaluating several models, so its a good idea to make an evaluation function that includes the common metrics and explaned variances etc.\n"]},{"cell_type":"code","id":"a4243c03-7e32-4eb1-93d3-3b913737809b","metadata":{},"outputs":[],"source":["def regression_results(y_true, y_pred, regr_type):\n\n    # Regression metrics\n    ev = explained_variance_score(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred) \n    mse = mean_squared_error(y_true, y_pred) \n    r2 = r2_score(y_true, y_pred)\n    \n    print('Evaluation metrics for ' + regr_type + ' Linear Regression')\n    print('explained_variance: ',  round(ev,4)) \n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mae,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))\n    print()\n"]},{"cell_type":"markdown","id":"03814d9e-8ec8-4068-9bc5-a8789779017c","metadata":{},"outputs":[],"source":["### Generate a simple dataset with one feature\n","We'll create a simple data set with a linear relationship between the target and a single feature.\n","We'll add some noise to the target to simulate real data.\n","We'll also create a data set from this one that has outliers added. \n","Then you'll compare the three linear regression model performances on the two datasets, with and without sigiifcant outliers added.\n"]},{"cell_type":"code","id":"19d4d478-f10a-4052-858e-939828c0ae5a","metadata":{},"outputs":[],"source":["# Generate synthetic data\nnoise=1\nnp.random.seed(42)\nX = 2 * np.random.rand(1000, 1)\ny = 4 + 3 * X + noise*np.random.randn(1000, 1)  # Linear relationship with some noise\ny_ideal =  4 + 3 * X\n# Specify the portion of the dataset to add outliers (e.g., the last 20%)\ny_outlier = pd.Series(y.reshape(-1).copy())\n\n# Identify indices where the feature variable X is greater than a certain threshold\nthreshold = 1.5  # Example threshold to add outliers for larger feature values\noutlier_indices = np.where(X.flatten() > threshold)[0]\n\n# Add outliers at random locations within the specified portion\nnum_outliers = 5  # Number of outliers to add\nselected_indices = np.random.choice(outlier_indices, num_outliers, replace=False)\n\n# Modify the target values at these indices to create outliers (add significant noise)\ny_outlier[selected_indices] += np.random.uniform(50, 100, num_outliers)"]},{"cell_type":"markdown","id":"094cb199-cb36-4f41-ae7a-ed16ebc4d927","metadata":{},"outputs":[],"source":["### Plot the data with outliers and the ideal fit line\n"]},{"cell_type":"code","id":"2a427402-ebc1-4b3e-a8c9-5eaabc53565f","metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n\n# Scatter plot of the original data with outliers\nplt.scatter(X, y_outlier, alpha=0.4,ec='k', label='Original Data with Outliers')\nplt.plot(X, y_ideal,  linewidth=3, color='g',label='Ideal, noise free data')\n\nplt.xlabel('Feature (X)')\nplt.ylabel('Target (y)')\nplt.title('')\nplt.legend()\nplt.show()\n"]},{"cell_type":"markdown","id":"5dc89130-5b6e-45ec-88e0-1633d8c13e27","metadata":{},"outputs":[],"source":["### Exercise 1. Plot the data without the outliers and the ideal fit line\n"]},{"cell_type":"code","id":"3b67c3fd-20f2-4604-b5e3-f46ec11474e9","metadata":{},"outputs":[],"source":["# Enter your code here\nplt.figure(figsize=(12, 6))\n\n# Scatter plot of the original data with outliers\nplt.scatter(..., y, alpha=0.4,ec='k', label='Original Data without Outliers')\nplt.plot(..., ...,  linewidth=4, color='g',label='Ideal, noise free data')\n\nplt.xlabel('Feature (X)')\nplt.ylabel('Target (y)')\nplt.title('')\nplt.legend()\nplt.show()\n"]},{"cell_type":"markdown","id":"a547165f-c9cd-458d-885e-47de0c9941ee","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","    \n","```python\n","    \n","plt.figure(figsize=(12, 6))\n","\n","# Scatter plot of the original data with outliers\n","plt.scatter(X, y, alpha=0.4,ec='k', label='Original Data without Outliers')\n","plt.plot(X, y_ideal,  linewidth=4, color='g',label='Ideal, noise free data')\n","\n","plt.xlabel('Feature (X)')\n","plt.ylabel('Target (y)')\n","plt.title('')\n","plt.legend()\n","plt.show()\n","```\n"]},{"cell_type":"markdown","id":"0099e236-b797-46d5-bac7-22b9cffb242e","metadata":{},"outputs":[],"source":["### Fit Ordinary, Ridge, and Lasso regression models and use them to make predicitions on the original, outlier-free data\n"]},{"cell_type":"code","id":"1842c2f6-e404-4485-9c14-07dc8f20ffc2","metadata":{},"outputs":[],"source":["# Fit a simple linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(X, y_outlier)\ny_outlier_pred_lin = lin_reg.predict(X)\n\n# Fit a ridge regression model (regularization to control large coefficients)\nridge_reg = Ridge(alpha=1)\nridge_reg.fit(X, y_outlier)\ny_outlier_pred_ridge = ridge_reg.predict(X)\n\n# Fit a lasso regression model (regularization to control large coefficients)\nlasso_reg = Lasso(alpha=.2)\nlasso_reg.fit(X, y_outlier)\ny_outlier_pred_lasso = lasso_reg.predict(X)"]},{"cell_type":"markdown","id":"32ba3d5b-0337-4062-b2a0-d68ebd141bc0","metadata":{},"outputs":[],"source":["### Print the regression results\n"]},{"cell_type":"code","id":"7c4ead72-4174-4df7-baf0-ab64d4ac9ac2","metadata":{},"outputs":[],"source":["regression_results(y, y_outlier_pred_lin, 'Ordinary')\nregression_results(y, y_outlier_pred_ridge, 'Ridge')\nregression_results(y, y_outlier_pred_lasso, 'Lasso')"]},{"cell_type":"markdown","id":"4f7376e4-ef83-4b69-8f8f-dadb94183f32","metadata":{},"outputs":[],"source":["Judging from the low R^2 values, these are poor predictions\n","### Plot the data and the predictions for comparison\n","Let's see how well the predictions match expected ideal values.\n"]},{"cell_type":"code","id":"821afb8f-6104-46b8-9eb1-734eff068aba","metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n\n# Scatter plot of the original data with outliers\nplt.scatter(X, y, alpha=0.4,ec='k', label='Original Data')\n\n# Plot the ideal regression line (noise free data)\nplt.plot(X, y_ideal,  linewidth=2, color='k',label='Ideal, noise free data')\n\n# Plot predictions from the simple linear regression model\nplt.plot(X, y_outlier_pred_lin,  linewidth=5, label='Linear Regression')\n\n# Plot predictions from the ridge regression model\nplt.plot(X, y_outlier_pred_ridge, linestyle='--', linewidth=2, label='Ridge Regression')\n\n# Plot predictions from the lasso regression model\nplt.plot(X, y_outlier_pred_lasso,  linewidth=2, label='Lasso Regression')\n\nplt.xlabel('Feature (X)')\nplt.ylabel('Target (y)')\nplt.title('Comparison of Predictions with Outliers')\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"5b15a3f0-680a-48ec-9454-79ce9967ebac","metadata":{},"outputs":[],"source":["As you can see, ordinary linear and ridge resgression performed similarly, while Lasso outperformed both. \n","\n","Although the intercept is off for the Lasso fit line, it's slope is much closer to the ideal than the other fit lines.\n","\n","All three lines were 'pulled up' by the outliers (not plotted here - compare to the plot above where the outliers are shown), with Lasso dampening that effect.\n"]},{"cell_type":"markdown","id":"aff7386f-5d97-47d2-a44c-62bc8297473e","metadata":{},"outputs":[],"source":["### Exercise 2. Build the models and the prediction plots from the same data, excluding the outliers\n"]},{"cell_type":"code","id":"69b97d81-a5d5-4d38-a99e-4799646cc8db","metadata":{},"outputs":[],"source":["# Enter your code here:\n\n# Fit a simple linear regression model\nlin_reg = LinearRegression()\nlin_reg.fit(..., ...)\ny_pred_lin = lin_reg.predict(...)\n\n# Fit a ridge regression model (regularization to control large coefficients)\nridge_reg = Ridge(alpha=1)\nridge_reg.fit(..., ...)\ny_pred_ridge = ridge_reg.predict(...)\n\n# Fit a lasso regression model (regularization to control large coefficients)\nlasso_reg = Lasso(alpha=0.2)\nlasso_reg.fit(..., ...)\ny_pred_lasso = lasso_reg.predict(...)\n\n# Print the regression results\nregression_results(y, ..., 'Ordinary')\nregression_results(y, ..., 'Ridge')\nregression_results(y, ..., 'Lasso')\n\n\n# Plot the data and the predictions\nplt.figure(figsize=(12, 8))\n\n# # Scatter plot of the original data\nplt.scatter(X, y, alpha=0.4, ec='k', label='Original Data')\n\n# Plot the ideal regression line (noise free data)\nplt.plot(X, ...,  linewidth=2, color='k',label='Ideal, noise free data')\n\n# Plot predictions from the simple linear regression model\nplt.plot(X, ...,  linewidth=5, label='Linear Regression')\n\n# Plot predictions from the ridge regression model\nplt.plot(X, ..., linestyle='--',linewidth=2, label='Ridge Regression')\n\n# Plot predictions from the lasso regression model\nplt.plot(X, ...,  linewidth=2, label='Lasso Regression')\n\nplt.xlabel('Feature (X)')\nplt.ylabel('Target (y)')\n\nplt.title('Comparison of predictions with no outliers')\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"6b4dba06-994e-4ca1-8902-9dfb8923381e","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","    \n","```python\n","   \n","# Fit a simple linear regression model\n","lin_reg = LinearRegression()\n","lin_reg.fit(X, y)\n","y_pred_lin = lin_reg.predict(X)\n","\n","# Fit a ridge regression model (regularization to control large coefficients)\n","ridge_reg = Ridge(alpha=1)\n","ridge_reg.fit(X, y)\n","y_pred_ridge = ridge_reg.predict(X)\n","\n","# Fit a lasso regression model (regularization to control large coefficients)\n","lasso_reg = Lasso(alpha=0.2)\n","lasso_reg.fit(X, y)\n","y_pred_lasso = lasso_reg.predict(X)\n","\n","# Print the regression results\n","regression_results(y, y_pred_lin, 'Ordinary')\n","regression_results(y, y_pred_ridge, 'Ridge')\n","regression_results(y, y_pred_lasso, 'Lasso')\n","\n","\n","# Plot the data and the predictions\n","plt.figure(figsize=(12, 8))\n","\n","# # Scatter plot of the original data\n","plt.scatter(X, y, alpha=0.4,ec='k', label='Original Data')\n","\n","# Plot the ideal regression line (noise free data)\n","plt.plot(X, y_ideal,  linewidth=2, color='k',label='Ideal, noise free data')\n","\n","# Plot predictions from the simple linear regression model\n","plt.plot(X, y_pred_lin,  linewidth=5, label='Linear Regression')\n","\n","# Plot predictions from the ridge regression model\n","plt.plot(X, y_pred_ridge, linestyle='--',linewidth=2, label='Ridge Regression')\n","\n","# Plot predictions from the lasso regression model\n","plt.plot(X, y_pred_lasso,  linewidth=2, label='Lasso Regression')\n","\n","plt.xlabel('Feature (X)')\n","plt.ylabel('Target (y)')\n","# plt.ylim((0,20))\n","plt.title('Comparison of predictions with no outliers')\n","plt.legend()\n","plt.show()\n","```\n"]},{"cell_type":"markdown","id":"263fb815-a8f4-4788-8ebd-d479ece9a795","metadata":{},"outputs":[],"source":["## Multiple regression regularization and Lasso feature selction\n","Now that you have explored regularization for simple, one-dimensional regression, let's take a deeper look at a multiple regression modelling scenario.\n","You'll again compare performances of the three inear regression methods and then use the Lasso result to select important features to use in another modelling pass.\n"]},{"cell_type":"markdown","id":"492e7471-8110-4166-bce0-a4bc7d115198","metadata":{},"outputs":[],"source":["### Create a high dimensional synthetic dataset with a small number of informative features using `make_regression`\n","\n","The output of `make_regression` is generated by applying a random linear regression model based on `n_informative` nonzero regressors and some adjustable gaussian noise.\n","Along with the features and the target vairable, the regression model coefficients can also be obtained from the output.\n","\n","We'll split the data into training and testing sets, and also split the ideal predictions, which is a line based on the linear regression model.\n"]},{"cell_type":"code","id":"ce5b7d45-3888-43e3-928b-6a06313f7c30","metadata":{},"outputs":[],"source":["from sklearn.datasets import make_regression\n\nX, y, ideal_coef = make_regression(n_samples=100, n_features=100, n_informative=10, noise=10, random_state=42, coef=True)\n\n# Get the ideal predictions based on the informative coefficients used in the regression model\nideal_predictions = X @ ideal_coef\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test, ideal_train, ideal_test = train_test_split(X, y, ideal_predictions, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","id":"524d9ef4-963b-4979-a25b-9a99907f48d7","metadata":{},"outputs":[],"source":["### Initialize and fit the linear regression models and use them to predict the target.\n"]},{"cell_type":"code","id":"2e4b93be-1273-40e3-a90b-433443a4aeba","metadata":{},"outputs":[],"source":["lasso = Lasso(alpha=0.1)\nridge = Ridge(alpha=1.0)\nlinear = LinearRegression()\n\n# Fit the models\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\nlinear.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_linear = linear.predict(X_test)\ny_pred_ridge = ridge.predict(X_test)\ny_pred_lasso = lasso.predict(X_test)"]},{"cell_type":"markdown","id":"35afcde6-58e4-4271-8040-f0a224af6d00","metadata":{},"outputs":[],"source":["### Print the regression results\n"]},{"cell_type":"code","id":"809ffeae-809e-4fec-92b2-c818b726f9c4","metadata":{},"outputs":[],"source":["regression_results(y_test, y_pred_linear, 'Ordinary')\nregression_results(y_test, y_pred_ridge, 'Ridge')\nregression_results(y_test, y_pred_lasso, 'Lasso')"]},{"cell_type":"markdown","id":"df17740f-e77d-4a97-9011-70e7977d1627","metadata":{},"outputs":[],"source":["### Exercise 3. Do you have some immediate thoughts on these performance metrics?\n"]},{"cell_type":"code","id":"c3e11624-aaa6-4e9d-ad3b-fda4ea50bc71","metadata":{},"outputs":[],"source":["#### Enter your thoughts here (you can convert the cell to markdown)\n"]},{"cell_type":"markdown","id":"033648bb-939d-4dc7-b5a1-62a1b9ae59c8","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","    \n","The results for ordinary and ridge regession are poor. \n","\n","Explained variances are under 50%, and R^2 is very low.\n","\n","However, ther result for Lasso is stellar.\n"]},{"cell_type":"markdown","id":"40edfa47-09ec-4e7e-a015-950bf43ff87b","metadata":{},"outputs":[],"source":["### Plot the predictions vs actuals \n","Let's get some more insight into the perfomance of these models.\n"]},{"cell_type":"code","id":"698a7400-f5cd-460f-b4da-dfba2ac44f7d","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharey=True)\n\naxes[0,0].scatter(y_test, y_pred_linear, color=\"red\", label=\"Linear\")\naxes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\naxes[0,0].set_title(\"Linear Regression\")\naxes[0,0].set_xlabel(\"Actual\",)\naxes[0,0].set_ylabel(\"Predicted\",)\n\naxes[0,2].scatter(y_test, y_pred_lasso, color=\"blue\", label=\"Lasso\")\naxes[0,2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\naxes[0,2].set_title(\"Lasso Regression\",)\naxes[0,2].set_xlabel(\"Actual\",)\n\naxes[0,1].scatter(y_test, y_pred_ridge, color=\"green\", label=\"Ridge\")\naxes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\naxes[0,1].set_title(\"Ridge Regression\",)\naxes[0,1].set_xlabel(\"Actual\",)\n\naxes[0,2].scatter(y_test, y_pred_lasso, color=\"blue\", label=\"Lasso\")\naxes[0,2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\naxes[0,2].set_title(\"Lasso Regression\",)\naxes[0,2].set_xlabel(\"Actual\",)\n\n\n# Line plots for predictions compared to actual and ideal predictions\naxes[1,0].plot(y_test, label=\"Actual\", lw=2)\naxes[1,0].plot(y_pred_linear, '--', lw=2, color='red', label=\"Linear\")\naxes[1,0].set_title(\"Linear vs Ideal\",)\naxes[1,0].legend()\n \naxes[1,1].plot(y_test, label=\"Actual\", lw=2)\n# axes[1,1].plot(ideal_test, '--', label=\"Ideal\", lw=2, color=\"purple\")\naxes[1,1].plot(y_pred_ridge, '--', lw=2, color='green', label=\"Ridge\")\naxes[1,1].set_title(\"Ridge vs Ideal\",)\naxes[1,1].legend()\n \naxes[1,2].plot(y_test, label=\"Actual\", lw=2)\naxes[1,2].plot(y_pred_lasso, '--', lw=2, color='blue', label=\"Lasso\")\naxes[1,2].set_title(\"Lasso vs Ideal\",)\naxes[1,2].legend()\n \nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"8b3c4fc6-0320-4834-b06b-b706af307a1c","metadata":{},"outputs":[],"source":[" You can see visually how much more the predictions deviate from the ideal target values for the ordinary and ridge regression results than for the Lasso result.\n"]},{"cell_type":"markdown","id":"093ad525-10c5-4bdc-9ca2-9dca7efbad77","metadata":{},"outputs":[],"source":["### Model coefficients\n","Let's take a look at the coefficients for each model fit\n"]},{"cell_type":"code","id":"1a41a9d5-e584-4e6b-8f59-f56fdad8f574","metadata":{},"outputs":[],"source":["# Model coefficients\nlinear_coeff = linear.coef_\nridge_coeff = ridge.coef_\nlasso_coeff = lasso.coef_\n\n\n# Plot the coefficients\nx_axis = np.arange(len(linear_coeff))\nx_labels = np.arange(min(x_axis),max(x_axis),10)\nplt.figure(figsize=(12, 6))\n\nplt.scatter(x_axis, ideal_coef,  label='Ideal', color='blue', ec='k', alpha=0.4)\nplt.bar(x_axis - 0.25, linear_coeff, width=0.25, label='Linear Regression', color='blue')\nplt.bar(x_axis, ridge_coeff, width=0.25, label='Ridge Regression', color='green')\nplt.bar(x_axis + 0.25, lasso_coeff, width=0.25, label='Lasso Regression', color='red')\n\nplt.xlabel('Feature Index')\nplt.ylabel('Coefficient Value')\nplt.title('Comparison of Model Coefficients')\nplt.xticks(x_labels)\nplt.legend()\nplt.show()\n\n\n# Plot the coefficient residuals\nx_axis = np.arange(len(linear_coeff))\n\nplt.figure(figsize=(12, 6))\n\nplt.bar(x_axis - 0.25, ideal_coef - linear_coeff, width=0.25, label='Linear Regression', color='blue')\nplt.bar(x_axis, ideal_coef - ridge_coeff, width=0.25, label='Ridge Regression', color='green')\n# plt.bar(x_axis + 0.25, ideal_coef - lasso_coeff, width=0.25, label='Lasso Regression', color='red')\nplt.plot(x_axis, ideal_coef - lasso_coeff, label='Lasso Regression', color='red')\n\nplt.xlabel('Feature Index')\nplt.ylabel('Coefficient Value')\nplt.title('Comparison of Model Coefficient Residuals')\nplt.xticks(x_labels)\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"177e30c7-45e9-4804-9b42-1c522070fce9","metadata":{},"outputs":[],"source":["You can see from the first plot how much closer the Lasso coefficients are to the ideal coefficients than for the other two models.\n","An easier way to visualize the difference is to look at the residual errors, as in the second plot. Clearly the Lasso coefficient residuals are much closer to zero than the others.\n"]},{"cell_type":"markdown","id":"7475ea39-a7a8-4485-a04f-0f5f801403e5","metadata":{},"outputs":[],"source":["### Use Lasso to select the most important features and compare the three different linear regression models again on the resulting data.\n"]},{"cell_type":"markdown","id":"b94b4cfc-5707-4d6d-803f-2ea625b2838c","metadata":{},"outputs":[],"source":["#### Part 1. Choose a threshold value to select features based on the Lasso model coefficients\n","Use the coefficient residual plot to select a reasonable threshold value, beyond which the Lasso coefficients are significant.\n","To find a good threshold you can visually inspect the residuals plot and choose a value that distinguishes coefficients with larger than normal residuals.\n","\n","Create a dataframe to compare the Lasso coefficients with the ideal coefficients, for the features selected with the Lasso coefficent threshold.\n","\n","Include a boolean column that indicates whether the feature was selected as being important by the Lasso coefficent thresholding.\n","\n","Display two filtered versions of the resulting dataframe:\n","\n","1. Only those features identified as important by Lasso\n","\n","2. Only the nonzero ideal coefficient indices\n","\n","How did we do?\n"]},{"cell_type":"code","id":"33ef6442-0016-4088-b39e-3c56f11bd579","metadata":{},"outputs":[],"source":["threshold = 5 # selected by inspection of residuals plot\n\n# Create a dataframe containing the Lasso model and ideal coefficients\nfeature_importance_df = pd.DataFrame({\n    'Lasso Coefficient': lasso_coeff,\n    'Ideal Coefficient': ideal_coef\n})\n\n# Mark the selected features\nfeature_importance_df['Feature Selected'] = feature_importance_df['Lasso Coefficient'].abs() > threshold\n\n\nprint(\"Features Identified as Important by Lasso:\")\ndisplay(feature_importance_df[feature_importance_df['Feature Selected']])\n\nprint(\"\\nNonzero Ideal Coefficient Indices\")\ndisplay(feature_importance_df[feature_importance_df['Ideal Coefficient']>0])\n\n"]},{"cell_type":"markdown","id":"2f173b00-8d64-469e-99dc-ab6d749b1ab5","metadata":{},"outputs":[],"source":["The result is very good. We managed to correctly identify 9 out of the 10 important features.\n","#### Part 2. Use the threshold to select the most important features for use in modelling.\n","Also split your data into train, test sets, including the ideal targets. How many features did you end up selecting?\n"]},{"cell_type":"code","id":"d4f6c3a0-c615-4c89-842a-aeb878a21b64","metadata":{},"outputs":[],"source":["important_features = feature_importance_df[feature_importance_df['Feature Selected']].index"]},{"cell_type":"code","id":"24d985c4-f6fa-426d-9dfe-ef437d05dd3f","metadata":{},"outputs":[],"source":["# Filter features\nX_filtered = X[:, important_features]\nprint(\"Shape of the filtered feature set:\", X_filtered.shape)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test, ideal_train, ideal_test = train_test_split(X_filtered, y, ideal_predictions, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","id":"52fc8041-eae6-4aab-9e5e-3f66ba97235d","metadata":{},"outputs":[],"source":["#### Part 3. Fit and apply the three models to the selected features\n"]},{"cell_type":"code","id":"85e622d9-5fed-4aa9-a265-5b9af5ee890b","metadata":{},"outputs":[],"source":["# Initialize the models\nlasso = Lasso(alpha=0.1)\nridge = Ridge(alpha=1.0)\nlinear = LinearRegression()\n\n# Fit the models\nlasso.fit(X_train, y_train)\nridge.fit(X_train, y_train)\nlinear.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred_linear = linear.predict(X_test)\ny_pred_ridge = ridge.predict(X_test)\ny_pred_lasso = lasso.predict(X_test)"]},{"cell_type":"markdown","id":"39a04a32-2962-4583-a4e6-a51e4ed781ea","metadata":{},"outputs":[],"source":["### Exercise 4. Print the regression performance results\n"]},{"cell_type":"code","id":"ca31fc87-7561-47ac-903d-378cfbd57600","metadata":{},"outputs":[],"source":["# Enter your code here:\nregression_results(y_test, ..., 'Ordinary')\nregression_results(y_test, ..., 'Ridge')\nregression_results(y_test, ..., 'Lasso')"]},{"cell_type":"markdown","id":"40d63f47-80c5-4e50-ad30-00e62f9c5441","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","\n","```python\n","regression_results(y_test, y_pred_linear, 'Ordinary')\n","regression_results(y_test, y_pred_ridge, 'Ridge')\n","regression_results(y_test, y_pred_lasso, 'Lasso')\n","\n","\n","\n","```\n"]},{"cell_type":"markdown","id":"57a20084-b07d-4d78-af71-53cacf50379f","metadata":{},"outputs":[],"source":["### Let's compare the results to see the effect of feature selection using Lasso\n","\n","Here are the previous results:\n","\n","Evaluation metrics for Ordinary Linear Regression\n","| Metric | Score |\n","|:---|:---|\n","|explained_variance: |0.4346 |\n","|r2   |0.4012 |\n","|MAE  | 77.7479 |\n","|MSE  | 9855.428 |\n","|RMSE   |99.2745 |\n","\n","Evaluation metrics for Ridge Linear Regression\n","| Metric | score |\n","|:---|:---|\n","|explained_variance: | 0.4446|\n","|r2  |0.4079|\n","|MAE | 76.9622|\n","|MSE  |9744.4693|\n","|RMSE  |98.7141|\n","\n","Evaluation metrics for Lasso Linear Regression\n","| Metric | score |\n","|:---|:---|\n","|explained_variance: | 0.9815|\n","|r2  |0.9815|\n","|MAE | 13.8855|\n","|MSE  |304.644|\n","|RMSE | 17.4541|\n","\n","The new results are vastly improved for ordinary and Ridge regression, and slightly improved for Lasso, supporting the idea that **Lasso regression can be very beneficial when used as a feature selector.**\n"]},{"cell_type":"markdown","id":"d4a13478-cea6-44ba-bdfa-8f87576fc56e","metadata":{},"outputs":[],"source":["### Exercise 5. Regenerate the same plots as before and compare the results\n"]},{"cell_type":"code","id":"885869b9-9ec8-45b9-be27-1f1d5aa446d9","metadata":{},"outputs":[],"source":["# Enter your code here\n# Plot the predictions vs actuals\n"]},{"cell_type":"markdown","id":"e7182d45-a19e-4c39-8fef-d98ae3d73770","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","\n","```python\n","\n","# Plot the predictions vs actuals \n","fig, axes = plt.subplots(2, 3, figsize=(18, 10), sharey=True)\n","\n","axes[0,0].scatter(y_test, y_pred_linear, color=\"red\", label=\"Linear\")\n","axes[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n","axes[0,0].set_title(\"Linear Regression\",)\n","axes[0,0].set_xlabel(\"Actual\",)\n","\n","axes[0,2].scatter(y_test, y_pred_lasso, color=\"blue\", label=\"Lasso\")\n","axes[0,2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n","axes[0,2].set_title(\"Lasso Regression\",)\n","axes[0,2].set_xlabel(\"Actual\",)\n","axes[0,2].set_ylabel(\"Predicted\",)\n","\n","axes[0,1].scatter(y_test, y_pred_ridge, color=\"green\", label=\"Ridge\")\n","axes[0,1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n","axes[0,1].set_title(\"Ridge Regression\",)\n","axes[0,1].set_xlabel(\"Actual\",)\n","\n","axes[0,2].scatter(y_test, y_pred_lasso, color=\"blue\", label=\"Lasso\")\n","axes[0,2].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--')\n","axes[0,2].set_title(\"Lasso Regression\",)\n","axes[0,2].set_xlabel(\"Actual\",)\n","axes[0,2].set_ylabel(\"Predicted\",)\n","\n","# Line plots for predictions compared to actual and ideal predictions\n","axes[1,0].plot(y_test, label=\"Actual\", lw=2)\n","axes[1,0].plot(y_pred_linear, '--', lw=2, color='red', label=\"Linear\")\n","axes[1,0].set_title(\"Linear vs Ideal\",)\n","axes[1,0].legend()\n"," \n","axes[1,1].plot(y_test, label=\"Actual\", lw=2)\n","axes[1,1].plot(y_pred_ridge, '--', lw=2, color='green', label=\"Ridge\")\n","axes[1,1].set_title(\"Ridge vs Ideal\",)\n","axes[1,1].legend()\n"," \n","axes[1,2].plot(y_test, label=\"Actual\", lw=2)\n","axes[1,2].plot(y_pred_lasso, '--', lw=2, color='blue', label=\"Lasso\")\n","axes[1,2].set_title(\"Lasso vs Ideal\",)\n","axes[1,2].legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n","```\n","\n","<details><summary>Click here for comparison</summary>\n","As you can see, with the assistance of Lasso feature selection, all three models have performed extremely well! \n"]},{"cell_type":"markdown","id":"93e86896-a811-41c7-bcd2-b96596e77b7a","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson!\n"," \n"," \n","## Author\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n","\n","\n","### Other Contributors\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","\n","<!-- \n","## Changelog\n","\n","| Date | Version | Changed by | Change Description |\n","|:------------|:------|:------------------|:---------------------------------------|\n","| 2024-11-05 | 1.0  | Jeff Grossman    | Create content |\n"," -->\n","\n","\n","## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"5171ffc79f06dae955c4d2ffc092a5f06eaaa18a801aac84bd7a7e3ec2e2b836"},"nbformat":4,"nbformat_minor":4}