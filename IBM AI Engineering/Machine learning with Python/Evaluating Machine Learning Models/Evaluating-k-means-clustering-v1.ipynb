{"cells":[{"cell_type":"markdown","id":"4ba63653-a6a1-4e4f-8550-32c2de3fe201","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Evaluating k-means clustering\n","Estimated time needed: **30** minutes\n","    \n","\n","## Objectives\n","\n","After completing this lab you will be able to:\n","\n","* Implement and evaluate the performance of k-means clustering on synthetic data\n","* Interpret various evaluation metrics and visualizations\n","* Compare clustering results against known classes unsing synthetic data\n"]},{"cell_type":"markdown","id":"f5540e0f-9753-4755-88b2-8abd6e04b7bc","metadata":{},"outputs":[],"source":["## Introduction\n","In this lab you will:\n","- Generate synthetic data for running targeted experiments using scikit-learn\n","- Create k-means models and evaluate their comparative performance\n","- Investigate evaluation metrics and techniques for assessing clustering results \n","\n","Your goal in this lab is primarily for you to gain some intuition around the subjective problem of finding good clustering solutions.\n"]},{"cell_type":"markdown","id":"4bf85129-e6c6-4cdc-9dc1-1cc3c86cbee2","metadata":{},"outputs":[],"source":["## Import the required libraries\n"]},{"cell_type":"code","id":"07b82d39-77b1-4403-81ba-56791da4ed22","metadata":{},"outputs":[],"source":["!pip install numpy==2.2.0\n!pip install pandas==2.2.3\n!pip install scikit-learn==1.6.0\n!pip install matplotlib==3.9.3\n!pip install scipy==1.14.1"]},{"cell_type":"code","id":"fb953d23-6634-4bda-9ed6-a47fea0a52f7","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import silhouette_score, silhouette_samples, davies_bouldin_score\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom matplotlib.patches import Patch\nfrom matplotlib import cm"]},{"cell_type":"markdown","id":"bc518f8a-a2d6-4bb8-adcb-f6c83f248167","metadata":{},"outputs":[],"source":["### Clustering evaluation function\n","We'll define a function for evaluating the clustering models we'll be building.\n","We'll include silhouette scores and the Davies-Bouldin index, plus generate a plot displaying the silhouette scores\n"]},{"cell_type":"code","id":"3dc03ca5-f100-4d77-b42a-8a51157a5ebc","metadata":{},"outputs":[],"source":["def evaluate_clustering(X, labels, n_clusters, ax=None, title_suffix=''):\n    \"\"\"\n    Evaluate a clustering model using silhouette scores and the Davies-Bouldin index.\n    \n    Parameters:\n    X (ndarray): Feature matrix.\n    labels (array-like): Cluster labels assigned to each sample.\n    n_clusters (int): The number of clusters in the model.\n    ax: The subplot axes to plot on.\n    title_suffix (str): Optional suffix for plot titlec\n    \n    Returns:\n    None: Displays silhoutte scores and a silhouette plot.\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()  # Get the current axis if none is provided\n    \n    # Calculate silhouette scores\n    silhouette_avg = silhouette_score(X, labels)\n    sample_silhouette_values = silhouette_samples(X, labels)\n\n    # Plot silhouette analysis on the provided axis\n    unique_labels = np.unique(labels)\n    colormap = cm.tab10\n    color_dict = {label: colormap(float(label) / n_clusters) for label in unique_labels}\n    y_lower = 10\n    for i in unique_labels:\n        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n        ith_cluster_silhouette_values.sort()\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n        color = color_dict[i]\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                         0, ith_cluster_silhouette_values,\n                         facecolor=color, edgecolor=color, alpha=0.7)\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n        y_lower = y_upper + 10\n    \n    ax.set_title(f'Silhouette Score for {title_suffix} \\n' + \n                 f'Average Silhouette: {silhouette_avg:.2f}')\n    ax.set_xlabel('Silhouette Coefficient')\n    ax.set_ylabel('Cluster')\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n    ax.set_xlim([-0.25, 1])  # Set the x-axis range to [0, 1]\n\n    ax.set_yticks([])\n"]},{"cell_type":"markdown","id":"b63628d5-4b14-4a22-ae26-789f518b8224","metadata":{},"outputs":[],"source":["## Create synthetic data with four blobs to experiment with k-means clustering\n","Here we'll make some synthetic data consisting of slightly overlapping blobs, then run and evaluate k-means with k=4 clusters.\n"]},{"cell_type":"code","id":"a44403ea-b925-4dbd-b9cc-7af3ca5a279d","metadata":{},"outputs":[],"source":["X, y = make_blobs(n_samples=500, n_features=2, centers=4, cluster_std=[1.0, 3, 5, 2], random_state=42)\n\n# Apply KMeans clustering\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\ny_kmeans = kmeans.fit_predict(X)\n\ncolormap = cm.tab10\n\n# Plot the blobs\nplt.figure(figsize=(18, 6))\nplt.subplot(1, 3, 1)\nplt.scatter(X[:, 0], X[:, 1], s=50, alpha=0.6, edgecolor='k')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', alpha=0.9, label='Centroids')\nplt.title(f'Synthetic Blobs with {n_clusters} Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\n\n# Plot the clustering result\n# Create colors based on the predicted labels\ncolors = colormap(y_kmeans.astype(float) / n_clusters)\n\nplt.subplot(1, 3, 2)\nplt.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.6, edgecolor='k')\n\n# Label the clusters\ncenters = kmeans.cluster_centers_\n# Draw white circles at cluster centers\nplt.scatter(\n    centers[:, 0],\n    centers[:, 1],\n    marker=\"o\",\n    c=\"white\",\n    alpha=1,\n    s=200,\n    edgecolor=\"k\",\n    label='Centroids'\n)\n# Label the custer number\nfor i, c in enumerate(centers):\n    plt.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\nplt.title(f'KMeans Clustering with {n_clusters} Clusters')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\n\n# Evaluate the clustering\nplt.subplot(1, 3, 3)\nevaluate_clustering(X, y_kmeans, n_clusters, title_suffix=' k-Means Clustering')\nplt.show()"]},{"cell_type":"markdown","id":"82cc7c60-aac6-4711-b549-eb4ec60fd09d","metadata":{},"outputs":[],"source":["\n","Each point in a silhouette plot has a silhouette score ranging from -1 to 1. A high silhouette score indicates the data point is much more similar to its own cluster than its neighboring clusters. A score near 0 implies the point is at or near the decision boundary between two clusters. A negative score means the point might have been assigned to the wrong cluster. We'll take a closer look at the silhoutte plot later.\n","\n","The clustering result is quite plausible, particularly because we already know that there are four blobs in the synthetic data. Practically hoeever, we would not have such information.\n"]},{"cell_type":"markdown","id":"b50580f5-3ef2-41e9-8327-e2009a640992","metadata":{},"outputs":[],"source":["## Cluster Stability\n","How do the results change when K-means is run using different initial centroid seeds?\n","\n","To assess stability, we can measure *inertia* and repeatedly vary the random initialization of cluster centers to observe the impact of having different intitial centroid locations.\n","\n","Inertia measures the compactness of clusters in K-means. It is defined as the sum of squared distances between each data point and its cluster centroid. Lower inertia values indicate more compact clusters and a potentially better clustering outcome. However, inertia tends to decrease as the number of clusters increases, so it's important to interpret it alongside other metrics.\n","\n","To evaluate the stability of clustering, running k-means multiple times with different initial centroids by not fixing the random state helps determine if the algorithm consistently produces similar cluster assignments and inertia scores. Consistent inertia across runs suggests a stable solution that is less dependent on initial centroid positions.\n"]},{"cell_type":"code","id":"21e5617c-2f94-41da-8d91-e4ccccf3b123","metadata":{},"outputs":[],"source":["# Number of runs for k-means with different random states\nn_runs = 8\ninertia_values = []\n\n# Calculate number of rows and columns needed for subplots\nn_cols = 2 # Number of columns\nn_rows = -(-n_runs // n_cols) # Ceil division to determine rows\nplt.figure(figsize=(16, 16)) # Adjust the figure size for better visualization\n\n# Run K-Means multiple times with different random states\nfor i in range(n_runs):\nkmeans = KMeans(n_clusters=4, random_state=None) # Use the default `n_init`\nkmeans.fit(X)\ninertia_values.append(kmeans.inertia_)\n\n# Plot the clustering result\nplt.subplot(n_rows, n_cols, i + 1)\nplt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='tab10', alpha=0.6, edgecolor='k')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='x', label='Centroids')\nplt.title(f'K-Means Run {i + 1}')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend(loc='upper right', fontsize='small')\nplt.tight_layout()\nplt.show()\n\n# Print inertia values\nfor i, inertia in enumerate(inertia_values, start=1):\nprint(f'Run {i}: Inertia={inertia:.2f}')"]},{"cell_type":"markdown","id":"dbb349be-ae1c-4e88-9eb5-20c1bb4c214e","metadata":{},"outputs":[],"source":["### Exercise 1. What can you say about this result?\n"]},{"cell_type":"markdown","id":"ba605bce-6292-4629-a1a4-27a90fc1e49c","metadata":{},"outputs":[],"source":["<details><summary>Click here for some observations</summary>\n","As demonstrated by the clustering results, the cluster assignments vary between runs when using different initial centroid seeds. Additionally, the inertia values show inconsistency, indicating that the clustering process is sensitive to the initial placement of centroids. This inertial inconsistency implies an less reliable result. \n"]},{"cell_type":"markdown","id":"7fc11c65-7a6b-4f64-af37-8323e71eb2a2","metadata":{},"outputs":[],"source":["## Number of clusters\n","How do performance metrics change as the number of clusters increases?\n","\n","Can this analysis guide you in determining the optimal number of clusters?\n","\n","To explore this, we can examine how varying the value of K affects key metrics such as inertia, the Davies-Bouldin index, and silhouette scores. By plotting these scores as a function of K, we can analyze the results and potentially gain insights into the optimal number of clusters for our data.\n"]},{"cell_type":"code","id":"fde0b5f5-ed76-4d18-89ca-f3339bae5fd1","metadata":{},"outputs":[],"source":["# Range of k values to test\nk_values = range(2, 11)\n\n# Store performance metrics\ninertia_values = []\nsilhouette_scores = []\ndavies_bouldin_indices = []\n\nfor k in k_values:\n    kmeans = KMeans(n_clusters=k, random_state=42)\n    y_kmeans = kmeans.fit_predict(X)\n    \n    # Calculate and store metrics\n    inertia_values.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(X, y_kmeans))\n    davies_bouldin_indices.append(davies_bouldin_score(X, y_kmeans))\n\n# Plot the inertia values (Elbow Method)\nplt.figure(figsize=(18, 6))\n\nplt.subplot(1, 3, 1)\nplt.plot(k_values, inertia_values, marker='o')\nplt.title('Elbow Method: Inertia vs. k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia')\n\n# Plot silhouette scores\nplt.subplot(1, 3, 2)\nplt.plot(k_values, silhouette_scores, marker='o')\nplt.title('Silhouette Score vs. k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\n\n# Plot Davies-Bouldin Index\nplt.subplot(1, 3, 3)\nplt.plot(k_values, davies_bouldin_indices, marker='o')\nplt.title('Davies-Bouldin Index vs. k')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Davies-Bouldin Index')\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"ff46311f-571a-4827-871a-c7cf378df41b","metadata":{},"outputs":[],"source":["### Exercise 2. What do these results suggest to you?\n"]},{"cell_type":"markdown","id":"a4557a22-df08-4ad9-96fa-f3ae3c0c8307","metadata":{},"outputs":[],"source":["<details><summary>Click here for some observations</summary>\n","The first plot is often used in the 'elbow method,' where the ideal value of k is chosen near the point where the curve starts to level off. Since inertia decreases as the number of clusters increases, it’s important to find the balance where adding more clusters provides diminishing returns in reducing inertia.\n","\n","The inertia plot points to an optimal cluster number around 3 or 4. The silhouette score shows a clear peak at k = 3, while the Davies-Bouldin index reaches its lowest values between k = 3 and k = 4.\n","\n","Overall, these metrics suggest that three clusters may be optimal, although we know that the true number of clusters in this case is actually four.\n"]},{"cell_type":"markdown","id":"56b2c3ce-7feb-430c-bde2-02b4c6faf666","metadata":{},"outputs":[],"source":["### Exercise 3. Plot the blobs and the clustering results for k = 3, 4, and 5\n"]},{"cell_type":"code","id":"9ea7beed-b62f-43ee-ba16-75781505ea42","metadata":{},"outputs":[],"source":["# Enter your code here\n\nplt.figure(figsize=(18, 12))\ncolormap = cm.tab10\n\nfor i, k in enumerate([2, 3, 4]):\n    # Fit KMeans and predict the labels\n    kmeans = KMeans(n_clusters=..., random_state=42)\n    y_kmeans = kmeans.fit_predict(...)\n\n    # Create colors based on the predicted labels\n    colors = colormap(y_kmeans.astype(float) / k)\n\n    # Scatter plot for each k in the first row (1, 2, 3)\n    ax1 = plt.subplot(2, 3, i + 1)\n    ax1.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.6, edgecolor='k')\n    ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans....[:, 1], c='red', s=200, marker='X', label='Centroids')\n\n    # Labeling the clusters\n    centers = kmeans.cluster_centers_\n    # Draw white circles at cluster centers\n    plt.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n        label='Centroids'\n    )\n\n    for i_, c in enumerate(centers):\n        plt.scatter(c[0], c[1], marker=\"$%d$\" % i_, alpha=1, s=50, edgecolor=\"k\")\n\n    ax1.set_title(f'K-Means Clustering Results, k={k}')\n    ax1.set_xlabel('Feature 1')\n    ax1.set_ylabel('Feature 2')\n    ax1.legend()\n\n    # Silhouette plot for each k in the second row (4, 5, 6)\n    ax2 = plt.subplot(2, 3, i + 4)\n    evaluate_clustering(X, y_kmeans, k, ax=ax2, title_suffix=f' k={k}')\n\nplt.tight_layout()  # Adjust spacing between plots\nplt.show()\n"]},{"cell_type":"markdown","id":"f6342cf3-41f3-4303-85c1-364cd4758f5d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","plt.figure(figsize=(18, 12))\n","colormap = cm.tab10  # Define the colormap\n","\n","for i, k in enumerate([2, 3, 4]):\n","    # Fit KMeans and predict labels\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    y_kmeans = kmeans.fit_predict(X)\n","\n","    # Create colors based on the predicted labels\n","    colors = colormap(y_kmeans.astype(float) / k)\n","\n","    # Scatter plot for each k in the first row (1, 2, 3)\n","    ax1 = plt.subplot(2, 3, i + 1)\n","    ax1.scatter(X[:, 0], X[:, 1], c=colors, s=50, alpha=0.6, edgecolor='k')\n","    ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', s=200, marker='X', label='Centroids')\n","\n","    # Labeling the clusters\n","    centers = kmeans.cluster_centers_\n","    # Draw white circles at cluster centers\n","    plt.scatter(\n","        centers[:, 0],\n","        centers[:, 1],\n","        marker=\"o\",\n","        c=\"white\",\n","        alpha=1,\n","        s=200,\n","        edgecolor=\"k\",\n","        label='Centroids'\n","    )\n","\n","    for i_, c in enumerate(centers):\n","        plt.scatter(c[0], c[1], marker=\"$%d$\" % i_, alpha=1, s=50, edgecolor=\"k\")\n","\n","    ax1.set_title(f'K-Means Clustering Results, k={k}')\n","    ax1.set_xlabel('Feature 1')\n","    ax1.set_ylabel('Feature 2')\n","    ax1.legend()\n","\n","    # Silhouette plot for each k in the second row (4, 5, 6)\n","    ax2 = plt.subplot(2, 3, i + 4)\n","    evaluate_clustering(X, y_kmeans, k, ax=ax2, title_suffix=f' k={k}')\n","\n","plt.tight_layout()  # Adjust spacing between plots\n","plt.show()\n","``\n"]},{"cell_type":"markdown","id":"04f7cdbf-5a61-4592-bd38-50d89999697b","metadata":{},"outputs":[],"source":["### Exercise 4. Are these results consistent with our previous results, where we analyzed the evaluation metric plots against k?\n"]},{"cell_type":"markdown","id":"59ffc767-7660-4676-b309-e87d48319665","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","By examining the clustering results for k = 2, 3, and 4, it becomes apparent that the intuitive choice for the optimal number of clusters is four, although one could also argue for three. For k=3, the first cluster fails to distinguish between two regions with varying densities, whereas for k=4, these regions are split into two distinct clusters (clusters 1 and 3).\n","\n","The silhouette plot for k=4 shows relatively uniform block widths across clusters, suggesting clusters of similar sizes. However, the shape of these blocks indicates that many points are somewhat ambiguously assigned, highlighting that the clusters are not distinctly separated and may overlap to some extent.\n","\n","Determining the 'correct' number of clusters is not straightforward, as it often involves subjective judgment. Metrics alone point to k=3 as being optimal, given that the silhouette plot for k=3 shows better cluster separation, with higher and more consistent silhouette scores across clusters compared to other choices for k.\n"]},{"cell_type":"markdown","id":"779af22e-1f79-42a2-868f-75f7eb3520cc","metadata":{},"outputs":[],"source":["### Limitations of k-means - Shape sensitivity\n","Can you identify situations where K-means would not be appropriate? What alternatives could be used?\n","\n","Let's explore these questions with an experiment. Using `make_classification` we'll create a labelled, 2-d dataset cosisting of three classes. This time we'll have differently shaped sets of points in each class, not just spherical blobs.\n"]},{"cell_type":"code","id":"aff1a6b1-c9e4-4b07-ae95-1109367e301b","metadata":{},"outputs":[],"source":["# Generate synthetic classification data\nX, y_true = make_classification(n_samples=300, n_features=2, n_informative=2, n_redundant=0,\n                                n_clusters_per_class=1, n_classes=3, random_state=42)\n\n# Apply K-Means clustering\nkmeans = KMeans(n_clusters=3, random_state=42)\ny_kmeans = kmeans.fit_predict(X)\ncentroids = kmeans.cluster_centers_\n\n# Compute the Voronoi diagram\nvor = Voronoi(centroids)\n\n# Create a 2x2 grid of subplots\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Get consistent axis limits for all scatter plots\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n# Plot the true labels with Voronoi regions\ncolormap = cm.tab10\ncolors_true = colormap(y_true.astype(float) / 3)\naxes[0, 0].scatter(X[:, 0], X[:, 1], c=colors_true, s=50, alpha=0.5, ec='k')\nvoronoi_plot_2d(vor, ax=axes[0, 0], show_vertices=False, line_colors='red', line_width=2, line_alpha=0.6, point_size=2)\naxes[0, 0].set_title('Labelled Classification Data with Voronoi Regions')\naxes[0, 0].set_xlabel('Feature 1')\naxes[0, 0].set_ylabel('Feature 2')\naxes[0, 0].set_xlim(x_min, x_max)\naxes[0, 0].set_ylim(y_min, y_max)\n\n# Call evaluate_clustering for true labels\nevaluate_clustering(X, y_true, n_clusters=3, ax=axes[1, 0], title_suffix=' True Labels')\n\n# Plot K-Means clustering results with Voronoi regions\ncolors_kmeans = colormap(y_kmeans.astype(float) / 3)\naxes[0, 1].scatter(X[:, 0], X[:, 1], c=colors_kmeans, s=50, alpha=0.5, ec='k')\naxes[0, 1].scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, marker='x', label='Centroids')\nvoronoi_plot_2d(vor, ax=axes[0, 1], show_vertices=False, line_colors='red', line_width=2, line_alpha=0.6, point_size=2)\n\naxes[0, 1].set_title('K-Means Clustering with Voronoi Regions')\naxes[0, 1].set_xlabel('Feature 1')\naxes[0, 1].set_ylabel('Feature 2')\naxes[0, 1].set_xlim(x_min, x_max)\naxes[0, 1].set_ylim(y_min, y_max)\n\n# Call evaluate_clustering for K-Means labels\nevaluate_clustering(X, y_kmeans, n_clusters=3, ax=axes[1, 1], title_suffix=' K-Means Clustering')\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()\n"]},{"cell_type":"markdown","id":"76745e09-97a6-40b0-8b87-eb05542843cb","metadata":{},"outputs":[],"source":["### Exercise 5. What can you say about this result?\n","How well did k-means replicate the classes in this experiment? What are some considerations for different approaches?\n"]},{"cell_type":"markdown","id":"2a6203bd-3b29-434c-aaae-78438367c97d","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","K-means did a good job of identifying three clusters that mostly agree with the three classses. \n","    \n","However, looking at the finer details, k-means wasn't able to fully capture the inherent coherence of the two linearly shaped classes (purple and green). \n","\n","Another thing to notice is that the clusters are partitioned in a way that doesn't capture the density of the classes.\n","\n","The red dashed lines in the scatter plots indicate the boundaries between the \"voronoi\" regions that separate the clusters.\n","It's not clear why the two purple points on the lower portion of the yellow cluster were mislabelled. \n","Similarly, the green points between the two green and purple clusters were mislabeled.\n","\n","Interestingly, the silhoutte score is higher for the clustering result than it it for the actual class labels. This is appropriate though since the actual classes slightly overlap, as is also indicated by the negative values in the silhoutte plot for the pink and red classes.\n","\n","Go ahead and explore some different approaches. Of course, in real world clustering problems, you don't have the benefit of knowing the answer like we did in this experiment, so you will need to be creative.The K-means algorithm performed reasonably well in identifying three clusters that mostly align with the true class labels. However, a closer examination reveals that K-means struggles to accurately capture the elongated, linear structure of the two classes (represented in purple and green).\n","\n","One notable limitation is that K-means does not effectively account for the density distribution of points. The resulting clusters are separated by \"Voronoi\" boundaries (indicated by the red dashed lines), which split the space into regions that are equidistant to the nearest centroids. This approach inherently assumes spherical shapes for clusters and can misclassify points if the true distribution deviates from this assumption.\n","\n","For instance, the two purple points within the lower section of the yellow cluster were misclassified. Similarly, some green points situated between the main green and purple clusters were also labeled incorrectly. This highlights K-means' inability to fully respect the density-based continuity of data.\n","\n","A more flexible clustering approach, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise), might be more suitable for this type of data. DBSCAN takes density into account and can identify clusters of varying shapes and sizes, potentially capturing the true structure of the data more effectively.\n","\n","In real-world clustering tasks, the ground truth is not known, so exploring multiple algorithms and adapting them to the specific data characteristics is essential. Testing different methods like DBSCAN or hierarchical clustering, which consider density and proximity in non-linear ways, could provide better results for complex datasets.\n"]},{"cell_type":"markdown","id":"44529863-9bca-4fad-bca9-e02c5ac038c3","metadata":{},"outputs":[],"source":["### Congratulations! You're ready to move on to your next lesson!\n","\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\">Jeff Grossman</a>\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\">Abhishek Gagneja</a>\n","\n","\n","\n","<!-- ## Changelog\n","\n","| Date | Version | Changed by | Change Description |\n","|:------------|:------|:------------------|:---------------------------------------|\n","| 2024-11-05 | 1.0  | Jeff Grossman    | Create content |\n","| 2024-12-03 | 1.1  | Abhishek Gagneja | Typos corrected |\n"," -->\n","\n","\n","## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"52e50afe75160baf18a3987e506a2d1a66a7b767ce971f1c1b3fb5923841e4ae"},"nbformat":4,"nbformat_minor":4}