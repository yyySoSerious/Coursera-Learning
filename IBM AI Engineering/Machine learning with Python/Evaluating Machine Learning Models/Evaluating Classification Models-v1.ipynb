{"cells":[{"cell_type":"markdown","id":"acab0ee3-70f1-44fe-b299-8b67b941fc2c","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n","\n","\n","# Evaluating Classification Models\n","Estimated time needed: **25** minutes\n","    \n","\n","## Objectives\n","\n","\n","After completing this lab, you will be able to:\n","\n","* Implement and evaluate the performance of classification models on real-world data\n","* Interpret and compare various evaluation metrics and the confusion matrix for each model\n","\n"]},{"cell_type":"markdown","id":"27dd41f3-45b3-49e4-9abc-3ca51d14e33b","metadata":{},"outputs":[],"source":["## Introduction\n","In this lab, you will:\n","- Use the breast cancer data set included in scikit-learn to predict whether a tumor is benign or malignant\n","- Create two classification models and evaluate them. \n","- Add some Gaussian random noise to the features to simulate measurement errors\n"," \n","Interpreting and comparing the various evaluation metrics and the confusion matrix for each model will provide you with some valuable intuition regarding what the evaluation metrics mean and how they might impact your interpretation of the model performances.\n","\n","Your goal in this lab is **not** to find the best classifier - it is primarily intended for you to practice interpreting and comparing results in the context of a real-world problem.\n"]},{"cell_type":"markdown","id":"61dc6e3b-1761-40a2-8086-4ad96c975f01","metadata":{},"outputs":[],"source":["First, to make sure that the required libraries are available, execute the cell below.\n"]},{"cell_type":"code","id":"31951995-52eb-4e80-9b52-42174af5a240","metadata":{},"outputs":[],"source":["!pip install numpy==2.2.0\n!pip install pandas==2.2.3\n!pip install scikit-learn==1.6.0\n!pip install matplotlib==3.9.3\n!pip install seaborn==0.13.2"]},{"cell_type":"markdown","id":"b8c67d03-625d-41c0-ac9c-00f8f4e4613e","metadata":{},"outputs":[],"source":["## Import the required libraries\n"]},{"cell_type":"code","id":"11307345-9543-430f-ad7f-7022d7f7a4d6","metadata":{},"outputs":[],"source":["import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns"]},{"cell_type":"markdown","id":"e4d307da-e2bf-4fe4-83de-82dc08c01c71","metadata":{},"outputs":[],"source":["### Load the Breast Cancer data set\n"]},{"cell_type":"code","id":"dd242c22-c03f-4a2e-a1f3-92f332e5d331","metadata":{},"outputs":[],"source":["data = load_breast_cancer()\nX, y = data.data, data.target\nlabels = data.target_names\nfeature_names = data.feature_names"]},{"cell_type":"markdown","id":"bd2766d3-d412-4fbf-bd2d-e006922db448","metadata":{},"outputs":[],"source":["### Print the description of the Breast Cancer data set\n"]},{"cell_type":"code","id":"504dbdfb-b219-4bfb-8176-d7075cb649e9","metadata":{},"outputs":[],"source":["print(data.DESCR)"]},{"cell_type":"markdown","id":"91988fb2-e925-47a3-85d4-ff84b2c95ffd","metadata":{},"outputs":[],"source":["In summary, each observation in the data set consists of a variety attributes measured from a sample of cells from a suspicious mass taken from a patient. \n","The goal is to predict whether a mass is malignant (positive case) or benign (negative case):\n"]},{"cell_type":"code","id":"afa6152d-1b59-418c-95e2-28818f249b91","metadata":{},"outputs":[],"source":["print(data.target_names)"]},{"cell_type":"markdown","id":"f4c46972-4a1e-4527-b491-8bb0dd4929c5","metadata":{},"outputs":[],"source":["### Standardize the data\n"]},{"cell_type":"code","id":"98fa6980-ac8a-4d84-a842-bf67254ad4dd","metadata":{},"outputs":[],"source":["scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)"]},{"cell_type":"markdown","id":"dd76078d-d36e-4504-a287-a57fb3c6b4c0","metadata":{},"outputs":[],"source":["### Add some noise\n","Next, add some noise to simulate random measurement error, then view the first few rows of the original and noisy features for comparison.\n"]},{"cell_type":"code","id":"52c0ffe8-a118-44c3-aa1b-e32f91a3164b","metadata":{},"outputs":[],"source":["# Add Gaussian noise to the data set\nnp.random.seed(42)  # For reproducibility\nnoise_factor = 0.5 # Adjust this to control the amount of noise\nX_noisy = X_scaled + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape)\n\n# Load the original and noisy data sets into a DataFrame for comparison and visualization\ndf = pd.DataFrame(X_scaled, columns=feature_names)\ndf_noisy = pd.DataFrame(X_noisy, columns=feature_names)"]},{"cell_type":"code","id":"f8d76730-4e8c-46dd-b5f4-9848927169eb","metadata":{},"outputs":[],"source":["# Display the first few rows of the standardized original and noisy data sets for comparison\nprint(\"Original Data (First 5 rows):\")\ndf.head()"]},{"cell_type":"code","id":"935bee57-3d49-4ad3-b6a2-3f2dd08e5fa8","metadata":{},"outputs":[],"source":["print(\"\\nNoisy Data (First 5 rows):\")\ndf_noisy.head()"]},{"cell_type":"markdown","id":"7604bb44-8423-4d8d-97a4-1e34f8d09b3c","metadata":{},"outputs":[],"source":["### Visualizing the noise content. \n","You can get a good idea of how much noise there is in the features by comparing values in the previous tables. You can also visualize the differences in several ways.\n","Let's begin by plotting the histograms of one of the features with and without noise for comparison.\n","#### Histograms\n"]},{"cell_type":"code","id":"0c056b76-453b-418b-be02-acfd9f847b55","metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n\n# Original Feature Distribution (Noise-Free)\nplt.subplot(1, 2, 1)\nplt.hist(df[feature_names[5]], bins=20, alpha=0.7, color='blue', label='Original')\nplt.title('Original Feature Distribution')\nplt.xlabel(feature_names[5])\nplt.ylabel('Frequency')\n\n# Noisy Feature Distribution\nplt.subplot(1, 2, 2)\nplt.hist(df_noisy[feature_names[5]], bins=20, alpha=0.7, color='red', label='Noisy') \nplt.title('Noisy Feature Distribution')\nplt.xlabel(feature_names[5])  \nplt.ylabel('Frequency')\n\nplt.tight_layout()  # Ensures proper spacing between subplots\nplt.show()"]},{"cell_type":"markdown","id":"46c2c88f-5bca-44d9-bc40-bdf40283e96b","metadata":{},"outputs":[],"source":["The noise-free histogram is skewed to the left and appears to a log-normal distribution, while the noisy histogram is less skewed, tending toward a normal distribution.\n"]},{"cell_type":"markdown","id":"c8fa0978-a16a-4bcf-836b-1f53162e9567","metadata":{},"outputs":[],"source":["#### Plots\n","You can also plot the two features together to get a sense of their differences.\n"]},{"cell_type":"code","id":"484c3587-f5a8-4550-8ff0-7e1454450891","metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\nplt.plot(df[feature_names[5]], label='Original',lw=3)\nplt.plot(df_noisy[feature_names[5]], '--',label='Noisy',)\nplt.title('Scaled feature comparison with and without noise')\nplt.xlabel(feature_names[5])\nplt.legend()\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"4da27250-0c8c-4657-baaf-32a20506acb6","metadata":{},"outputs":[],"source":["#### Scatterplot\n","Finally, you can compare the two features ussing a scatterplot. This gives you an excellent idea of how well the two features are correlated.\n"]},{"cell_type":"code","id":"3685447e-51f7-45d9-8e9d-049278c25a07","metadata":{},"outputs":[],"source":["plt.figure(figsize=(12, 6))\nplt.scatter(df[feature_names[5]], df_noisy[feature_names[5]],lw=5)\nplt.title('Scaled feature comparison with and without noise')\nplt.xlabel('Original Feature')\nplt.ylabel('Noisy Feature')\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"6961b1aa-c4d3-4893-aec9-418a8b58ee62","metadata":{},"outputs":[],"source":["### Exercise 1. Split the data, and fit the KNN and SVM models to the noisy training data\n"]},{"cell_type":"code","id":"791fb1cb-9bba-43e4-8637-42765e65aaa2","metadata":{},"outputs":[],"source":["# Enter your code here\n\n# Split the data set into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(..., y, test_size=0.3, random_state=42)\n\n# Initialize the models\nknn = ...(n_neighbors=5)\nsvm = ...(kernel='linear', C=1, random_state=42)\n\n# Fit the models to the training data\nknn.fit(X_train, ...)\nsvm.fit(X_train, ...)"]},{"cell_type":"markdown","id":"04e375d6-0cd8-4892-a7c7-9aef0a139150","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","# Split the data set into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_noisy, y, test_size=0.3, random_state=42)\n","#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Initialize the models\n","knn = KNeighborsClassifier(n_neighbors=5)\n","svm = SVC(kernel='linear', C=1, random_state=42)\n","\n","# Fit the models to the training data\n","knn.fit(X_train, y_train)\n","svm.fit(X_train, y_train)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"fe63da4b-1ed3-4027-a00c-36ff972a26fc","metadata":{},"outputs":[],"source":["### Evaluate the models\n"]},{"cell_type":"markdown","id":"d3fdc692-7b96-4a21-b951-94692be52148","metadata":{},"outputs":[],"source":["#### Predict on the test set\n"]},{"cell_type":"code","id":"e7df785a-f516-47bf-86dd-a418bdb71d2e","metadata":{},"outputs":[],"source":["y_pred_knn = knn.predict(X_test)\ny_pred_svm = svm.predict(X_test)"]},{"cell_type":"markdown","id":"02d9f639-adc3-477e-8f6a-3682ba012e6a","metadata":{},"outputs":[],"source":["#### Print the accuracy scores and classification reports for both models\n"]},{"cell_type":"code","id":"4e3dc393-19d2-4a73-8dc0-a7bad91e7372","metadata":{},"outputs":[],"source":["print(f\"KNN Testing Accuracy: {accuracy_score(y_test, y_pred_knn):.3f}\")\nprint(f\"SVM Testing Accuracy: {accuracy_score(y_test, y_pred_svm):.3f}\")\n\nprint(\"\\nKNN Testing Data Classification Report:\")\nprint(classification_report(y_test, y_pred_knn))\n\nprint(\"\\nSVM Testing Data Classification Report:\")\nprint(classification_report(y_test, y_pred_svm))"]},{"cell_type":"markdown","id":"a93f798b-d905-4840-ad48-4f267c134077","metadata":{},"outputs":[],"source":["#### Plot the confusion matrices\n"]},{"cell_type":"code","id":"908056c3-cb32-4b54-9f4b-d3da843aba30","metadata":{},"outputs":[],"source":["conf_matrix_knn = confusion_matrix(y_test, y_pred_knn)\nconf_matrix_svm = confusion_matrix(y_test, y_pred_svm)\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns.heatmap(conf_matrix_knn, annot=True, cmap='Blues', fmt='d', ax=axes[0],\n            xticklabels=labels, yticklabels=labels)\n\naxes[0].set_title('KNN Testing Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\n\nsns.heatmap(conf_matrix_svm, annot=True, cmap='Blues', fmt='d', ax=axes[1],\n            xticklabels=labels, yticklabels=labels)\naxes[1].set_title('SVM Testing Confusion Matrix')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"17ff3d63-ce4d-4329-a814-093fabdef253","metadata":{},"outputs":[],"source":["### Exercise 2. What is the worst kind of prediction error in this context?\n","It would indeed be very unpleasant to be told you have cancer, when in fact you don't.\n","But the consequences of being told you don't have cancer when you actually do are life threatening.\n","State this worse-case scenario in terms of true/false positive/negative diagnoses, and identify their counts from the confusion matrices.\n"]},{"cell_type":"code","id":"9b0bcf3b-70b0-4843-ad48-322b862a428b","metadata":{},"outputs":[],"source":["#### Please enter your descriptive answer here and change the cell type to \"Markdown\" for formatting.\n"]},{"cell_type":"markdown","id":"68c38cd6-8dbf-4e5f-b056-004183e29459","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","By convention, a positive test for malignancy means a diagnosis of a mass being malignant.\n","Thus, a benign prediction is a negative prediction.\n","The worse-case scenario then is a false negative prediction, where the test incorrectly predicts that the mass is benign. \n","\n","For the KNN model, the number of false negatives is 7, while for the SVM model the count is 2.\n","We can say that the SVM model has a higher prediction sensitivity than the KNN model does.\n","    \n","</details>\n"]},{"cell_type":"markdown","id":"32d7aa0e-fbda-44ef-86cd-5bf5f27f44bb","metadata":{},"outputs":[],"source":["### Exercise 3. What can you say to compare the overall performances of the two models?\n"]},{"cell_type":"code","id":"7e523504-2d2c-48a4-ae2f-db2a333be98f","metadata":{},"outputs":[],"source":["#### Please enter your descriptive answer here and change the cell type to \"Markdown\" for formatting.\n"]},{"cell_type":"markdown","id":"4c6ab000-6f32-48bc-bb9b-0ec276410178","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","SVM outperformed KNN in terms of precision, recall, and F1-score for both for the individual classes and their overall averages. \n","This indicates that SVM is a stronger classifier. \n","Although KNN performed quite well with an accuracy of 94%, SVM has better ability to correctly classify both malignant and beinign cases, with fewer errors.\n","Given that the goal would be to choose the model with better generalization and fewer false negatives, SVM is certainly the preferred classifier.\n","</details>\n"]},{"cell_type":"markdown","id":"ff0b3b00-a821-407d-b68d-909a283716f5","metadata":{},"outputs":[],"source":["### Are we overfitting?\n","Let's evaluate the results on the training data and compare them against the test data results.\n"]},{"cell_type":"markdown","id":"07671f87-739f-4de7-afbf-eaff45f8a9ac","metadata":{},"outputs":[],"source":["### Exercise 4. Obtain the prediction results using the training data.\n"]},{"cell_type":"code","id":"d30da9d4-5088-4a83-bc5f-9edaf27568f2","metadata":{},"outputs":[],"source":["# Enter your code here:\ny_pred_train_knn = ...\ny_pred_train_svm = ..."]},{"cell_type":"markdown","id":"3043ac5d-20a1-44be-84ba-42a17412aba0","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","```python\n","y_pred_train_knn = knn.predict(X_train)\n","y_pred_train_svm = svm.predict(X_train)\n","```\n","\n","</details>\n"]},{"cell_type":"code","id":"cc67d13d-1689-4cc5-9b9c-1dff912b9aa5","metadata":{},"outputs":[],"source":["# Evaluate the models on the training data\nprint(f\"KNN Training Accuracy: {accuracy_score(y_train, y_pred_train_knn):.3f}\")\nprint(f\"SVM Training Accuracy: {accuracy_score(y_train, y_pred_train_svm):.3f}\")\n\nprint(\"\\nKNN Training Classification Report:\")\nprint(classification_report(y_train, y_pred_train_knn))\n\nprint(\"\\nSVM Training Classification Report:\")\nprint(classification_report(y_train, y_pred_train_svm))"]},{"cell_type":"markdown","id":"2342b8b7-7df5-4cee-87ae-b2fed0f9a0be","metadata":{},"outputs":[],"source":["### Exercise 5. Plot the confusion matrices for the training data\n"]},{"cell_type":"code","id":"de45b972-af08-4457-a8d0-8f08b2149f90","metadata":{},"outputs":[],"source":["# Enter your code here\nconf_matrix_knn = confusion_matrix(y_train, ...)\nconf_matrix_svm = confusion_matrix(y_train, ...)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns....(..., annot=True, cmap='Blues', fmt='d', ax=axes[0],\n            xticklabels=labels, yticklabels=labels)\n\naxes[0].set_title('KNN Training Confusion Matrix')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\n\nsns.heatmap(..., annot=True, cmap='Blues', fmt='d', ax=axes[1],\n            xticklabels=labels, yticklabels=labels)\naxes[1].set_title('SVM Training Confusion Matrix')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"c1a2d04f-f832-4384-a000-77a6c2df5235","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","    \n","```python\n","# Plot the confusion matrices\n","conf_matrix_knn = confusion_matrix(y_train, y_pred_train_knn)\n","conf_matrix_svm = confusion_matrix(y_train, y_pred_train_svm)\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","sns.heatmap(conf_matrix_knn, annot=True, cmap='Blues', fmt='d', ax=axes[0],\n","            xticklabels=labels, yticklabels=labels)\n","\n","axes[0].set_title('KNN Training Confusion Matrix')\n","axes[0].set_xlabel('Predicted')\n","axes[0].set_ylabel('Actual')\n","\n","sns.heatmap(conf_matrix_svm, annot=True, cmap='Blues', fmt='d', ax=axes[1],\n","            xticklabels=labels, yticklabels=labels)\n","axes[1].set_title('SVM Training Confusion Matrix')\n","axes[1].set_xlabel('Predicted')\n","axes[1].set_ylabel('Actual')\n","\n","plt.tight_layout()\n","plt.show()\n","```\n","</details>\n"]},{"cell_type":"markdown","id":"0a370d10-0bc2-4dc8-adba-b8d430c31ba5","metadata":{},"outputs":[],"source":["### Exercise 6. Comparing training and testing accuracies for both models\n","What can you say about the accuracy of the two models on the training and test data sets?\n","\n","What do these results possibly indicate?\n"]},{"cell_type":"code","id":"db1359ba-db79-4ec9-adaf-a3d1256c38f2","metadata":{},"outputs":[],"source":["#### Please enter your descriptive answer here and change the cell type to \"Markdown\" for formatting.\n"]},{"cell_type":"markdown","id":"b7c5b898-5fbd-432c-9a58-bb6f5d686db1","metadata":{},"outputs":[],"source":["<details><summary>Click here for the solution</summary>\n","\n","\n","Ideally the accuracy of a model would be almost the same on the training and testing data sets.\n","\n","It would be unusual for the accuracy to be higher on the test set and this might occur due to chance or some sort of data leakage. For example, here we have normalized all of the data rather than fitting StandardScaler to the training data and only then applying it to the train and test sets separately. We'll revisit this and other pitfalls in another lab. \n","\n","When the accuracy is substantially higher on the training data than on the testing data, the model is likely memorizing details in the training data that don't generalize to the unseen data - the model is overfitting to the training data.\n","\n","\n","| Model | Phase |  Accuracy |\n","| ------------  | -------- | --------- |\n","| KNN  | Train  | 95.5% |\n","| KNN  | Test   | 93.6% |\n","| SVM  | Train  | 97.2% |\n","| SVM  | Test   | 97.1% |\n","\n","For the SVM model, the training and testing accuracies are essentially the same at about 97%. This is ideal - the SVM model is likely not overfit.\n","For the KNN model, however, the training accuracy is about 2% higher that the test accuracy, indicating there might be some overfitting.\n","\n","In summary, the SVM model is both more convincing and has a higher accuracy than the KNN model. \n","Remember, we aren't trying to tune these models; we are just comparing their performance with a fixed set of hyperparamters.\n","\n","\n","</details>\n"]},{"cell_type":"markdown","id":"45cfe798-7601-476c-bf2b-486ff29192a0","metadata":{},"outputs":[],"source":["## Summary\n","Congratulations! You're ready to move on to your next lesson! \n","In this lab, you learned how to implement and assess the performance of classification models using real-world data. You explored different evaluation metrics and the confusion matrix to understand how well your models performed. By working with the breast cancer data set, you created two classification models to predict whether a mass is benign or malignant. Additionally, you simulated measurement errors by adding Gaussian random noise to the features and then evaluated the impact on your model's performance.\n","\n","\n","## Author\n","\n","<a href=\"https://www.linkedin.com/in/jpgrossman/\" target=\"_blank\">Jeff Grossman</a>\n","\n","\n","### Other Contributors\n","\n","<a href=\"https://www.linkedin.com/in/abhishek-gagneja-23051987/\" target=\"_blank\">Abhishek Gagneja</a>\n","\n","<!-- ## Changelog\n","\n","| Date | Version | Changed by | Change Description |\n","|:------------|:------|:------------------|:---------------------------------------|\n","| 2024-11-05 | 1.0  | Jeff Grossman    | Create content | -->\n","\n","\n","\n","## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"99894bef5d4129388bef7155ee21312dd542cc5ef9ffca25617d69e9394356ae"},"nbformat":4,"nbformat_minor":4}