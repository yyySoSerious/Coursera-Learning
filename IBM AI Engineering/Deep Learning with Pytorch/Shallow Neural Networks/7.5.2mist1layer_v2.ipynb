{"cells":[{"cell_type":"markdown","id":"42001317-0c5d-46b5-b2ba-15411d96fc17","metadata":{},"outputs":[],"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"c67a2ecf-8327-4183-affe-ad9589fb79b1","metadata":{},"outputs":[],"source":["<h1>Test Sigmoid, Tanh, and Relu Activations Functions on the MNIST Dataset</h1>\n"]},{"cell_type":"markdown","id":"c94c969b-1af3-4d78-987e-013d31490ee2","metadata":{},"outputs":[],"source":["<h2>Objective</h2><ul><li> How to apply different activation functions on the MNIST dataset.</li></ul> \n"]},{"cell_type":"markdown","id":"291276e2-4e21-4ba3-9cf7-70e4b07978df","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will test sigmoid, tanh, and relu activation functions on the MNIST dataset.</p>\n","\n","<ul>\n","    <li><a href=\"#Model\">Neural Network Module and Training Function</a></li>\n","    <li><a href=\"#Makeup_Data\">Make Some Data</a></li>\n","    <li><a href=\"#Train\">Define Several Neural Network, Criterion Function, and Optimizer</a></li>\n","    <li><a href=\"#Test\">Test Sigmoid, Tanh, and Relu</a></li>\n","    <li><a href=\"#Result\">Analyze Results</a></li>\n","</ul>\n","<p></p>\n","Estimated Time Needed: <strong>25 min</strong>\n","</div>\n","\n","<hr>\n"]},{"cell_type":"markdown","id":"d6519074-7b97-4289-882e-dde393a09d40","metadata":{},"outputs":[],"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","id":"f4c9587b-e517-4ce1-9d4f-068391b3cad8","metadata":{},"outputs":[],"source":["We'll need the following libraries\n"]},{"cell_type":"code","id":"88e5fdda-5757-46fb-9a8a-9d3ddad68e4e","metadata":{},"outputs":[],"source":["# Uncomment the following line to install the torchvision library\n# !mamba install -y torchvision\n\n# Import the libraries we need for this lab\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\nimport matplotlib.pylab as plt\nimport numpy as np"]},{"cell_type":"markdown","id":"63ee9a39-ee84-4d4c-b7de-d49cf10e1693","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"d5560376-fc3a-4f65-aa67-7b671efef84f","metadata":{},"outputs":[],"source":["<h2 id=\"Model\">Neural Network Module and Training Function</h2> \n"]},{"cell_type":"markdown","id":"8e56f09b-b5a4-4a7b-b37c-ddbfac325ab8","metadata":{},"outputs":[],"source":["Define the neural network module or class using the sigmoid activation function: \n"]},{"cell_type":"code","id":"f9a65ec3-d7d1-4847-ac51-bc85720ea523","metadata":{},"outputs":[],"source":["# Build the model with sigmoid function\n\nclass Net(nn.Module):\n    \n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n    \n    # Prediction\n    def forward(self, x):\n        x = torch.sigmoid(self.linear1(x))  \n        x = self.linear2(x)\n        return x"]},{"cell_type":"markdown","id":"e23ad51a-110d-458e-bdc4-97c41142eaf4","metadata":{},"outputs":[],"source":["\n","Define the neural network module or class using the Tanh activation function:\n"]},{"cell_type":"code","id":"887ca11a-172e-4505-ae70-0ed693b6b2d3","metadata":{},"outputs":[],"source":["# Build the model with Tanh function\n\nclass NetTanh(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetTanh, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.tanh(self.linear1(x))\n        x = self.linear2(x)\n        return x"]},{"cell_type":"markdown","id":"facef1fc-49f3-4c73-95f3-f52ed2aa72f7","metadata":{},"outputs":[],"source":["Define the neural network module or class using the Relu activation function:\n"]},{"cell_type":"code","id":"e2584e15-a1b6-415c-b1ed-1cbb7818bc66","metadata":{},"outputs":[],"source":["# Build the model with Relu function\n\nclass NetRelu(nn.Module):\n\n    # Constructor\n    def __init__(self, D_in, H, D_out):\n        super(NetRelu, self).__init__()\n        self.linear1 = nn.Linear(D_in, H)\n        self.linear2 = nn.Linear(H, D_out)\n\n    # Prediction\n    def forward(self, x):\n        x = torch.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x"]},{"cell_type":"markdown","id":"f87f9968-62e0-4566-9cd0-f8f235d3fa04","metadata":{},"outputs":[],"source":["Define a function to train the model. In this case, the function returns a Python dictionary to store the training loss for each iteration  and accuracy on the validation data.\n"]},{"cell_type":"code","id":"f3297b63-aa9d-44af-95ad-d222006f0eac","metadata":{},"outputs":[],"source":["# Define the function for training the model\n\ndef train(model, criterion, train_loader, validation_loader, optimizer, epochs = 100):\n    i = 0\n    useful_stuff = {'training_loss':[], 'validation_accuracy':[]}  \n\n    for epoch in range(epochs):\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            z = model(x.view(-1, 28 * 28))\n            loss = criterion(z, y)\n            loss.backward()\n            optimizer.step()\n            useful_stuff['training_loss'].append(loss.item())\n\n        correct = 0\n        for x, y in validation_loader:\n            z = model(x.view(-1, 28 * 28))\n            _, label=torch.max(z, 1)\n            correct += (label == y).sum().item()\n        accuracy = 100 * (correct / len(validation_dataset))\n        useful_stuff['validation_accuracy'].append(accuracy)\n\n    return useful_stuff"]},{"cell_type":"markdown","id":"48491f70-6532-4846-bd67-d8655f34d3f4","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"336ce7c5-75bf-4745-9543-350181251ff7","metadata":{},"outputs":[],"source":["<h2 id=\"Makeup_Data\">Make Some Data</h2> \n"]},{"cell_type":"markdown","id":"2cb6b42b-b8a1-44a3-84af-fa35391c34a2","metadata":{},"outputs":[],"source":["Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.\n"]},{"cell_type":"code","id":"caab1db9-4766-4b3e-b281-721eeda35195","metadata":{},"outputs":[],"source":["# Create the training dataset\n\ntrain_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())"]},{"cell_type":"markdown","id":"5b1ac639-2ca9-48eb-8ba7-7312663ed30c","metadata":{},"outputs":[],"source":["Load the testing dataset by setting the parameter <code>train</code> to <code>False</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.\n"]},{"cell_type":"code","id":"18d6483b-8c2c-4d2a-b67a-9520b169bd67","metadata":{},"outputs":[],"source":["# Create the validation  dataset\n\nvalidation_dataset = dsets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"]},{"cell_type":"markdown","id":"9622eb8c-be0f-4cb4-81f5-3a5ac7f86427","metadata":{},"outputs":[],"source":["Create the criterion function:  \n"]},{"cell_type":"code","id":"617315c3-6212-48f3-bd8f-115d79950476","metadata":{},"outputs":[],"source":["# Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"27533774-3aeb-47b5-85fd-bcf72fb0644d","metadata":{},"outputs":[],"source":["Create the training-data loader and the validation-data loader object:\n"]},{"cell_type":"code","id":"7a9029d9-3bf6-460e-84eb-77c6900678a7","metadata":{},"outputs":[],"source":["# Create the training data loader and validation data loader object\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=2000, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000, shuffle=False)"]},{"cell_type":"markdown","id":"de1c10ce-2006-4cf2-b59f-0dccb7021f3a","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"04423448-fa8c-45e6-8096-f5a05e1fd410","metadata":{},"outputs":[],"source":["<h2 id=\"Train\">Define the Neural Network, Criterion Function, Optimizer, and Train the Model</h2> \n"]},{"cell_type":"markdown","id":"767ddd90-35da-4898-a70f-e4ae339151af","metadata":{},"outputs":[],"source":["Create the criterion function: \n"]},{"cell_type":"code","id":"c3cf1bac-4d7e-4f9f-9002-bc65a14adcc7","metadata":{},"outputs":[],"source":["# Create the criterion function\n\ncriterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","id":"bfce40dc-eed2-41c0-85b2-ec95140c684f","metadata":{},"outputs":[],"source":["Create the model with 100 hidden neurons:  \n"]},{"cell_type":"code","id":"e94b2f43-c7eb-440e-9e5c-3bd3b2591cc7","metadata":{},"outputs":[],"source":["# Create the model object\n\ninput_dim = 28 * 28\nhidden_dim = 100\noutput_dim = 10\n\nmodel = Net(input_dim, hidden_dim, output_dim)"]},{"cell_type":"markdown","id":"a47b7806-5fe3-4a27-8fe8-e0b2ebad8208","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"f79d065d-d61c-4afc-8a35-52c297421862","metadata":{},"outputs":[],"source":["<h2 id=\"Test\">Test Sigmoid, Tanh, and Relu</h2> \n"]},{"cell_type":"markdown","id":"0da7f064-22cc-4b00-814a-9ea81deb58f9","metadata":{},"outputs":[],"source":["Train the network by using the sigmoid activations function:\n"]},{"cell_type":"code","id":"b0293671-8fc1-4391-828f-430fb5b82602","metadata":{},"outputs":[],"source":["# Train a model with sigmoid function\n\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\ntraining_results = train(model, criterion, train_loader, validation_loader, optimizer, epochs=30)"]},{"cell_type":"markdown","id":"d8d64791-a287-40a3-b578-20b21e27e028","metadata":{},"outputs":[],"source":["Train the network by using the Tanh activations function:\n"]},{"cell_type":"code","id":"ef7aff87-ae4b-4267-933f-7b7ff5c0dcac","metadata":{},"outputs":[],"source":["# Train a model with Tanh function\n\nmodel_Tanh = NetTanh(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(model_Tanh.parameters(), lr=learning_rate)\ntraining_results_tanch = train(model_Tanh, criterion, train_loader, validation_loader, optimizer, epochs=30)"]},{"cell_type":"markdown","id":"802a080a-f2e1-4dff-8548-04b49321feb1","metadata":{},"outputs":[],"source":["Train the network by using the Relu activations function:\n"]},{"cell_type":"code","id":"e1be41a8-1a47-4fbb-805f-c1342e31987c","metadata":{},"outputs":[],"source":["# Train a model with Relu function\n\nmodelRelu = NetRelu(input_dim, hidden_dim, output_dim)\noptimizer = torch.optim.SGD(modelRelu.parameters(), lr=learning_rate)\ntraining_results_relu = train(modelRelu, criterion, train_loader, validation_loader, optimizer, epochs=30)"]},{"cell_type":"markdown","id":"40364a96-f071-49fb-913c-7a6cb9fbf0e2","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"6e1fdbe1-43f7-416b-be33-e53b5c5e7880","metadata":{},"outputs":[],"source":["<h2 id=\"Result\">Analyze Results</h2> \n"]},{"cell_type":"markdown","id":"13e28d41-95e0-4965-949a-8faa5b08ef29","metadata":{},"outputs":[],"source":["Compare the training loss for each activation: \n"]},{"cell_type":"code","id":"40d3c8f0-25d7-449e-9158-d8903acf91a3","metadata":{},"outputs":[],"source":["# Compare the training loss\n\nplt.plot(training_results_tanch['training_loss'], label='tanh')\nplt.plot(training_results['training_loss'], label='sigmoid')\nplt.plot(training_results_relu['training_loss'], label='relu')\nplt.ylabel('loss')\nplt.title('training loss iterations')\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"8ae61f48-e5ce-4fbb-bc2a-164b08d374f9","metadata":{},"outputs":[],"source":["Compare the validation loss for each model:  \n"]},{"cell_type":"code","id":"14cec51e-194e-4ca8-bac1-a3b6e3dd07fb","metadata":{},"outputs":[],"source":["# Compare the validation loss\n\nplt.plot(training_results_tanch['validation_accuracy'], label='tanh')\nplt.plot(training_results['validation_accuracy'], label='sigmoid')\nplt.plot(training_results_relu['validation_accuracy'], label='relu') \nplt.ylabel('validation accuracy')\nplt.xlabel('epochs ')\nplt.legend()\nplt.show()"]},{"cell_type":"markdown","id":"4b3f2c12-b41e-4578-b1fa-2d0cc7b1032e","metadata":{},"outputs":[],"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","id":"4c1f1c2b-8e9f-434e-a72b-8a1a13d3edce","metadata":{},"outputs":[],"source":["## Which activation function performed best ?\n"]},{"cell_type":"markdown","id":"3d1dbbf6-2b7e-432e-bf71-34488ca4bc78","metadata":{},"outputs":[],"source":["\n","<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?utm_source=skills_network&utm_content=in_lab_content_link&utm_id=Lab-IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork&context=cpdaas&apps=data_science_experience%2Cwatson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"></a>\n"]},{"cell_type":"markdown","id":"75bda104-6fa2-4dd5-a5f3-7c594b01b793","metadata":{},"outputs":[],"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","id":"23de6434-370e-4ec6-bd31-02393e08ed72","metadata":{},"outputs":[],"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"https://www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","id":"323774a6-299b-4531-bfb9-b7b5ad9b92b6","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2020-09-23  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n","-->\n"]},{"cell_type":"markdown","id":"28be8ab8-cae2-49eb-a6d0-1804c02cbabc","metadata":{},"outputs":[],"source":["<hr>\n"]},{"cell_type":"markdown","id":"32f04000-8893-44b3-b539-ed2cacfb5209","metadata":{},"outputs":[],"source":["\n","\n","\n","## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"422cf833aa8207dd98e8e8d27c87d81816d43276ee5b7b9f39c80705e0a7ef7d"},"nbformat":4,"nbformat_minor":4}