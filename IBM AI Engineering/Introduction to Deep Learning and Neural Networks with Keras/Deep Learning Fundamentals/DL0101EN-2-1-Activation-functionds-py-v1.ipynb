{"cells":[{"cell_type":"markdown","id":"4c178032-43f0-46a6-8d5f-f9d03f8080ac","metadata":{},"outputs":[],"source":["<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n","\n","# Activation Functions and Vanishing Gradients\n","\n","Estimated time needed: **20** mins\n"]},{"cell_type":"markdown","id":"9cbf73f4-417c-4faa-a3ff-404877e3ae6c","metadata":{},"outputs":[],"source":["## Introduction\n"]},{"cell_type":"markdown","id":"c9079702-54be-4a31-812a-964a847957c2","metadata":{},"outputs":[],"source":["In this lab, we will explore two different types of activations functions and see their corresponding derivatives or the gradients.\n"]},{"cell_type":"markdown","id":"95d3f8b4-03b6-4f48-acb8-9e4fd9ee0ef8","metadata":{},"outputs":[],"source":["## Objective for this notebook\n","* Define sigmoid activation function and its derivative\n","* Define ReLu activation function and its derivative\n","* Calculate the Sigmoid and ReLU activations and their derivatives for a range of input values\n","* Plot the output values\n"]},{"cell_type":"markdown","id":"9a824ece-ca7b-4d48-a836-669e515af343","metadata":{},"outputs":[],"source":["# Recap\n"]},{"cell_type":"markdown","id":"18cda111-4333-4f0f-8d1b-ee077b717ceb","metadata":{},"outputs":[],"source":["From the videos, we have learnt that different activation functions have different type of characteristic output curves. And, the corresponding gradient values or their derivatives are also different. In certain cases, with higher input values, the derivative starts to reach to zero, vausing vanishing gradient problem. In this lab, we compare \"Sigmoid\" and \"ReLU\" activation functions and explore their derivatives for vanishing gradients.\n","\n","__Sigmoid Activation Function__\n","* The sigmoid function transforms input values into a range between 0 and 1\n","* Produces an \"S-shaped\" curve, making it suitable for probabilistic outputs or binary classification tasks.\n","* Continuous, differentiable, and monotonically increasing across its domain (-∞, +∞).\n","* Its derivative is simple to compute: σ′(z)=σ(z)⋅(1−σ(z)). This is essential for backpropagation.\n","* Useful for output layers where probabilities are required (e.g., logistic regression).\n","* Enables non-linear decision boundaries, allowing neural networks to learn complex relationships.\n","\n","  __Limitations__\n","    * Suffers from the vanishing gradient problem: For large positive or negative inputs, the gradient approaches zero, slowing down learning in deep networks.\n","    * Computationally expensive due to the exponential operation involved.\n","\n","\n","__ReLU (Rectified Linear Unit) Activation Function__\n","\n","* ReLU outputs the maximum of zero and the input value using f(x)=max⁡(0,x)f(x)=max(0,x).\n","* Non-linear but simpler than sigmoid; it outputs zero for negative inputs and the input value for positive inputs.\n","* The derivative is straightforward: f′(x)=1f′(x)=1 for x>0x>0, and f′(x)=0f′(x)=0 for x≤0x≤0.\n","* Mitigates the vanishing gradient problem by maintaining a constant gradient (1) for positive inputs, enabling faster convergence in deep networks.\n","* Computationally efficient due to its simplicity.\n","\n","  __Limitations__\n","\n","    * Can lead to \"dead neurons,\" where neurons output zero consistently due to negative inputs. This can be mitigated using variants like Leaky ReLU.\n"]},{"cell_type":"markdown","id":"a3815374-5930-4fe2-aae4-c9c90f2fca6e","metadata":{},"outputs":[],"source":["### Installing Required Libraries\n"]},{"cell_type":"markdown","id":"6c7d81fb-2650-4ccd-965f-8383a5165b21","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","id":"8c8e12f5-0b1d-43f6-a064-e87faa649aa2","metadata":{},"outputs":[],"source":["!pip install numpy==2.0.2\n!pip install matplotlib==3.9.2"]},{"cell_type":"markdown","id":"87baa7aa-6cc2-4bbc-a00f-47339c113c25","metadata":{},"outputs":[],"source":["## Import required libraries\n","Let's start with importing the required libraries: Numpy , for matrix calculations\n","and matplotlib for visualization\n"]},{"cell_type":"code","id":"036e4e34-459c-4633-af87-8160fc252a83","metadata":{},"outputs":[],"source":["import numpy as np\nimport matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"d17f6423-0a86-4fb4-92e4-a4dbab27f925","metadata":{},"outputs":[],"source":["#### Define the sigmoid activation function\n"]},{"cell_type":"code","id":"8b7a4244-9376-4517-806f-850262658168","metadata":{},"outputs":[],"source":["# Sigmoid function and its derivatives\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))"]},{"cell_type":"markdown","id":"6de7a7d3-6fb0-48e6-a828-c6564a17d8c2","metadata":{},"outputs":[],"source":["#### Define Sigmoid derivative, \"the sigmoid gradient\"\n"]},{"cell_type":"code","id":"93f2251a-c1bf-4dd5-a750-819efaa8c857","metadata":{},"outputs":[],"source":["def sigmoid_derivative(z):\n    return sigmoid(z) * (1 - sigmoid(z))"]},{"cell_type":"markdown","id":"72a4f190-708a-4c64-adae-9134c9ca1997","metadata":{},"outputs":[],"source":["#### Define the ReLU function\n"]},{"cell_type":"code","id":"93bf6d6c-6e0c-4e2d-ba5f-e7386b4accf7","metadata":{},"outputs":[],"source":["def relu(z):\n    return np.maximum(0, z)"]},{"cell_type":"markdown","id":"137c9c0e-32de-462f-90d8-370b78bc85ca","metadata":{},"outputs":[],"source":["#### Define the ReLU derivative (ReLU gradient)\n"]},{"cell_type":"markdown","id":"076ddc14-30b5-4e46-bcf2-7b4907c03323","metadata":{},"outputs":[],"source":["Let's create a function **relu_derivative** to get the output of the Relu derivative (gradient) \n","The function would take a value $z$ and give the corresponding relu derivative by returning the value **1** for all $z$ greater than 0. \n"]},{"cell_type":"code","id":"14eade9b-35ba-4ca4-8e04-1beb2c63d9ec","metadata":{},"outputs":[],"source":["### type your answer here\n"]},{"cell_type":"markdown","id":"d501f167-62d3-45b1-bcf3-92e708d98d05","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","def relu_derivative(z):\n","    return np.where(z > 0, 1, 0)\n","-->\n"]},{"cell_type":"markdown","id":"fd8569e8-7de3-49a5-aae2-31c7e6f0daf2","metadata":{},"outputs":[],"source":["#### Define the number of input values\n","Generate 400 synthetic values between -10 and 10. The gradient would be calculated for each of these values.\n"]},{"cell_type":"code","id":"9c016c81-5309-44e0-a379-e55677783a15","metadata":{},"outputs":[],"source":["# Generate a range of input values\nz = np.linspace(-10, 10, 400)"]},{"cell_type":"markdown","id":"992d3ff3-8911-4817-9e12-0d059b5889a3","metadata":{},"outputs":[],"source":["Next, let's compute the derivatives for Sigmoid and Relu activations and assign the values to \n","**sigmoid_grad** and **relu_grad** respectively \n"]},{"cell_type":"code","id":"db00b4c9-e37e-4ca8-8aa2-cb720e73adfb","metadata":{},"outputs":[],"source":["### type your answer here\n"]},{"cell_type":"markdown","id":"b9d1f057-3a99-46cf-be99-6903527375ab","metadata":{},"outputs":[],"source":["Double-click __here__ for the solution.\n","<!-- The correct answer is:\n","sigmoid_grad = sigmoid_derivative(z)\n","relu_grad = relu_derivative(z)\n","-->\n"]},{"cell_type":"markdown","id":"c3e923fb-5861-482e-bf0c-3d13323b7db6","metadata":{},"outputs":[],"source":["Awesome! So now, let's visualize the sigmoid and relu activations and their gradients by plotting their output values\n"]},{"cell_type":"code","id":"adeb6d34-68cf-430d-9bdd-ad9fe760f086","metadata":{},"outputs":[],"source":["# Plot the activation functions\nplt.figure(figsize=(12, 6))\n\n# Plot Sigmoid and its derivative\nplt.subplot(1, 2, 1)\nplt.plot(z, sigmoid(z), label='Sigmoid Activation', color='b')\nplt.plot(z, sigmoid_grad, label=\"Sigmoid Derivative\", color='r', linestyle='--')\nplt.title('Sigmoid Activation & Gradient')\nplt.xlabel('Input Value (z)')\nplt.ylabel('Activation / Gradient')\nplt.legend()\n\n# Plot ReLU and its derivative\nplt.subplot(1, 2, 2)\nplt.plot(z, relu(z), label='ReLU Activation', color='g')\nplt.plot(z, relu_grad, label=\"ReLU Derivative\", color='r', linestyle='--')\nplt.title('ReLU Activation & Gradient')\nplt.xlabel('Input Value (z)')\nplt.ylabel('Activation / Gradient')\nplt.legend()\n\nplt.tight_layout()\nplt.show()"]},{"cell_type":"markdown","id":"362eef1a-846c-43bb-b5c1-33f824b27731","metadata":{},"outputs":[],"source":["The above codes should help you understand the working and differences between Sigmoid and ReLU activation functions.\n"]},{"cell_type":"markdown","id":"ef50a543-b395-440a-bc80-dce1a278413f","metadata":{},"outputs":[],"source":["<h3>Practice Exercise 1</h3>\n"]},{"cell_type":"markdown","id":"686f1159-8a93-4e3b-b160-9e605da8f799","metadata":{},"outputs":[],"source":["In this practice exercise, let's try to create the hyperbolic tangent function and it's derivative.\n"]},{"cell_type":"code","id":"a64cb660-e15b-4e71-9f9c-4f0fe7fc8b27","metadata":{},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","id":"a3090c5f-cbe0-48e7-9fc8-e3389032cb58","metadata":{},"outputs":[],"source":["Double-click <b>here</b> for the solution.\n","\n","<!-- Your answer is below:\n","def tanh(z):\n","    return np.tanh(z)\n","\n","def tanh_derivative(z):\n","    return 1 - np.tanh(z) ** 2\n","    -->\n"]},{"cell_type":"markdown","id":"3ae8a8ed-33ac-4524-9e09-e6b12f6edd7e","metadata":{},"outputs":[],"source":["<h3>Practice Exercise 2</h3>\n"]},{"cell_type":"markdown","id":"f3a6027b-fa9e-410b-bbfb-489f879b97ca","metadata":{},"outputs":[],"source":["Now, let's plot and compare the output of tanH function with the ReLU function for 100 synthetic values between -5 to 5\n"]},{"cell_type":"code","id":"a4f0f5b3-457d-493a-9a74-711a1b819473","metadata":{},"outputs":[],"source":["# Write your code here\n"]},{"cell_type":"markdown","id":"f2cd377f-d143-46b4-b378-c5356f1dfce6","metadata":{},"outputs":[],"source":["Double-click <b>here</b> for the solution.\n","\n","<!-- Your answer is below:\n","# Generate a range of input values\n","z = np.linspace(-5, 5, 100)\n","\n","# Get the Relu and tanH gradient values\n","tanh_grad = tanh_derivative(z)\n","relu_grad = relu_derivative(z)\n","\n","\n","# Plot the activation functions\n","plt.figure(figsize=(12, 6))\n","\n","# Plot Relu and its derivative\n","plt.subplot(1, 2, 1)\n","plt.plot(z, relu(z), label='ReLU Activation', color='g')\n","plt.plot(z, relu_grad, label=\"ReLU Derivative\", color='r', linestyle='--')\n","plt.title('ReLU Activation & Gradient')\n","plt.xlabel('Input Value (z)')\n","plt.ylabel('Activation / Gradient')\n","plt.legend()\n","\n","\n","# Plot tanH and its derivative\n","plt.subplot(1, 2, 2)\n","plt.plot(z, tanh(z), label='tanH Activation', color='g')\n","plt.plot(z, tanh_grad, label=\"tanH Derivative\", color='r', linestyle='--')\n","plt.title('tanH Activation & Gradient')\n","plt.xlabel('Input Value (z)')\n","plt.ylabel('Activation / Gradient')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","-->\n"]},{"cell_type":"markdown","id":"9fdac587-aeb3-4fa5-88c8-a0e665695d85","metadata":{},"outputs":[],"source":["### Thank you for completing this lab!\n","\n","This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"]},{"cell_type":"markdown","id":"d83451e2-07bb-4236-bfa1-2fdeb60cd9c2","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2024-11-20  | 3.0  | Aman  |  Created the lab |\n","<hr>\n","\n","## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"]},{"cell_type":"markdown","id":"a1c51218-4274-40f8-9018-9b9a67da1976","metadata":{},"outputs":[],"source":["## <h3 align=\"center\"> &#169; IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"13852ef972b5a871226737880e643e80366d77f56daacccee4788d47850f268b"},"nbformat":4,"nbformat_minor":4}