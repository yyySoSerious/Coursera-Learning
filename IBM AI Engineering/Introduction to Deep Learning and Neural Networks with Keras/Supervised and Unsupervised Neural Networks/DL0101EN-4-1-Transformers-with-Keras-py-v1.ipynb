{"cells":[{"cell_type":"markdown","id":"18d361d0-8fd8-4bf1-ad3f-e0346ee0d42f","metadata":{},"outputs":[],"source":["<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n","\n","# Transformers with Keras\n","\n","Estimated time needed **45** mins\n"]},{"cell_type":"markdown","id":"6f6e35e5-7ff0-460b-bec9-5a56cd6e6ac2","metadata":{},"outputs":[],"source":["In this lab, we will learn how to use the Keras library to build a transformer using a sequence-to-sequence architecture with self-attention for translation. We will train the model using a sample dataset and then use this model for English to Spanish translation.\n"]},{"cell_type":"markdown","id":"96a6ec8f-f443-4ed0-83c4-89dce58d2df9","metadata":{},"outputs":[],"source":["# Objectives for this Notebook    \n","* How to use the Keras library to build transformers model\n","* Train the transformer model using a given dataset\n","* Use the trained transformer model to translate\n"]},{"cell_type":"markdown","id":"36936894-9d64-49a8-8f38-216ed3d858bf","metadata":{},"outputs":[],"source":["<h2>Table of Contents</h2>\n","\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 4>\n","1. <a href=\"#Import-Keras-and-Packages\">Import Keras and Packages</a><br>\n","2. <a href=\"#Step-1:-Data-Preparation\">Step 1: Data Preparation</a><br>\n","3. <a href=\"#Step-2:-Self-Attention-Layer\">Step 2: Self-Attention Layer</a><br>\n","4. <a href=\"#Step-3:-Model-Architecture\">Step 3: Model Architecture</a><br>\n","5. <a href=\"#Step-4:-Training-the-Model\">Step 4: Training the Model</a><br>\n","6. <a href=\"#Step-5:-Plotting-the-training-loss\">Step 5: Plotting the training loss</a><br>\n","\n","</font>\n","</div>\n"]},{"cell_type":"markdown","id":"15e09398-3e45-4c55-bff9-a46b72306e9e","metadata":{},"outputs":[],"source":["## Import Keras and Packages\n"]},{"cell_type":"markdown","id":"f93ab464-1307-47ac-9cc7-068043371e05","metadata":{},"outputs":[],"source":["Let's start by importing the keras libraries and the packages that we would need to build a neural network.\n"]},{"cell_type":"markdown","id":"ca0f0ec8-0273-420c-a8f1-8e8cb4d21cc4","metadata":{},"outputs":[],"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","id":"68e65ffe-c076-4b58-86a5-54d66f8caab8","metadata":{},"outputs":[],"source":["!pip install tensorflow==2.17.1\n!pip install matplotlib==3.9.2\n\nprint(\"==== All required libraries are installed =====\")"]},{"cell_type":"markdown","id":"58b6698d-3cbb-4046-beff-2219941d4eef","metadata":{},"outputs":[],"source":["#### Suppress the tensorflow warning messages\n","We use the following code to  suppress the warning messages due to use of CPU architechture for tensoflow.\n","\n","You may want to **comment out** these lines if you are using the GPU architechture\n"]},{"cell_type":"code","id":"30ec5512-a446-4e25-8774-1111452084a3","metadata":{},"outputs":[],"source":["import os\nos.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"]},{"cell_type":"markdown","id":"6bb8169c-27d3-43f9-9a2b-1c5dc63d6815","metadata":{},"outputs":[],"source":["##### To use Keras, you will also need to install a backend framework – such as TensorFlow.\n","\n","If you install TensorFlow 2.16 or above, it will install Keras by default.\n","\n","We are using the CPU version of tensorflow since we are dealing with smaller datasets. \n","You may install the GPU version of tensorflow on your machine to accelarate the processing of larger datasets\n"]},{"cell_type":"markdown","id":"5ff95da7-40b3-47f6-97d3-e5ccaa2bf057","metadata":{},"outputs":[],"source":["### Import the necessary libraries\n"]},{"cell_type":"code","id":"93d26526-f908-4ae9-aed9-c50def07085f","metadata":{},"outputs":[],"source":["import numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Embedding, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\nfrom keras.layers import Layer\nimport warnings\nwarnings.simplefilter('ignore', FutureWarning)"]},{"cell_type":"markdown","id":"1d4c50cd-f5dd-45de-96c2-3a452663ee4f","metadata":{},"outputs":[],"source":["# Step 1: Data Preparation\n","We start by define the sentences and text for translation training\n","Sentence Pairs: Defines a small dataset of English-Spanish sentence pairs.\n","Target Sequences:\n","Prepends \"startseq\" and appends \"endseq\" to each target sentence for the decoder to learn when to start and stop translating.\n"]},{"cell_type":"code","id":"1f9987df-4835-40b1-874c-49d58afbcb43","metadata":{},"outputs":[],"source":["# Sample parallel sentences (English -> Spanish)\ninput_texts = [\n    \"Hello.\", \"How are you?\", \"I am learning machine translation.\", \"What is your name?\", \"I love programming.\"\n]\ntarget_texts = [\n    \"Hola.\", \"¿Cómo estás?\", \"Estoy aprendiendo traducción automática.\", \"¿Cuál es tu nombre?\", \"Me encanta programar.\"\n]\n\ntarget_texts = [\"startseq \" + x + \" endseq\" for x in target_texts]"]},{"cell_type":"markdown","id":"ba970cc5-057d-4472-91da-0581acb3073a","metadata":{},"outputs":[],"source":["## Next, we convert the text from the sentences to tokens and create a vocabulary\n","Tokenization: Uses Tokenizer to convert words into numerical sequences.\n"]},{"cell_type":"code","id":"269bb486-3321-4b6f-836a-47970734768e","metadata":{},"outputs":[],"source":["# Tokenization\ninput_tokenizer = Tokenizer()\ninput_tokenizer.fit_on_texts(input_texts)\ninput_sequences = input_tokenizer.texts_to_sequences(input_texts)\n\noutput_tokenizer = Tokenizer()\noutput_tokenizer.fit_on_texts(target_texts)\noutput_sequences = output_tokenizer.texts_to_sequences(target_texts)\n\ninput_vocab_size = len(input_tokenizer.word_index) + 1\noutput_vocab_size = len(output_tokenizer.word_index) + 1"]},{"cell_type":"markdown","id":"c5cc8bd3-03b4-4e0c-85a0-6380416c8dff","metadata":{},"outputs":[],"source":["### Now pad the corresponding sentences\n","Padding: Ensures all sequences have the same length.\n"]},{"cell_type":"code","id":"9916ada6-5530-483e-abb3-adb4eba04e9f","metadata":{},"outputs":[],"source":["# Padding\nmax_input_length = max([len(seq) for seq in input_sequences])\nmax_output_length = max([len(seq) for seq in output_sequences])\n\ninput_sequences = pad_sequences(input_sequences, maxlen=max_input_length, padding='post')\noutput_sequences = pad_sequences(output_sequences, maxlen=max_output_length, padding='post')"]},{"cell_type":"code","id":"5fabe671-6387-43ef-a302-d33b5bea8f5d","metadata":{},"outputs":[],"source":["# Prepare the target data for training\ndecoder_input_data = output_sequences[:, :-1]\ndecoder_output_data = output_sequences[:, 1:]\n\n# Convert to one-hot\ndecoder_output_data = np.array([np.eye(output_vocab_size)[seq] for seq in decoder_output_data])"]},{"cell_type":"markdown","id":"47695b1e-ee64-4329-a7ab-8f203c1e753f","metadata":{},"outputs":[],"source":["# Step 2: Self-Attention Layer\n","Self-attention is a mechanism that allows a model to **focus on relevant parts of the input sequence** while processing each word. This is particularly useful in:\n","1) Machine Translation (e.g., aligning words correctly)\n","2) Text Summarization\n","3) Speech Recognition\n","4) Image Processing (Vision Transformers)\n","In this implementation, self-attention is used for text based sequence-to-sequence modeling.\n","\n","\n","Self-Attention works for a given an input sequence by computing a weighted representation of all words for each position. It does so using three key components:\n","\n","1. Query **(Q)**, Key **(K)**, and Value **(V)** Matrices\n","For each word (token) in a sequence:\n","\n","Query (Q): What this word is looking for.\n","Key (K): What this word represents.\n","Value (V): The actual information in the word.\n","\n","2. Compute **Attention Scores**\n","Next, we **calculate the similarity between each query and key** using dot-product attention:\n","Each word in a sequence attends to every other word based on these scores.\n","\n","3. Apply **Scaling & Softmax**\n","Since dot-product values can be large, we scale them. \n","Next, Applying softmax converts scores into attention weights:\n"]},{"cell_type":"markdown","id":"0ec06747-e423-4c08-8b9e-b5814f8c415d","metadata":{},"outputs":[],"source":["### Self-Attention class\n","In this implementation of self-attention layer:\n","1. We first initialize the weights in the **build** method, where:\n","    1. **self.Wq**, **self.Wk**, **self.Wv** are the trainable weight matrices.\n","    2. Their **shape is (feature_dim, feature_dim)**, meaning they transform input features into Q, K, and V representations.\n","2. Applying Attention using **call** method. The **call()** method:\n","   1. Computes **Q, K, V** by multiplying inputs (encoder/decoder output) with their respective weight matrices.\n","   2. Computes **dot-product attention scores** using K.batch_dot(q, k, axes=[2, 2]), resulting in a (batch_size, seq_len, seq_len) matrix.\n","   3. **Scales** the scores to avoid large values.\n","   4. Applies **softmax** to normalize the attention scores.\n","   5. **Multiplies attention weights with V** to get the final output.\n","3. The **compute_output_shape** method defines the shape of the output tensor after the layer processes an input.\n","    1. The output shape of the Self-Attention layer **remains the same** as the input shape.\n","    2. The attention mechanism **transforms** the input but does not change its dimensions.4\n","    3. If the attention layer changed the shape, you would modify compute_output_shape\n"]},{"cell_type":"code","id":"4287d3f5-3688-487a-99c6-3ff80fccc5f3","metadata":{},"outputs":[],"source":["# Define the Self-Attention Layer\nclass SelfAttention(Layer):\n    def __init__(self, **kwargs):\n        super(SelfAttention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        feature_dim = input_shape[-1]\n        # Weight matrices for Q, K, V\n        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n                                  initializer='glorot_uniform', \n                                  trainable=True, \n                                  name='Wq')\n        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n                                  initializer='glorot_uniform', \n                                  trainable=True, \n                                  name='Wk')\n        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n                                  initializer='glorot_uniform', \n                                  trainable=True, \n                                  name='Wv')\n        super(SelfAttention, self).build(input_shape)\n\n    def call(self, inputs):\n        # Linear projections\n        q = K.dot(inputs, self.Wq)  # Query\n        k = K.dot(inputs, self.Wk)  # Key\n        v = K.dot(inputs, self.Wv)  # Value\n\n        # Scaled Dot-Product Attention\n        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n\n        # Weighted sum of values\n        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n        return output\n\n    def compute_output_shape(self, input_shape):\n        return input_shape"]},{"cell_type":"markdown","id":"9c607fc3-20c4-4a95-8c1a-8176907f6321","metadata":{},"outputs":[],"source":["# Step 3: Model Architecture\n","The model follows an Encoder-Decoder structure:\n","\n","### Encoder:\n","1) Takes input sentences (padded and tokenized).\n","2) Uses an Embedding layer (word representations) + LSTM (to process sequences).\n","    1. The LSTMs are used as the **help process variable-length input sentences** and generate meaningful translations.\n","4) Outputs context vectors (hidden & cell states).\n","\n","### Attention Layer\n","1) Applied to both the encoder and decoder outputs.\n","2) Helps the decoder focus on relevant words during translation.\n","\n","### Decoder\n","1) Receives target sequences (shifted one step ahead).\n","2) Uses an LSTM with encoder states as initial states.\n","3) Applies self-attention for better learning.\n","4) Uses a Dense layer (Softmax) to predict the next word.\n"]},{"cell_type":"code","id":"e98f7b7c-c78c-476a-9dd7-bba63b88da89","metadata":{},"outputs":[],"source":["# Encoder\nencoder_inputs = Input(shape=(max_input_length,))\nencoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\nencoder_lstm = LSTM(256, return_sequences=True, return_state=True)\nencoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\nencoder_states = [state_h, state_c]\n\n# Attention Mechanism\nattention_layer = SelfAttention()(encoder_outputs)\n\n# Decoder\ndecoder_inputs = Input(shape=(max_output_length - 1,))\ndecoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\ndecoder_lstm = LSTM(256, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_attention = SelfAttention()(decoder_outputs)  # Apply attention\ndecoder_dense = Dense(output_vocab_size, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_attention)\n\n# Full Model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary\nmodel.summary()\n"]},{"cell_type":"markdown","id":"fefd6545-aa2c-41cd-b328-7cbd253f434a","metadata":{},"outputs":[],"source":["# Step 4: Training the Model\n","Uses categorical_crossentropy as the loss function since output words are one-hot encoded.\n","Trains using Adam optimizer for 100 epochs.\n"]},{"cell_type":"code","id":"6c27926c-a3a7-485e-a48d-9ec32089cc9d","metadata":{},"outputs":[],"source":["# Step 6: Train the Model\nhistory_glorot_adam = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)"]},{"cell_type":"markdown","id":"5f0d231b-0aa1-419f-94a9-129e25b78127","metadata":{},"outputs":[],"source":["# Step 5: Plotting the training loss\n"]},{"cell_type":"code","id":"8c07b1df-03e8-4f9e-a612-6d8d11c23118","metadata":{},"outputs":[],"source":["# Plotting training loss\nimport matplotlib.pyplot as plt\nplt.plot(history_glorot_adam.history['loss'])\nplt.title('Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()"]},{"cell_type":"markdown","id":"14bde3ff-2fc8-40d8-8027-b1661eac4af2","metadata":{},"outputs":[],"source":["# Awesome, now you have succesfully trained a transformers model.\n","### Now let's try some practice excercises\n"]},{"cell_type":"markdown","id":"f0855d65-2545-4dc8-863a-c7368e2239ac","metadata":{},"outputs":[],"source":["## Practice excercise 1\n"]},{"cell_type":"markdown","id":"6230c08f-6453-4ad7-a145-5a5fb3d1e5a2","metadata":{},"outputs":[],"source":["In this practice exercise, let's train the model using \"he_uniform\" initializer instead of \"glorot_uniform\". Then, compare the training loss between model using \"glorot_uniform\" vs \"he_uniform\" initializers by plotting them using matplotlib\n"]},{"cell_type":"code","id":"360eb9d3-d42d-4fc8-bdf2-f2da26080a3f","metadata":{},"outputs":[],"source":["## Write your answer here\n\n"]},{"cell_type":"markdown","id":"452df71e-fc86-4cbb-90e5-6063ef79dcb7","metadata":{},"outputs":[],"source":["Double-click <b>here</b> for the solution.\n","\n","<!-- Your answer is below:\n","\n","\n","#Define the Self-Attention Layer\n","class SelfAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SelfAttention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        feature_dim = input_shape[-1]\n","        # Weight matrices for Q, K, V\n","        self.Wq = self.add_weight(shape=(feature_dim, feature_dim), \n","                                  initializer='he_uniform', \n","                                  trainable=True, \n","                                  name='Wq')\n","        self.Wk = self.add_weight(shape=(feature_dim, feature_dim), \n","                                  initializer='he_uniform', \n","                                  trainable=True, \n","                                  name='Wk')\n","        self.Wv = self.add_weight(shape=(feature_dim, feature_dim), \n","                                  initializer='he_uniform', \n","                                  trainable=True, \n","                                  name='Wv')\n","        super(SelfAttention, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        # Linear projections\n","        q = K.dot(inputs, self.Wq)  # Query\n","        k = K.dot(inputs, self.Wk)  # Key\n","        v = K.dot(inputs, self.Wv)  # Value\n","\n","        # Scaled Dot-Product Attention\n","        scores = K.batch_dot(q, k, axes=[2, 2])  # (batch, seq_len, seq_len)\n","        scores = scores / K.sqrt(K.cast(K.shape(k)[-1], dtype=K.floatx()))  # Scale\n","        attention_weights = K.softmax(scores, axis=-1)  # Normalize\n","\n","        # Weighted sum of values\n","        output = K.batch_dot(attention_weights, v)  # (batch, seq_len, feature_dim)\n","        return output\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape\n","\n","    \n","#Encoder\n","encoder_inputs = Input(shape=(max_input_length,))\n","encoder_embedding = Embedding(input_vocab_size, 256)(encoder_inputs)\n","encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n","encoder_states = [state_h, state_c]\n","\n","#Attention Mechanism\n","attention_layer = SelfAttention()(encoder_outputs)\n","\n","#Decoder\n","decoder_inputs = Input(shape=(max_output_length - 1,))\n","decoder_embedding = Embedding(output_vocab_size, 256)(decoder_inputs)\n","decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n","decoder_attention = SelfAttention()(decoder_outputs)  # Apply attention\n","decoder_dense = Dense(output_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_attention)\n","\n","#Full Model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","#Step 6: Train the Model\n","history_he = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n","\n","#Plotting training losses for glorot_uniform and he_uniform inititalizers\n","import matplotlib.pyplot as plt\n","plt.plot(history_glorot_adam.history['loss'], label=\"glorot_uniform\", color='red')\n","plt.plot(history_he.history['loss'], label=\"he_uniform\", color='blue')\n","plt.title('Training Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","-->\n"]},{"cell_type":"markdown","id":"9ced5c66-b585-4b26-910f-5dc6c9088cb7","metadata":{},"outputs":[],"source":["## Practice excercise 2\n"]},{"cell_type":"markdown","id":"56ff7782-3ec9-4696-9e75-359f5828ffe0","metadata":{},"outputs":[],"source":["In this practice exercise, try to use adaptive gradient optimizer instead of adam. Then, plot and compare the results between adam and adaptive gradient optimizers\n"]},{"cell_type":"code","id":"9d166af3-696c-439f-8ef1-80198b85a1b2","metadata":{},"outputs":[],"source":["### Write your answer here\n\n"]},{"cell_type":"markdown","id":"32728ab2-b9c9-4388-97f4-be73d3116ec5","metadata":{},"outputs":[],"source":["Double-click <b>here</b> for the solution.\n","\n","<!-- Your answer is below:\n","\n","#Full Model\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","model.compile(optimizer='adagrad', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","#Step 6: Train the Model\n","history_adagrad = model.fit([input_sequences, decoder_input_data], decoder_output_data, epochs=100, batch_size=16)\n","\n","#Plotting training losses for glorot_uniform and he_uniform inititalizers\n","import matplotlib.pyplot as plt\n","plt.plot(history_glorot_adam.history['loss'], label=\"adam\", color='red')\n","plt.plot(history_adagrad.history['loss'], label=\"adagrad\", color='blue')\n","plt.title('Training Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","-->\n"]},{"cell_type":"markdown","id":"a9c98e8f-2410-4cd3-8f79-9516122f15fd","metadata":{},"outputs":[],"source":["## Thank you for completing this lab!\n","\n","This notebook was created by [Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman/). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"]},{"cell_type":"markdown","id":"97135693-2e44-4428-b464-258bbaa399a9","metadata":{},"outputs":[],"source":["<!--\n","## Change Log\n","\n","|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n","|---|---|---|---|\n","| 2024-11-20  | 1.0  | Aman  |  Created the lab |\n","<hr>\n","-->\n","## <h3 align=\"center\"> © IBM Corporation. All rights reserved. <h3/>\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"89fa9a3db18ab099ea8b241e966f29a2f658cfbd6a742128f10daea40c67df82"},"nbformat":4,"nbformat_minor":4}