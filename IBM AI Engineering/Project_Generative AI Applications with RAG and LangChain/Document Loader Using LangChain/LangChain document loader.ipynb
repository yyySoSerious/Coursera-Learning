{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Documents Using LangChain for Different Sources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **20** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine you are working as a data scientist at a consulting firm, and you've been tasked with analyzing documents from multiple clients. Each client provides their data in different formats: some in PDFs, others in Word documents, CSV files, or even HTML webpages. Manually loading and parsing each document type is not only time-consuming but also prone to errors. Your goal is to streamline this process, making it efficient and error-free.\n",
    "\n",
    "To achieve this, you'll use LangChain’s powerful document loaders. These loaders allow you to read and convert various file formats into a unified document structure that can be easily processed. For example, you'll load client policy documents from text files, financial reports from PDFs, marketing strategies from Word documents, and product reviews from JSON files. By the end of this lab, you will have a robust pipeline that can handle any new file formats clients might send, saving you valuable time and effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Hvf-jk8b5Fs-E_E4AJyEow/loader.png\" width=\"50%\" alt=\"indexing\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will explore how to use various loaders provided by LangChain to load and process data from different file formats. These loaders simplify the task of reading and converting files into a document format that can be processed downstream. By the end of this lab, you will be able to efficiently load text, PDF, Markdown, JSON, CSV, DOCX, and other file formats into a unified format, allowing for seamless data analysis and manipulation for LLM applications.\n",
    "\n",
    "(Note: In this lab, we just introduced several commonly used file format loaders. LangChain provides more document loaders for various document formats [here](https://python.langchain.com/v0.2/docs/integrations/document_loaders/).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Load-from-TXT-files\">Load from TXT files</a></li>\n",
    "    <li><a href=\"#Load-from-PDF-files\">Load from PDF files</a></li>\n",
    "    <li><a href=\"#Load-from-Markdown-files\">Load from Markdown files</a></li>\n",
    "    <li><a href=\"#Load-from-JSON-files\">Load from JSON files</a></li>\n",
    "    <li><a href=\"#Load-from-CSV-files\">Load from CSV files</a></li>\n",
    "    <li><a href=\"#Load-from-URL/Website-files\">Load from URL/Webpage files</a></li>\n",
    "    <li><a href=\"#Load-from-WORD-files\">Load from WORD files</a></li>\n",
    "    <li><a href=\"#Load-from-Unstructured-Files\">Load from Unstructured Files</a></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"#Exercises\">Exercises</a>\n",
    "<ol>\n",
    "    <li><a href=\"#Exercise-1---Try-to-use-other-PDF-loaders\">Exercise 1 - Try to use other PDF loaders</a></li>\n",
    "    <li><a href=\"#Exercise-2---Load-from-Arxiv\">Exercise 2 - Load from Arxiv</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Understand how to use `TextLoader` to load text files.\n",
    " - Learn how to load PDFs using `PyPDFLoader` and `PyMuPDFLoader`.\n",
    " - Use `UnstructuredMarkdownLoader` to load Markdown files.\n",
    " - Load JSON files with `JSONLoader` using jq schemas.\n",
    " - Process CSV files with `CSVLoader` and `UnstructuredCSVLoader`.\n",
    " - Load Webpage content using `WebBaseLoader`.\n",
    " - Load Word documents using `Docx2txtLoader`.\n",
    " - Utilize `UnstructuredFileLoader` for various file types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "The following required libraries are __not__ preinstalled in the Skills Network Labs environment. __You must run the following cell__ to install them:\n",
    "\n",
    "**Note:** We are pinning the version here to specify the version. It's recommended that you do this as well. Even if the library is updated in the future, the installed library could still support this lab work.\n",
    "\n",
    "This might take approximately 1 minute. \n",
    "\n",
    "As we use `%%capture` to capture the installation, you won't see the output process. But after the installation completes, you will see a number beside the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "#After executing the cell,please RESTART the kernel and run all the cells.\n",
    "!pip install --user \"langchain-community==0.2.1\"\n",
    "!pip install --user \"pypdf==4.2.0\"\n",
    "!pip install --user \"PyMuPDF==1.24.5\"\n",
    "!pip install --user \"unstructured==0.14.8\"\n",
    "!pip install --user \"markdown==3.6\"\n",
    "!pip install --user  \"jq==1.7.0\"\n",
    "!pip install --user \"pandas==2.2.2\"\n",
    "!pip install --user \"docx2txt==0.8\"\n",
    "!pip install --user \"requests==2.32.3\"\n",
    "!pip install --user \"beautifulsoup4==4.12.3\"\n",
    "!pip install --user \"nltk==3.8.0\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you install the libraries, restart your kernel. You can do that by clicking the **Restart the kernel** icon.\n",
    "\n",
    "<p style=\"text-align:left\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/1_Bd_EvpEzLH9BbxRXXUGQ/screenshot-to-replace.png\" width=\"50%\"/>\n",
    "    </a>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also use this section to suppress warnings generated by your code:\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "import json\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders.csv_loader import UnstructuredCSVLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from TXT files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TextLoader` is a tool designed to load textual data from various sources.\n",
    "\n",
    "It is the simplest loader, reading a file as text and placing all the content into a single document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a .txt file for you to load. First, we need to download it from a remote source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 12:57:15--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 6363 (6.2K) [text/plain]\n",
      "Saving to: ‘new-Policies.txt’\n",
      "\n",
      "new-Policies.txt    100%[===================>]   6.21K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 12:57:15 (614 MB/s) - ‘new-Policies.txt’ saved [6363/6363]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Ec5f3KYU1CpbKRp1whFLZw/new-Policies.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the `TextLoader` class to load the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7effcb8f31a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"new-Policies.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `load` method to load the data as documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's present the entire data (document) here.\n",
    "\n",
    "This is a `document` object that includes `page_content` and `metadata` attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\\n\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the `pprint` function to print the first 1000 characters of the `page_content` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1. Code of Conduct\\n'\n",
      " '\\n'\n",
      " 'Our Code of Conduct establishes the core values and ethical standards that '\n",
      " 'all members of our organization must adhere to. We are committed to '\n",
      " 'fostering a workplace characterized by integrity, respect, and '\n",
      " 'accountability.\\n'\n",
      " '\\n'\n",
      " 'Integrity: We commit to the highest ethical standards by being honest and '\n",
      " 'transparent in all our dealings, whether with colleagues, clients, or the '\n",
      " 'community. We protect sensitive information and avoid conflicts of '\n",
      " 'interest.\\n'\n",
      " '\\n'\n",
      " \"Respect: We value diversity and every individual's contribution. \"\n",
      " 'Discrimination, harassment, or any form of disrespect is not tolerated. We '\n",
      " 'promote an inclusive environment where differences are respected, and '\n",
      " 'everyone is treated with dignity.\\n'\n",
      " '\\n'\n",
      " 'Accountability: We are responsible for our actions and decisions, complying '\n",
      " 'with all relevant laws and regulations. We aim for continuous improvement '\n",
      " 'and report any breaches of this code, supporting investigations into such '\n",
      " 'matters.\\n'\n",
      " '\\n'\n",
      " 'Safety: We prioritize the safety of our employees, c')\n"
     ]
    }
   ],
   "source": [
    "pprint(data[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from PDF files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we may have files in PDF format that we want to load for processing.\n",
    "\n",
    "LangChain provides several classes for loading PDFs. Here, we introduce two classes: `PyPDFLoader` and `PyMuPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the PDF using `PyPDFLoader` into an array of documents, where each document contains the page content and metadata with the page number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(pdf_url)\n",
    "\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first page of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the first three pages of the PDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page number 1\n",
      "page_content='LAB: L ARGE -SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1 I NTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article }) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1arXiv:2403.01081v3  [cs.CL]  29 Apr 2024' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0}\n",
      "page number 2\n",
      "page_content='training captures enough of the distribution of language and knowledge, such that a small amount of\n",
      "supervised training can “unlock” or shape latent abilities related to the ultimate desired instruction-\n",
      "following behavior of the model. However, unlike the unstructured data that is abundantly available\n",
      "in the public domain, high-quality, human-generated task-specific instruction data is costly to pro-\n",
      "cure, even via crowd-sourcing, and human-generated instruction data is typically closely guarded\n",
      "by model builders, even for ostensibly “open” model-building efforts. In this work, we address\n",
      "the challenges associated with scaling of the alignment-tuning phase and propose a new method\n",
      "called LAB: Large-scale Alignment for chatBots. The LAB method consists of two components:\n",
      "(i) a taxonomy-guided synthetic data generation method and quality assurance process that yields a\n",
      "highly diverse and high-quality instruction dataset, without resorting to the use of proprietary LLMs\n",
      "like GPT-4 or substantial human curation, and (ii) a novel multi-phase training framework and un-\n",
      "conventional tuning regime that allows for adding new knowledge and instruction-following abilities\n",
      "into pre-trained LLMs without suffering from catastrophic forgetting. Our findings show that LAB-\n",
      "trained models can perform competitively with proprietary and open-source models that use human\n",
      "annotations and/or synthetic data generated using GPT-4 on a number of benchmarks.\n",
      "2 R ELATED WORK\n",
      "Existing methods for instruction tuning typically either rely on humans for generating high-quality\n",
      "datasets, or use synthetic data generation using a large teacher model. OpenAI (Ouyang et al.,\n",
      "2022) arguably set the standard for model alignment from human data, employing human annota-\n",
      "tors to gather data for supervised fine tuning (SFT) and reinforcement learning with human feed-\n",
      "back (RLHF) training. Collecting human-generated data for these steps is complex undertaking; the\n",
      "selection of annotators requires a rigorous multi-stage screening process aimed at achieving high\n",
      "inter-annotator agreement, and collecting even modest amounts data (by LLM standards) requires\n",
      "the coordination of large groups of annotators. The creators of the LLaMA 2 model series (Touvron\n",
      "et al., 2023) followed a similar recipe, collecting tens of thousands of human-generated instruction\n",
      "samples, and approximately 1 million human-annotated binary comparisons for reward modeling.\n",
      "Not only are such approaches expensive and time consuming, but they can also potentially limit\n",
      "agility in exploring the space of instructions and capabilities the model is trained to perform. Alter-\n",
      "natives to this approach, such as transforming existing human datasets into instructions via templat-\n",
      "ing (Wei et al.) can be more cost effective, but face limitations in the naturalness and length of the\n",
      "responses used for training.\n",
      "More recently, training with synthetic data generated from LLMs has emerged as an alternative to\n",
      "purely human-data-based approaches. Wang et al. (2023) introduced Self-Instruct, which leverages\n",
      "a small number of handwritten human seed instructions as input to bootstrapping process to generate\n",
      "a large number of samples using an LLM’s own generation abilities. Taori et al. (2023) built upon\n",
      "Self-Instruct, using a larger teacher model to generate synthetic data to train a smaller student model,\n",
      "and incorporating principles in the generation prompt to promote diversity in the generated instruc-\n",
      "tion data. Xu et al. (2023) introduces Evol-Instruct, another variant of Self-Instruct, that synthesizes\n",
      "iteratively more complex instruction to overcome shortcomings of previous methods. Mukherjee\n",
      "et al. (2023), Mitra et al. (2023) present a synthetic data generation approach to enhance task di-\n",
      "versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n",
      "page number 3\n",
      "page_content='versity and scalability, alongside a progressive training framework aimed at improving the model’s\n",
      "reasoning ability and response style to match teacher models. This is achieved by generating rich\n",
      "reasoning signals in the generated answer and progressively training on datasets of varying difficulty\n",
      "in incremental phases.\n",
      "Similar to LAB, concurrent work, GLAN (Li et al., 2024), employs a semi-automatic approach to\n",
      "synthetic data generation that uses a human-curated taxonomy to generate instruction tuning data\n",
      "from a teacher model. However, as explained in section 3.2.2, unlike LAB, GLAN cannot be used\n",
      "to generate synthetic data from domains that are not captured in the teacher model’s support. As\n",
      "such, while LAB uses the open-source Mixtral model as the teacher, like many other synthetic\n",
      "data generation approaches, GLAN has to rely on a large proprietary model (GPT-4). This poses\n",
      "complicated questions about the usability of generated data (especially for commercial purposes)\n",
      "since the terms of use of proprietary models typically forbid using the model to improve other\n",
      "models.\n",
      "2' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "for p,page in enumerate(pages[0:3]):\n",
    "    print(f\"page number {p+1}\")\n",
    "    print(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyMuPDFLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyMuPDFLoader` is the fastest of the PDF parsing options. It provides detailed metadata about the PDF and its pages, and returns one document per page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.pdf.PyMuPDFLoader at 0x7effe2fef9b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyMuPDFLoader(pdf_url)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LAB: LARGE-SCALE ALIGNMENT FOR CHATBOTS\n",
      "MIT-IBM Watson AI Lab and IBM Research\n",
      "Shivchander Sudalairaj∗\n",
      "Abhishek Bhandwaldar∗\n",
      "Aldo Pareja∗\n",
      "Kai Xu\n",
      "David D. Cox\n",
      "Akash Srivastava∗,†\n",
      "*Equal Contribution, †Corresponding Author\n",
      "ABSTRACT\n",
      "This work introduces LAB (Large-scale Alignment for chatBots), a novel method-\n",
      "ology designed to overcome the scalability challenges in the instruction-tuning\n",
      "phase of large language model (LLM) training. Leveraging a taxonomy-guided\n",
      "synthetic data generation process and a multi-phase tuning framework, LAB sig-\n",
      "nificantly reduces reliance on expensive human annotations and proprietary mod-\n",
      "els like GPT-4. We demonstrate that LAB-trained models can achieve compet-\n",
      "itive performance across several benchmarks compared to models trained with\n",
      "traditional human-annotated or GPT-4 generated synthetic data. Thus offering a\n",
      "scalable, cost-effective solution for enhancing LLM capabilities and instruction-\n",
      "following behaviors without the drawbacks of catastrophic forgetting, marking a\n",
      "step forward in the efficient training of LLMs for a wide range of applications.\n",
      "1\n",
      "INTRODUCTION\n",
      "Large language models (LLMs) have achieved remarkable levels of success in various natural lan-\n",
      "guage processing (NLP) applications, including question-answering , entity extraction , and sum-\n",
      "marization . This has been made possible, in large part, by the introduction of the transformer\n",
      "architecture , which can leverage large amounts of unlabeled, unstructured data, enabling the scal-\n",
      "ing of LLMs to billions, or even trillions of parameters. LLMs are typically trained in phases: a\n",
      "self-supervised pre-training phase, followed by supervised alignment tuning phases.\n",
      "The majority of the cost of training an LLM comes from the pre-training phase. During this phase, a\n",
      "model is trained in an auto-regressive manner to predict the next token in the target language using\n",
      "trillions of tokens worth of unlabeled data, requiring thousands of GPUs training for months at a\n",
      "time. Alignment tuning, typically happens in two stages: instruction tuning, followed by prefer-\n",
      "ence tuning. Instruction tuning is more akin to the traditional model training approach in machine\n",
      "learning, where the model is trained directly on tasks of interest. In this stage, the model is given a\n",
      "task description in the form of an natural language instuction (e.g. Summarize the following news\n",
      "article in 2 lines: {News article}) and the model is trained to maximize the likelihood of the pro-\n",
      "vided ground truth summary. Preference tuning, on the other hand, is done using techniques such\n",
      "as RLHF (Stiennon et al., 2022; Ouyang et al., 2022) and DPO (Rafailov et al., 2023), where the\n",
      "response from an instruction-tuned model is rated as preferred or unpreferred using human feedback.\n",
      "In comparison to pre-training, the instruction tuning and preference tuning stages comprise a small\n",
      "fraction of the overall training procedure, both in terms of the data used as well as the compute\n",
      "infrastructure required to train models Touvron et al. (2023). For example, Meta’s LLaMA 2 models\n",
      "were trained with just tens of thousands of high quality human-generated instruction/response data\n",
      "pairs, followed by multiple rounds of RLHF with a comparatively limited number of examples as\n",
      "compared to pretraining data volumes Touvron et al. (2023). From a traditional machine learning\n",
      "training perspective, this imbalance in the scale across the phases is unconventional—typically one\n",
      "would expect a model to perform best when it has been trained directly on the desired tasks, using as\n",
      "much data as possible. The deviation from the tradtional LLM approach relies on the idea that pre-\n",
      "1\n",
      "arXiv:2403.01081v3  [cs.CL]  29 Apr 2024\n",
      "' metadata={'source': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'file_path': 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/Q81D33CdRLK6LswuQrANQQ/instructlab.pdf', 'page': 0, 'total_pages': 10, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240501000524Z', 'modDate': 'D:20240501000524Z', 'trapped': ''}\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `metadata` attribute reveals that `PyMuPDFLoader` provides more detailed metadata information than `PyPDFLoader`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Markdown files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, our file source might be in Markdown format.\n",
    "\n",
    "LangChain provides the `UnstructuredMarkdownLoader` to load content from Markdown files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 13:07:50--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 3398 (3.3K) [text/markdown]\n",
      "Saving to: ‘markdown-sample.md’\n",
      "\n",
      "markdown-sample.md  100%[===================>]   3.32K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 13:07:50 (450 MB/s) - ‘markdown-sample.md’ saved [3398/3398]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/eMSP5vJjj9yOfAacLZRWsg/markdown-sample.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader at 0x7effe3511310>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_path = \"markdown-sample.md\"\n",
    "loader = UnstructuredMarkdownLoader(markdown_path)\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from JSON files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSONLoader uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files. It uses the jq python package, which we've installed before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 13:13:24--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 2167 (2.1K) [application/json]\n",
      "Saving to: ‘facebook-chat.json’\n",
      "\n",
      "facebook-chat.json  100%[===================>]   2.12K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 13:13:24 (373 MB/s) - ‘facebook-chat.json’ saved [2167/2167]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hAmzVJeOUAMHzmhUHNdAUg/facebook-chat.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's use `pprint` to take a look at the JSON file and its structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='facebook-chat.json'\n",
    "data = json.loads(Path(file_path).read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': {'creation_timestamp': 1675549016, 'uri': 'image_of_the_chat.jpg'},\n",
      " 'is_still_participant': True,\n",
      " 'joinable_mode': {'link': '', 'mode': 1},\n",
      " 'magic_words': [],\n",
      " 'messages': [{'content': 'Bye!',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675597571851},\n",
      "              {'content': 'Oh no worries! Bye',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675597435669},\n",
      "              {'content': 'No Im sorry it was my mistake, the blue one is not '\n",
      "                          'for sale',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675596277579},\n",
      "              {'content': 'I thought you were selling the blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595140251},\n",
      "              {'content': 'Im not interested in this bag. Im interested in the '\n",
      "                          'blue one!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675595109305},\n",
      "              {'content': 'Here is $129',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595068468},\n",
      "              {'photos': [{'creation_timestamp': 1675595059,\n",
      "                           'uri': 'url_of_some_picture.jpg'}],\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595060730},\n",
      "              {'content': 'Online is at least $100',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675595045152},\n",
      "              {'content': 'How much do you want?',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675594799696},\n",
      "              {'content': 'Goodmorning! $50 is too low.',\n",
      "               'sender_name': 'User 2',\n",
      "               'timestamp_ms': 1675577876645},\n",
      "              {'content': 'Hi! Im interested in your bag. Im offering $50. Let '\n",
      "                          'me know if you are interested. Thanks!',\n",
      "               'sender_name': 'User 1',\n",
      "               'timestamp_ms': 1675549022673}],\n",
      " 'participants': [{'name': 'User 1'}, {'name': 'User 2'}],\n",
      " 'thread_path': 'inbox/User 1 and User 2 chat',\n",
      " 'title': 'User 1 and User 2 chat'}\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `JSONLoader` to load data from the JSON file. However, JSON files can have various attribute-value pairs. If we want to load a specific attribute and its value, we need to set an appropriate `jq schema`.\n",
    "\n",
    "So for example, if we want to load the `content` from the JSON file, we need to set `jq_schema='.messages[].content'`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.messages[].content',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 1}, page_content='Bye!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 2}, page_content='Oh no worries! Bye'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 3}, page_content='No Im sorry it was my mistake, the blue one is not for sale'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 4}, page_content='I thought you were selling the blue one!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 5}, page_content='Im not interested in this bag. Im interested in the blue one!'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 6}, page_content='Here is $129'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 7}, page_content=''),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 8}, page_content='Online is at least $100'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 9}, page_content='How much do you want?'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 10}, page_content='Goodmorning! $50 is too low.'),\n",
      " Document(metadata={'source': '/resources/AI0214EN/facebook-chat.json', 'seq_num': 11}, page_content='Hi! Im interested in your bag. Im offering $50. Let me know if you are interested. Thanks!')]\n"
     ]
    }
   ],
   "source": [
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from CSV files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are a common format for storing tabular data. The `CSVLoader` provides a convenient way to read and process this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 13:30:45--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 848 [text/csv]\n",
      "Saving to: ‘mlb-teams-2012.csv’\n",
      "\n",
      "mlb-teams-2012.csv  100%[===================>]     848  --.-KB/s    in 0s      \n",
      "\n",
      "2025-05-22 13:30:46 (83.8 MB/s) - ‘mlb-teams-2012.csv’ saved [848/848]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IygVG_j0M87BM4Z0zFsBMA/mlb-teams-2012.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='mlb-teams-2012.csv')\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 0}, page_content='Team: Nationals\\n\"Payroll (millions)\": 81.34\\n\"Wins\": 98'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 1}, page_content='Team: Reds\\n\"Payroll (millions)\": 82.20\\n\"Wins\": 97'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 2}, page_content='Team: Yankees\\n\"Payroll (millions)\": 197.96\\n\"Wins\": 95'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 3}, page_content='Team: Giants\\n\"Payroll (millions)\": 117.62\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 4}, page_content='Team: Braves\\n\"Payroll (millions)\": 83.31\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 5}, page_content='Team: Athletics\\n\"Payroll (millions)\": 55.37\\n\"Wins\": 94'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 6}, page_content='Team: Rangers\\n\"Payroll (millions)\": 120.51\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 7}, page_content='Team: Orioles\\n\"Payroll (millions)\": 81.43\\n\"Wins\": 93'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 8}, page_content='Team: Rays\\n\"Payroll (millions)\": 64.17\\n\"Wins\": 90'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 9}, page_content='Team: Angels\\n\"Payroll (millions)\": 154.49\\n\"Wins\": 89'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 10}, page_content='Team: Tigers\\n\"Payroll (millions)\": 132.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 11}, page_content='Team: Cardinals\\n\"Payroll (millions)\": 110.30\\n\"Wins\": 88'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 12}, page_content='Team: Dodgers\\n\"Payroll (millions)\": 95.14\\n\"Wins\": 86'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 13}, page_content='Team: White Sox\\n\"Payroll (millions)\": 96.92\\n\"Wins\": 85'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 14}, page_content='Team: Brewers\\n\"Payroll (millions)\": 97.65\\n\"Wins\": 83'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 15}, page_content='Team: Phillies\\n\"Payroll (millions)\": 174.54\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 16}, page_content='Team: Diamondbacks\\n\"Payroll (millions)\": 74.28\\n\"Wins\": 81'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 17}, page_content='Team: Pirates\\n\"Payroll (millions)\": 63.43\\n\"Wins\": 79'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 18}, page_content='Team: Padres\\n\"Payroll (millions)\": 55.24\\n\"Wins\": 76'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 19}, page_content='Team: Mariners\\n\"Payroll (millions)\": 81.97\\n\"Wins\": 75'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 20}, page_content='Team: Mets\\n\"Payroll (millions)\": 93.35\\n\"Wins\": 74'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 21}, page_content='Team: Blue Jays\\n\"Payroll (millions)\": 75.48\\n\"Wins\": 73'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 22}, page_content='Team: Royals\\n\"Payroll (millions)\": 60.91\\n\"Wins\": 72'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 23}, page_content='Team: Marlins\\n\"Payroll (millions)\": 118.07\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 24}, page_content='Team: Red Sox\\n\"Payroll (millions)\": 173.18\\n\"Wins\": 69'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 25}, page_content='Team: Indians\\n\"Payroll (millions)\": 78.43\\n\"Wins\": 68'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 26}, page_content='Team: Twins\\n\"Payroll (millions)\": 94.08\\n\"Wins\": 66'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 27}, page_content='Team: Rockies\\n\"Payroll (millions)\": 78.06\\n\"Wins\": 64'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 28}, page_content='Team: Cubs\\n\"Payroll (millions)\": 88.19\\n\"Wins\": 61'),\n",
       " Document(metadata={'source': 'mlb-teams-2012.csv', 'row': 29}, page_content='Team: Astros\\n\"Payroll (millions)\": 60.65\\n\"Wins\": 55')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you load data from a CSV file, the loader typically creates a separate `Document` object for each row of data in the CSV.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnstructuredCSVLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to `CSVLoader`, which treats each row as an individual document with headers defining the data, `UnstructuredCSVLoader` considers the entire CSV file as a single unstructured table element. This approach is beneficial when you want to analyze the data as a complete table rather than as separate entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredCSVLoader(\n",
    "    file_path=\"mlb-teams-2012.csv\", mode=\"elements\"\n",
    ")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nTeam\\n\"Payroll (millions)\"\\n\"Wins\"\\n\\n\\nNationals\\n81.34\\n98\\n\\n\\nReds\\n82.20\\n97\\n\\n\\nYankees\\n197.96\\n95\\n\\n\\nGiants\\n117.62\\n94\\n\\n\\nBraves\\n83.31\\n94\\n\\n\\nAthletics\\n55.37\\n94\\n\\n\\nRangers\\n120.51\\n93\\n\\n\\nOrioles\\n81.43\\n93\\n\\n\\nRays\\n64.17\\n90\\n\\n\\nAngels\\n154.49\\n89\\n\\n\\nTigers\\n132.30\\n88\\n\\n\\nCardinals\\n110.30\\n88\\n\\n\\nDodgers\\n95.14\\n86\\n\\n\\nWhite Sox\\n96.92\\n85\\n\\n\\nBrewers\\n97.65\\n83\\n\\n\\nPhillies\\n174.54\\n81\\n\\n\\nDiamondbacks\\n74.28\\n81\\n\\n\\nPirates\\n63.43\\n79\\n\\n\\nPadres\\n55.24\\n76\\n\\n\\nMariners\\n81.97\\n75\\n\\n\\nMets\\n93.35\\n74\\n\\n\\nBlue Jays\\n75.48\\n73\\n\\n\\nRoyals\\n60.91\\n72\\n\\n\\nMarlins\\n118.07\\n69\\n\\n\\nRed Sox\\n173.18\\n69\\n\\n\\nIndians\\n78.43\\n68\\n\\n\\nTwins\\n94.08\\n66\\n\\n\\nRockies\\n78.06\\n64\\n\\n\\nCubs\\n88.19\\n61\\n\\n\\nAstros\\n60.65\\n55\\n\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table border=\"1\" class=\"dataframe\">\n",
      "  <tbody>\n",
      "    <tr>\n",
      "      <td>Team</td>\n",
      "      <td>\"Payroll (millions)\"</td>\n",
      "      <td>\"Wins\"</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Nationals</td>\n",
      "      <td>81.34</td>\n",
      "      <td>98</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Reds</td>\n",
      "      <td>82.20</td>\n",
      "      <td>97</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Yankees</td>\n",
      "      <td>197.96</td>\n",
      "      <td>95</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Giants</td>\n",
      "      <td>117.62</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Braves</td>\n",
      "      <td>83.31</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Athletics</td>\n",
      "      <td>55.37</td>\n",
      "      <td>94</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rangers</td>\n",
      "      <td>120.51</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Orioles</td>\n",
      "      <td>81.43</td>\n",
      "      <td>93</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rays</td>\n",
      "      <td>64.17</td>\n",
      "      <td>90</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Angels</td>\n",
      "      <td>154.49</td>\n",
      "      <td>89</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Tigers</td>\n",
      "      <td>132.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cardinals</td>\n",
      "      <td>110.30</td>\n",
      "      <td>88</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Dodgers</td>\n",
      "      <td>95.14</td>\n",
      "      <td>86</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>White Sox</td>\n",
      "      <td>96.92</td>\n",
      "      <td>85</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Brewers</td>\n",
      "      <td>97.65</td>\n",
      "      <td>83</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Phillies</td>\n",
      "      <td>174.54</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Diamondbacks</td>\n",
      "      <td>74.28</td>\n",
      "      <td>81</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Pirates</td>\n",
      "      <td>63.43</td>\n",
      "      <td>79</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Padres</td>\n",
      "      <td>55.24</td>\n",
      "      <td>76</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mariners</td>\n",
      "      <td>81.97</td>\n",
      "      <td>75</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Mets</td>\n",
      "      <td>93.35</td>\n",
      "      <td>74</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Blue Jays</td>\n",
      "      <td>75.48</td>\n",
      "      <td>73</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Royals</td>\n",
      "      <td>60.91</td>\n",
      "      <td>72</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Marlins</td>\n",
      "      <td>118.07</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Red Sox</td>\n",
      "      <td>173.18</td>\n",
      "      <td>69</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Indians</td>\n",
      "      <td>78.43</td>\n",
      "      <td>68</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Twins</td>\n",
      "      <td>94.08</td>\n",
      "      <td>66</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Rockies</td>\n",
      "      <td>78.06</td>\n",
      "      <td>64</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Cubs</td>\n",
      "      <td>88.19</td>\n",
      "      <td>61</td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "      <td>Astros</td>\n",
      "      <td>60.65</td>\n",
      "      <td>55</td>\n",
      "    </tr>\n",
      "  </tbody>\n",
      "</table>\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata[\"text_as_html\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from URL/Website files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use `BeautifulSoup` package to load and parse a HTML or XML file. But it has some limitations.\n",
    "\n",
    "The following code is using `BeautifulSoup` to parse a website. Let's see what limitation it has.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.ibm.com/topics/langchain'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the print output, we can see that `BeautifulSoup` not only load the web content, but also a lot of HTML tags and external links, which are not necessary if we just want to load the text content of the web.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So LangChain's `WebBaseLoader` can effectively address this limitation.\n",
    "\n",
    "`WebBaseLoader` is designed to extract all text from HTML webpages and convert it into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from single web page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://www.ibm.com/topics/langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Agents\\n\\n\\n\\nWelcome\\n\\n\\n\\n\\n\\nCaret right\\n\\nIntroduction\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agents vs AI assistants\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n\\nAgentic AI vs generative AI\\n\\n\\n\\n\\nTypes of AI agents\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nComponents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nPerception\\n\\n\\n\\n\\nReasoning\\n\\n\\n\\n\\nMemory\\n\\n\\n\\n\\nPlanning\\n\\n\\n\\n\\n\\nCaret right\\n\\nTool calling\\n\\n\\n\\n\\nWhat is tool calling?\\n\\n\\n\\n\\nTutorial: Ollama tool calling\\n\\n\\n\\n\\n\\n\\nCommunication\\n\\n\\n\\n\\nLearning\\n\\n\\n\\n\\nAgentic workflows\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nArchitecture\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nReAct agents\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent orchestration\\n\\n\\n\\n\\nWhat is agent orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMultiagent systems\\n\\n\\n\\n\\nWhat are multiagent systems?\\n\\n\\n\\n\\nTutorial: crewAI multiagent call analysis\\n\\n\\n\\n\\n\\n\\nModel Context Protocol (MCP)\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nFrameworks\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangGraph\\n\\n\\n\\n\\nWhat is LangGraph?\\n\\n\\n\\n\\nTutorial: LangGraph IT support agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangChain\\n\\n\\n\\n\\nWhat is LangChain?\\n\\n\\n\\n\\nTutorial: LangChain agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\ncrewAI\\n\\n\\n\\n\\nWhat is crewAI?\\n\\n\\n\\n\\nTutorial: crewAI retail shelf optimization\\n\\n\\n\\n\\n\\n\\nAutoGPT\\n\\n\\n\\n\\nChatDev\\n\\n\\n\\n\\nMetaGPT\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nGovernance\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agent ethics\\n\\n\\n\\n\\nAI agent evaluation\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic RAG\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nTutorial: LangChain agentic RAG \\n\\n\\n\\n\\nTutorial: Agentic chunking\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nUse cases / Applications\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nCustomer service\\n\\n\\n\\n\\nHuman resources\\n\\n\\n\\n\\nSales\\n\\n\\n\\n\\nProcurement\\n\\n\\n\\n\\nAgentic automation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Authors\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDave Bergmann\\n\\nSenior Writer, AI Models\\nIBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCole Stryker\\n\\nEditorial Lead, AI Models\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents.\\u202f\\n\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI\\xa0(genAI) more accessible to enthusiasts and startups in the wake of its widespread popularity. Advancements in accessibility for agentic AI are currently enabling a revolution in automation.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even AI\\xa0agents capable of robotic process automation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntegrations with LLMs\\n\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of artificial intelligence products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIndustry newsletter\\n\\n\\n\\nThe latest AI trends, brought to you by experts\\n\\n\\n\\nGet curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.\\n\\n\\n\\n\\nThank you! You are subscribed.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\n\\nImporting language models\\n\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like Meta AI’s LLaMa, Deepseek's Deepseek-LLM, IBM's Granite and Google’s Flan-T5, can be accessed through Hugging Face. IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\n\\nPrompt templates\\n\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is\\xa0LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define\\xa0chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call\\xa0chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications\\xa0(link resides outside ibm.com). This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\nWith LangChain, we can also build agentic RAG systems.\\xa0In traditional RAG applications, the LLM is provided with a vector database to reference when forming its responses. In contrast, agentic AI applications are not restricted to only data retrieval. Agenic RAG can also encompass tools for tasks such as solving mathematical calculations, writing emails, performing data analysis and more.\\n\\n\\nMemory\\n\\n\\nLLMs, by default, do not have any long-term memory of previous interactions (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent pre-built LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\r\\n        LangChain agents\\r\\n    \\n\\n\\n\\nWe can build an\\xa0agent\\xa0with the LangChain framework to give an LLM the ability to make decisions, use tools and complete complex tasks step-by-step, rather than just generating a single text response. Unlike a simple prompt-response interaction with just an LLM, an agent powered by LangChain can think, plan, execute a sequence of actions, learn and adapt.\\nLangChain provides a streamlined user experience with a ready-made, extensible framework for creating AI agents, so there’s no need to build new tool selection logic, reasoning loops (such as for ReAct agents), observation/action tracking or prompt orchestration and formatting.\\nThe specific LangChain packages, classes and methods vary depending on the AI platform you intend to use. Some key components of the WatsonxLLM class that allow for communication with watsonx.ai models using LangChain include:\\nlangchain_ibm: The package responsible for the LangChain IBM integration. It is necessary to install this package to use any of the following classes and methods.ibm_watsonx_ai: The library that allows connection to watsonx.ai services like IBM Cloud and IBM Cloud Pak for Data.APIClient: The main class of the ibm_watsonx_ai\\xa0library that manages the API service resources. The parameters include the API credentials and endpoint.WatsonxLLM: The wrapper for IBM watsonx.ai foundation models. This wrapper provides chain integration and is necessary to import. The parameters include the model ID, watsonx.ai API key, URL endpoint, project ID as well as any LLM parameters.ModelInference: The class that instantiates the model interface. The parameters include the model ID, watsonx.ai credentials, project ID, model parameters and more. Once instantiated, the model can then be passed into the class.invoke: The method that calls the model directly with a single prompt of string type.\\xa0generate:\\xa0The method that calls the model with multiple prompts of string type in a list.\\nAnother LangChain class for building AI agents with the integration of tool calling and chaining with\\xa0watsonx.ai models is ChatWatsonx. This class, which is leveraged in many of our tutorials, uses the bind_tools method to pass a list of tools to the LLM upon each iteration. These can include both custom and pre-built tools. To retrieve the AI agent response, the invoke method\\xa0can be used. Once the agent is invoked, the tool_calls attribute\\xa0of the response displays the name, arguments, id and type of each tool call made, if any.\\n\\n\\nLangGraph\\n\\n\\nLangGraph, created by LangChain,\\xa0is an open source AI agent framework that supports multi-agent orchestration and enables developers to build\\xa0agentic workflows\\xa0where different agents interact, specialize and collaborate.\\xa0\\nAt its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an\\xa0AI agent workflow.\\xa0Combined with the human-in-the-loop monitoring mechanism and a set of API and tool integrations, LangGraph provides users with a versatile platform for developing AI solutions and workflows including\\xa0chatbots, state graphs and\\xa0other agent-based systems.\\xa0\\nWith the langchain-mcp-adapters library, LangGraph agents can also use tools defined on model context protocol (MCP) servers. The mcp library allows users to build custom MCP servers as well. Essentially, MCP enables a secure connection between an AI system, such as an AI agent, and external tools. Thus, various LLMs can connect to the same tools and data sources given the standard MCP.\\xa0\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\xa0\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. The use of LangSmith is not limited to applications built using the LangChain ecosystem. The evaluation of agent performance is done using LLM-as-a-judge evaluators. This observability\\xa0and these key metrics aim to optimize more robust, cost-efficient applications.\\xa0\\n\\n\\n\\n\\r\\n        Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github.\\nLangChain can also be installed on Python with a simple pip command: pip install langchain. To install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are provided by IBM including LangChain tool calling, agentic RAG, LLM agent orchestration,\\xa0agentic chunking and more.\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nAI Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots\\n\\n\\nChatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\\n\\n\\nSummarization\\n\\n\\nLanguage models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\\n\\n\\nQuestion answering\\n\\n\\nUsing specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\\n\\n\\nData augmentation\\n\\n\\nLLMs can be used to generate\\xa0synthetic data\\xa0for use in\\xa0machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\\n\\n\\nVirtual agents\\n\\n\\nIntegrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                Top Strategic Technology Trends for 2025: Agentic AI\\n            \\nDownload this Gartner research to learn the potential opportunities and risks of agentic AI for IT leaders and learn how to prepare for this next wave of AI innovation.\\r\\n\\r\\n\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                AI models\\n            \\n\\n                Explore IBM Granite\\n            \\nDiscover IBM® Granite™, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Article\\n            \\n\\n                Discover the power of LLMs\\n            \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guide\\n            \\n\\n                The CEO’s guide to model optimization\\n            \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                A differentiated approach to AI foundation models\\n            \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                Unlock the Power of Generative AI and ML\\n            \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                AI in Action 2024\\n            \\nRead about 2,000 organizations we surveyed about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore Granite 3.2 and the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\nExplore watsonx.ai\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\n\\nExplore AI solutions\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\n\\nExplore AI services\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\n\\nDiscover watsonx.ai\\n\\n\\n\\n\\n\\nExplore IBM Granite AI models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Footnotes\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n1\\xa0The fastest-growing open-source startups in Q2 2023\\xa0(link resides outside ibm.com), Runa Capital, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load from multiple web pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load multiple webpages simultaneously by passing a list of URLs to the loader. This will return a list of documents corresponding to the order of the URLs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.ibm.com/topics/langchain', 'title': 'What Is LangChain? | IBM', 'description': 'LangChain is an open source orchestration framework for the development of applications using large language models (LLMs), like chatbots and virtual agents.\\u202f', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat Is LangChain? | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                    \\n\\n\\n\\n  \\n    What is LangChain?\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAI Agents\\n\\n\\n\\nWelcome\\n\\n\\n\\n\\n\\nCaret right\\n\\nIntroduction\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agents vs AI assistants\\n\\n\\n\\n\\nAgentic AI\\n\\n\\n\\n\\nAgentic AI vs generative AI\\n\\n\\n\\n\\nTypes of AI agents\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nComponents\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nPerception\\n\\n\\n\\n\\nReasoning\\n\\n\\n\\n\\nMemory\\n\\n\\n\\n\\nPlanning\\n\\n\\n\\n\\n\\nCaret right\\n\\nTool calling\\n\\n\\n\\n\\nWhat is tool calling?\\n\\n\\n\\n\\nTutorial: Ollama tool calling\\n\\n\\n\\n\\n\\n\\nCommunication\\n\\n\\n\\n\\nLearning\\n\\n\\n\\n\\nAgentic workflows\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nArchitecture\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nReAct agents\\n\\n\\n\\n\\n\\nCaret right\\n\\nAI agent orchestration\\n\\n\\n\\n\\nWhat is agent orchestration?\\n\\n\\n\\n\\nTutorial: Agent orchestration\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nMultiagent systems\\n\\n\\n\\n\\nWhat are multiagent systems?\\n\\n\\n\\n\\nTutorial: crewAI multiagent call analysis\\n\\n\\n\\n\\n\\n\\nModel Context Protocol (MCP)\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nFrameworks\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangGraph\\n\\n\\n\\n\\nWhat is LangGraph?\\n\\n\\n\\n\\nTutorial: LangGraph IT support agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nLangChain\\n\\n\\n\\n\\nWhat is LangChain?\\n\\n\\n\\n\\nTutorial: LangChain agent\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\ncrewAI\\n\\n\\n\\n\\nWhat is crewAI?\\n\\n\\n\\n\\nTutorial: crewAI retail shelf optimization\\n\\n\\n\\n\\n\\n\\nAutoGPT\\n\\n\\n\\n\\nChatDev\\n\\n\\n\\n\\nMetaGPT\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nGovernance\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nAI agent ethics\\n\\n\\n\\n\\nAI agent evaluation\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nAgentic RAG\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nTutorial: LangChain agentic RAG \\n\\n\\n\\n\\nTutorial: Agentic chunking\\n\\n\\n\\n\\n\\n\\n\\nCaret right\\n\\nUse cases / Applications\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\nCustomer service\\n\\n\\n\\n\\nHuman resources\\n\\n\\n\\n\\nSales\\n\\n\\n\\n\\nProcurement\\n\\n\\n\\n\\nAgentic automation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Authors\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDave Bergmann\\n\\nSenior Writer, AI Models\\nIBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCole Stryker\\n\\nEditorial Lead, AI Models\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        What is LangChain?\\r\\n    \\n\\n\\n\\nLangChain is an open source orchestration framework for application development using large language models (LLMs). Available in both Python- and Javascript-based libraries, LangChain’s tools and APIs simplify the process of building LLM-driven applications like chatbots and AI agents.\\u202f\\n\\n\\nLangChain serves as a generic interface for nearly any LLM, providing a centralized development environment to build LLM applications and integrate them with external data sources and software workflows. LangChain’s module-based approach allows developers and data scientists to dynamically compare different prompts and even different foundation models with minimal need to rewrite code. This modular environment also allows for programs that use multiple LLMs: for example, an application that uses one LLM to interpret user queries and another LLM to author a response.\\nLaunched by Harrison Chase in October 2022, LangChain enjoyed a meteoric rise to prominence: as of June 2023, it was the single fastest-growing open source project on Github.1 Coinciding with the momentous launch of OpenAI’s ChatGPT the following month, LangChain has played a significant role in making generative AI\\xa0(genAI) more accessible to enthusiasts and startups in the wake of its widespread popularity. Advancements in accessibility for agentic AI are currently enabling a revolution in automation.\\nLangChain can facilitate most use cases for LLMs and natural language processing (NLP), like chatbots, intelligent search, question-answering, summarization services or even AI\\xa0agents capable of robotic process automation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntegrations with LLMs\\n\\n\\nLLMs are not standalone applications: they are pre-trained statistical models that must be paired with an application (and, in some cases, specific data sources) in order to meet their purpose.\\nFor example, Chat-GPT is not an LLM: it is a chatbot application that, depending on the version you’ve chosen, uses the GPT-3.5 or GPT-4 language model. While it’s the GPT model that interprets the user’s input and composes a natural language response, it’s the application that (among other things) provides an interface for the user to type and read and a UX design that governs the chatbot experience. Even at the enterprise level, Chat-GPT is not the only application using the GPT model: Microsoft uses GPT-4 to power Bing Chat.\\nFurthermore, though foundation models (like those powering LLMs) are pre-trained on massive datasets, they are not omniscient. If a particular task requires access to specific contextual information, like internal documentation or domain expertise, LLMs must be connected to those external data sources. Even if you simply want your model to reflect real-time awareness of current events, it requires external information: a model’s internal data is only up-to-date through the time period during which it was pre-trained.\\nLikewise, if a given generative AI task requires access to external software workflows—for example, if you wanted your virtual agent to integrate with Slack—then you will need a way to integrate the LLM with the API for that software.\\nWhile these integrations can generally be achieved with fully manual code, orchestration frameworks such as LangChain and the IBM watsonx portfolio of artificial intelligence products greatly simplify the process. They also make it much easier to experiment with different LLMs to compare results, as different models can be swapped in and out with minimal changes to code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIndustry newsletter\\n\\n\\n\\nThe latest AI trends, brought to you by experts\\n\\n\\n\\nGet curated insights on the most important—and intriguing—AI news. Subscribe to our weekly Think newsletter. See the IBM Privacy Statement.\\n\\n\\n\\n\\nThank you! You are subscribed.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYour subscription will be delivered in English. You will find an unsubscribe link in every newsletter. You can manage your subscriptions or unsubscribe here. Refer to our IBM Privacy Statement for more information.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\r\\n        How does LangChain work?\\r\\n    \\n\\n\\n\\nAt LangChain’s core is a development environment that streamlines the programming of LLM applications through the use of\\xa0abstraction: the simplification of code by representing one or more complex processes as a named component that encapsulates all of its constituent steps.\\n\\n\\nAbstractions are a common element of everyday life and language. For example, “π” allows us to represent the ratio of the length of a circle’s circumference to that of its diameter without having to write out its infinite digits. Similarly, a thermostat allows us to control the temperature in our home without needing to understand the complex circuitry this entails—we only need to know how different thermostat settings translate to different temperatures.\\nLangChain is essentially a library of abstractions for Python and Javascript, representing common steps and concepts necessary to work with language models. These modular components—like functions and object classes—serve as the building blocks of generative AI programs. They can be “chained” together to create applications, minimizing the amount of code and fine understanding required to execute complex NLP tasks. Though LangChain’s abstracted approach may limit the extent to which an expert programmer can finely customize an application, it empowers specialists and newcomers alike to quickly experiment and prototype.\\n\\n\\nImporting language models\\n\\n\\nNearly any LLM can be used in LangChain. Importing language models into LangChain is easy, provided you have an API key. The LLM class is designed to provide a standard interface for all models.\\nMost LLM providers will require you to create an account in order to receive an API key. Some of these APIs—particularly those for proprietary closed-source models, like those offered by OpenAI or Anthropic—may have associated costs.\\nMany open source models, like Meta AI’s LLaMa, Deepseek's Deepseek-LLM, IBM's Granite and Google’s Flan-T5, can be accessed through Hugging Face. IBM watsonx, through its partnership with Hugging Face, also offers a curated suite of open source models. Creating an account with either service will allow you to generate an API key for any of the models offered by that provider.\\nLangChain is not limited to out-of-the-box foundation models: the CustomLLM class\\xa0allows for custom LLM wrappers. Likewise, you can use the IBM watsonx APIs and Python SDK, which includes a LangChain integration, to build applications in LangChain with models that you’ve already trained or fine-tuned for your specific needs using the WatsonxLLM class (and that model’s specific project ID).\\n\\n\\nPrompt templates\\n\\n\\nPrompts are the instructions given to an LLM. The “art” of composing prompts that effectively provide the context necessary for the LLM to interpret input and structure output in the way most useful to you is often called prompt engineering.\\nThe PromptTemplate class in LangChain formalizes the composition of prompts without the need to manually hard code context and queries. Important elements of a prompt are likewise entered as formal classes, like input_variables. A prompt template can thus contain and reproduce context, instructions (like “do not use technical terms”), a set of examples to guide its responses (in what is called “few-shot prompting”), a specified output format or a standardized question to be answered.\\u202fYou can save and name an effectively structured prompt template and easily reuse it as needed.\\nThough these elements can all be manually coded, PromptTemplate modules empower smooth integration with other LangChain features, like the eponymous chains.\\n\\n\\nChains\\n\\n\\nAs its name implies, chains are the core of LangChain’s workflows. They combine LLMs with other components, creating applications by executing a sequence of functions.\\nThe most basic chain is\\xa0LLMChain. It simply calls a model and prompt template for that model. For example, imagine you saved a prompt as “ExamplePrompt” and wanted to run it against Flan-T5. You can import LLMChain from langchain.chains, then define\\xa0chain_example = LLMChain(llm = flan-t5, prompt = ExamplePrompt). To run the chain for a given input, you simply call\\xa0chain_example.run(“input”).\\nTo use the output of one function as the input for the next function, you can use SimpleSequentialChain. Each function could utilize different prompts, different tools, different parameters or even different models, depending on your specific needs.\\n\\n\\nIndexes\\n\\n\\nTo achieve certain tasks, LLMs will need access to specific external data sources not included in its training dataset, such as internal documents, emails or datasets. LangChain collectively refers to such external documentation as “indexes”.\\n\\n\\nDocument loaders\\n\\n\\nLangChain offers\\xa0a wide variety of document loaders for third party applications\\xa0(link resides outside ibm.com). This allows for easy importation of data from sources like file storage services (like Dropbox, Google Drive and Microsoft OneDrive), web content (like YouTube, PubMed or specific URLs), collaboration tools (like Airtable, Trello, Figma and Notion), databases (like Pandas, MongoDB and Microsoft), among many others.\\n\\n\\nVector databases\\n\\n\\nUnlike “traditional” structured databases,\\xa0vector databases\\xa0represent data points by converting them into\\xa0vector embeddings: numerical representations in the form of vectors with a fixed number of dimensions, often clustering related data points using\\xa0unsupervised learning methods. This enables low latency queries, even for massive datasets, which greatly increases efficiency. Vector embeddings also store each vector’s metadata, further enhancing search possibilities.\\nLangChain provides integrations for over 25 different embedding methods, as well as for over 50 different vector stores (both cloud-hosted and local).\\n\\n\\nText splitters\\xa0\\n\\n\\nTo increase speed and reduce computational demands, it’s often wise to split large text documents into smaller pieces. LangChain’s\\xa0TextSplitters\\xa0split text up into small, semantically meaningful chunks that can then be combined using methods and parameters of your choosing.\\n\\n\\nRetrieval\\n\\n\\nOnce external sources of knowledge have been connected, the model must be able to quickly retrieve and integrate relevant information as needed. Like watsonx, LangChain offers\\xa0retrieval augmented generation (RAG):\\xa0its\\xa0retriever\\xa0modules accept a string query as an input and return a list of\\xa0Document’s as output.\\nWith LangChain, we can also build agentic RAG systems.\\xa0In traditional RAG applications, the LLM is provided with a vector database to reference when forming its responses. In contrast, agentic AI applications are not restricted to only data retrieval. Agenic RAG can also encompass tools for tasks such as solving mathematical calculations, writing emails, performing data analysis and more.\\n\\n\\nMemory\\n\\n\\nLLMs, by default, do not have any long-term memory of previous interactions (unless that chat history is used as input for a query). LangChain solves this problem with simple utilities for adding memory to a system, with options ranging from retaining the entirety of all conversations to retaining a summarization of the conversation thus far to retaining the n\\xa0most recent exchanges.\\n\\n\\nTools\\n\\n\\nDespite their heralded power and versatility, LLMs have important limitations: namely, a lack of up-to-date information, a lack of domain-specific expertise and a general difficulty with math.\\nLangChain tools\\xa0are a set of functions that empower LangChain agents to interact with real-world information in order to expand or improve the services it can provide. Examples of prominent pre-built LangChain tools include:\\n\\nWolfram Alpha: provides access to powerful computational and data visualization functions, enabling sophisticated mathematical capabilities.\\nGoogle Search: provides access to Google Search, equipping applications and agents with real-time information.\\nOpenWeatherMap: fetches weather information.\\nWikipedia: provides efficient access to information from Wikipedia articles.\\n\\n\\n\\n\\r\\n        LangChain agents\\r\\n    \\n\\n\\n\\nWe can build an\\xa0agent\\xa0with the LangChain framework to give an LLM the ability to make decisions, use tools and complete complex tasks step-by-step, rather than just generating a single text response. Unlike a simple prompt-response interaction with just an LLM, an agent powered by LangChain can think, plan, execute a sequence of actions, learn and adapt.\\nLangChain provides a streamlined user experience with a ready-made, extensible framework for creating AI agents, so there’s no need to build new tool selection logic, reasoning loops (such as for ReAct agents), observation/action tracking or prompt orchestration and formatting.\\nThe specific LangChain packages, classes and methods vary depending on the AI platform you intend to use. Some key components of the WatsonxLLM class that allow for communication with watsonx.ai models using LangChain include:\\nlangchain_ibm: The package responsible for the LangChain IBM integration. It is necessary to install this package to use any of the following classes and methods.ibm_watsonx_ai: The library that allows connection to watsonx.ai services like IBM Cloud and IBM Cloud Pak for Data.APIClient: The main class of the ibm_watsonx_ai\\xa0library that manages the API service resources. The parameters include the API credentials and endpoint.WatsonxLLM: The wrapper for IBM watsonx.ai foundation models. This wrapper provides chain integration and is necessary to import. The parameters include the model ID, watsonx.ai API key, URL endpoint, project ID as well as any LLM parameters.ModelInference: The class that instantiates the model interface. The parameters include the model ID, watsonx.ai credentials, project ID, model parameters and more. Once instantiated, the model can then be passed into the class.invoke: The method that calls the model directly with a single prompt of string type.\\xa0generate:\\xa0The method that calls the model with multiple prompts of string type in a list.\\nAnother LangChain class for building AI agents with the integration of tool calling and chaining with\\xa0watsonx.ai models is ChatWatsonx. This class, which is leveraged in many of our tutorials, uses the bind_tools method to pass a list of tools to the LLM upon each iteration. These can include both custom and pre-built tools. To retrieve the AI agent response, the invoke method\\xa0can be used. Once the agent is invoked, the tool_calls attribute\\xa0of the response displays the name, arguments, id and type of each tool call made, if any.\\n\\n\\nLangGraph\\n\\n\\nLangGraph, created by LangChain,\\xa0is an open source AI agent framework that supports multi-agent orchestration and enables developers to build\\xa0agentic workflows\\xa0where different agents interact, specialize and collaborate.\\xa0\\nAt its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an\\xa0AI agent workflow.\\xa0Combined with the human-in-the-loop monitoring mechanism and a set of API and tool integrations, LangGraph provides users with a versatile platform for developing AI solutions and workflows including\\xa0chatbots, state graphs and\\xa0other agent-based systems.\\xa0\\nWith the langchain-mcp-adapters library, LangGraph agents can also use tools defined on model context protocol (MCP) servers. The mcp library allows users to build custom MCP servers as well. Essentially, MCP enables a secure connection between an AI system, such as an AI agent, and external tools. Thus, various LLMs can connect to the same tools and data sources given the standard MCP.\\xa0\\n\\n\\n\\r\\n        LangSmith\\r\\n    \\n\\n\\n\\nReleased in the fall of 2023, LangSmith aims to bridge the gap between the accessible prototyping capabilities that brought LangChain to prominence and building production-quality LLM applications.\\xa0\\nLangSmith provides tools to monitor, evaluate and debug applications, including the ability to automatically trace all model calls to spot errors and test performance under different model configurations. The use of LangSmith is not limited to applications built using the LangChain ecosystem. The evaluation of agent performance is done using LLM-as-a-judge evaluators. This observability\\xa0and these key metrics aim to optimize more robust, cost-efficient applications.\\xa0\\n\\n\\n\\n\\r\\n        Getting started with LangChain\\r\\n    \\n\\n\\n\\nLangChain is open source and free to use: source code is\\xa0available for download on Github.\\nLangChain can also be installed on Python with a simple pip command: pip install langchain. To install all LangChain dependencies (rather than only those you find necessary), you can run the command\\xa0pip install langchain[all].\\nMany step-by-step tutorials are provided by IBM including LangChain tool calling, agentic RAG, LLM agent orchestration,\\xa0agentic chunking and more.\\n\\n\\n\\r\\n        LangChain use cases\\r\\n    \\n\\n\\n\\nAI Applications made with LangChain provide great utility for a variety of use cases, from straightforward question-answering and text generation tasks to more complex solutions that use an LLM as a “reasoning engine.”\\n\\n\\nChatbots\\n\\n\\nChatbots are among the most intuitive uses of LLMs. LangChain can be used to provide proper context for the specific use of a chatbot, and to integrate chatbots into existing communication channels and workflows with their own APIs.\\n\\n\\nSummarization\\n\\n\\nLanguage models can be tasked with summarizing many types of text, from breaking down complex academic articles and transcripts to providing a digest of incoming emails.\\n\\n\\nQuestion answering\\n\\n\\nUsing specific documents or specialized knowledge bases (like Wolfram, arXiv or PubMed), LLMs can retrieve relevant information from storage and articulate helpful answers). If fine-tuned or properly prompted, some LLMs can answer many questions even without external information.\\n\\n\\nData augmentation\\n\\n\\nLLMs can be used to generate\\xa0synthetic data\\xa0for use in\\xa0machine learning. For example, an LLM can be trained to generate additional data samples that closely resemble the data points in a training dataset.\\n\\n\\nVirtual agents\\n\\n\\nIntegrated with the right workflows, LangChain’s Agent modules can use an LLM to autonomously determine next steps and take action using robotic process automation (RPA).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                Top Strategic Technology Trends for 2025: Agentic AI\\n            \\nDownload this Gartner research to learn the potential opportunities and risks of agentic AI for IT leaders and learn how to prepare for this next wave of AI innovation.\\r\\n\\r\\n\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n\\n  \\n    Resources\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                AI models\\n            \\n\\n                Explore IBM Granite\\n            \\nDiscover IBM® Granite™, our family of open, performant and trusted AI models, tailored for business and optimized to scale your AI applications. Explore language, code, time series and guardrail options.\\n\\nMeet Granite\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                How to choose the right foundation model\\n            \\nLearn how to select the most suitable AI foundation model for your use case.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Article\\n            \\n\\n                Discover the power of LLMs\\n            \\nDive into IBM Developer articles, blogs and tutorials to deepen your knowledge of LLMs.\\n\\nExplore the articles\\n\\n\\n\\n\\n\\n\\n\\n\\n                Guide\\n            \\n\\n                The CEO’s guide to model optimization\\n            \\nLearn how to continually push teams to improve model performance and outpace the competition by using the latest AI techniques and infrastructure.\\n\\nRead the guide\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                A differentiated approach to AI foundation models\\n            \\nExplore the value of enterprise-grade foundation models that\\r\\nprovide trust, performance and cost-effective benefits to\\r\\nall industries.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n                Ebook\\n            \\n\\n                Unlock the Power of Generative AI and ML\\n            \\nLearn how to incorporate generative AI, machine learning and foundation models into your business operations for improved performance.\\n\\nRead the ebook\\n\\n\\n\\n\\n\\n\\n\\n\\n                Report\\n            \\n\\n                AI in Action 2024\\n            \\nRead about 2,000 organizations we surveyed about their AI initiatives to discover what's working, what's not and how you can get ahead.\\n\\nRead the report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\n\\n     \\n    Related solutions\\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Foundation models\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nExplore Granite 3.2 and the IBM library of foundation models in the watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\nExplore watsonx.ai\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Artificial intelligence solutions\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nPut AI to work in your business with IBM’s industry-leading AI expertise and portfolio of solutions at your side.\\n\\n\\n\\n\\nExplore AI solutions\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    AI consulting and services\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nReinvent critical workflows and operations by adding AI to maximize experiences, real-time decision-making and business value.\\n\\n\\n\\n\\nExplore AI services\\n                \\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTake the next step\\n\\n\\n\\n\\nExplore the IBM library of foundation models in the IBM watsonx portfolio to scale generative AI for your business with confidence.\\n\\n\\n\\n\\n\\n\\nDiscover watsonx.ai\\n\\n\\n\\n\\n\\nExplore IBM Granite AI models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Footnotes\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n1\\xa0The fastest-growing open-source startups in Q2 2023\\xa0(link resides outside ibm.com), Runa Capital, 2023\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://www.redhat.com/en/topics/ai/what-is-instructlab', 'title': 'What is InstructLab?', 'description': 'InstructLab is an open source project for enhancing large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to contentRed HatNavigationAIOur approachNews and insightsTechnical blogResearchvLLM office hoursExplore AI at Red HatOur portfolioRed Hat AIRed Hat Enterprise Linux AIRed Hat OpenShift AIRed Hat AI Inference ServerNewEngage & learnAI learning hubAI partnersServices for AIHybrid cloudUse casesArtificial intelligenceBuild, deploy, and monitor AI models and apps.Linux standardizationGet consistency across operating environments.Application developmentSimplify the way you build, deploy, and manage apps.AutomationScale automation and unite tech, teams, and environments.VirtualizationModernize operations for virtualized and containerized workloads.SecurityCode, build, deploy, and monitor security-focused software.Edge computingDeploy workloads closer to the source with edge technology.Explore solutionsSolutions by industryAutomotiveFinancial servicesHealthcareIndustrial sectorMedia and entertainmentPublic sectorTelecommunicationsDiscover cloud technologiesLearn how to use our cloud products and solutions at your own pace in the Red Hat® Hybrid Cloud Console. ProductsPlatformsRed Hat AIDevelop and deploy AI solutions across the hybrid cloud.Red Hat Enterprise LinuxSupport hybrid cloud innovation on a flexible operating system.New versionRed Hat OpenShiftBuild, modernize, and deploy apps at scale.Red Hat Ansible Automation PlatformImplement enterprise-wide automation.FeaturedRed Hat OpenShift Virtualization EngineRed Hat OpenShift Service on AWSMicrosoft Azure Red Hat OpenShiftSee all productsTry & buyStart a trialBuy onlineIntegrate with major cloud providersServices & supportConsultingProduct supportServices for AITechnical Account ManagementExplore servicesTrainingTraining & certificationCourses and examsCertificationsRed Hat AcademyLearning communityLearning subscriptionExplore trainingFeaturedRed Hat Certified System Administrator examRed Hat System Administration IRed Hat Learning Subscription trial (No cost)Red Hat Certified Engineer examRed Hat Certified OpenShift Administrator examServicesConsultingPartner trainingProduct supportServices for AITechnical Account ManagementLearnBuild your skillsDocumentationHands-on labsHybrid cloud learning hubInteractive learning experiencesTraining and certificationMore ways to learnBlogEvents and webinarsPodcasts and video seriesRed Hat TVResource libraryFor developersDiscover resources and tools to help you build, deliver, and manage cloud-native applications and services.PartnersFor customersOur partnersRed Hat Ecosystem CatalogFind a partnerFor partnersPartner ConnectBecome a partnerTrainingSupportAccess the partner portalBuild solutions powered by trusted partnersFind solutions from our collaborative community of experts and technologies in the Red Hat® Ecosystem Catalog.SearchI\\'d like to:Start a trialManage subscriptionsSee Red Hat jobsExplore tech topicsContact salesContact customer serviceHelp me find:DocumentationDeveloper resourcesSkills assessmentsArchitecture centerSecurity updatesSupport casesI want to learn more about:AIApplication modernizationAutomationCloud-native applicationsLinuxVirtualizationConsoleDocsSupportNew For youRecommendedWe\\'ll recommend resources you may like as you browse. Try these suggestions for now.Product trial centerCourses and examsAll productsTech topicsResource libraryLog inSign in or create an account to get more from Red Hat World-class support Training resources Product trials Console accessA subscription may be required for some services.Log in or registerSelect a language简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolContact us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                                Topics                                                            Open source                            \\n                    What is InstructLab?                     \\n\\n\\nWhat is InstructLab?\\n\\n\\n\\n\\nPublished  May 7, 2024•5-minute readCopy URL\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\nOverviewInstructLab is an open source project for enhancing\\xa0large language models (LLMs) used in\\xa0generative artificial intelligence (gen AI) applications. Created by IBM and Red Hat, the InstructLab community project provides a cost-effective solution for improving the alignment of LLMs and opens the doors for those with minimal machine learning experience to contribute.Join the InstructLab community \\nWhat does InstructLab do?LLMs serve as the foundation for generative AI use cases, like chatbots and coding assistants. These LLMs can be proprietary (such as OpenAI’s GPT models and Anthropic’s Claude models) or offer varying degrees of openness around pretraining data and usage restrictions (such as Meta’s Llama models, Mistral AI’s Mistral models, and IBM’s Granite models).AI practitioners often need to adapt a pretrained LLM to suit a particular business purpose. But there are limits to the ways you can modify an LLM:Fine-tuning an LLM to understand a specific area of knowledge or skills typically involves forking an existing open model, then running expensive, resource-intensive training.There’s no way to incorporate improvements back to the upstream project, and thus no way for models to continuously improve from community contributions.LLM refinements have typically required large amounts of human-generated data, which can be time-consuming and expensive to get.InstructLab follows an approach that punches through those limitations. It can enhance an LLM using far less human-generated information and far fewer computing resources than are typically used to retrain a model. And it makes it possible for upstream contributions to continuously make the model better.InstructLab is named after and based on IBM Research’s work on Large-scale Alignment for chatBots, abbreviated as LAB. The LAB method is described in a\\xa02024 research paper by members of the MIT-IBM Watson AI Lab and IBM Research.InstructLab is not model-specific. It can provide supplemental skills and knowledge fine-tuning to an LLM of your choice. This “tree of skills and knowledge” improves continuously from community contributions and can be applied to support regular builds of an enhanced LLM. InstructLab maintains an\\xa0enhanced version of IBM Granite. Two other lab-enhanced models released by IBM include\\xa0Labradorite, which is derived from Llama 2, and\\xa0Merlinite, which is derived from Mistral. The InstructLab project prioritizes fast iteration and intends to retrain models on a regular basis. Organizations can also use the InstructLab model alignment tools to train their own private LLMs with their own proprietary skills and knowledge.How to apply AI at the enterprise\\xa0 \\n\\nRed Hat resourcesKeep reading\\n\\n\\nHow does InstructLab work?The LAB method consists of 3 components:Taxonomy-driven data curation. Taxonomy is a set of diverse training data curated by humans as examples of new knowledge and skills for the model.Large-scale synthetic data generation. The model is then used to generate new examples based on the seed training data. Recognizing that synthetic data can vary in quality, the LAB method adds an automated step to refine the example answers, making sure they’re grounded and safe.Iterative, large-scale alignment tuning. Finally, the model is retrained based on the set of synthetic data. The LAB method includes 2 tuning phases: knowledge tuning, followed by skill tuning.The contributions of data from the community can lead to regular iterative builds of enhanced LLMs, each made better by the tree of skills generated from community contributions. \\nHow is InstructLab different from other methods of training an LLM?Let’s compare InstructLab to the other steps in creating and improving an LLM.PretrainingDuring pretraining, an LLM is trained to predict the next token using trillions of tokens of unlabeled data. This gets really expensive, sometimes requiring thousands of GPUs and months of time. Pretraining a highly capable LLM is only possible for organizations with significant resources.Alignment tuningAfter pretraining, LLMs undergo alignment tuning to make the model’s answers as accurate and useful as possible. The 1st step in alignment tuning is typically instruction tuning, in which a model is trained directly on specific tasks of interest. Next is preference tuning, which can include reinforcement learning from human feedback (RLHF). In this step, humans test the model and rate its output, noting if the model’s answers are preferred or unpreferred. An RLHF process may include multiple rounds of feedback and refinement to optimize a model.Researchers have found that the amount of feedback at this alignment tuning stage can be much smaller than the initial set of training data―tens of thousands of human annotations, compared to the trillions of tokens of data required for pretraining―and still unlock latent capabilities of the model.InstructLabThe LAB method emerged from the idea that it should be possible to realize the benefits of model alignment from an even smaller set of human-generated data. An AI model can use a handful of human examples to generate a large amount of synthetic data―then refine that list for quality―and use that high-quality synthetic data set for further tuning and training. In contrast to instruction tuning, which typically need thousands of examples of human feedback, LAB can make a model significantly better using relatively few examples provided by humans.How is InstructLab different from retrieval-augmented generation (RAG)?The short answer is InstructLab and retrieval-augmented generation (RAG) solve different problems.RAG is a cost-efficient method for supplementing an LLM with domain-specific knowledge that wasn’t part of its pretraining. RAG makes it possible for a chatbot to accurately answer questions related to a specific field or business without retraining the model. Knowledge documents are stored in a vector database, then retrieved in chunks and sent to the model as part of user queries. This is helpful for anyone who wants to add proprietary data to an LLM without giving up control of their information, or who needs an LLM to access timely information.\\xa0This is in contrast to the InstructLab method, which sources end-user contributions to support regular builds of an enhanced version of an LLM. InstructLab helps add knowledge and unlock new skills of an LLM.It’s possible to \"supercharge\" a RAG process by using the RAG technique on an InstructLab-tuned model.Learn more about RAG\\xa0 \\nWhat are the components of the InstructLab project?InstructLab is composed of several projects.TaxonomyInstructLab is driven by taxonomies, which are largely created manually and with care. InstructLab contains a taxonomy tree that lets users create models tuned with human-provided data, which is then enhanced with synthetic data generation.Command-line interface (CLI)The InstructLab CLI lets contributors test their contributions using their laptop or workstation. Community members can use the InstructLab technique to generate a low-fidelity approximation of synthetic data generation and model-instruction tuning without access to specialized hardware.Model training infrastructureFinally, there’s the process of creating the enhanced LLMs. It takes GPU-intensive infrastructure to regularly retrain models based on new contributions from the community. IBM donates and maintains the infrastructure necessary to frequently retrain the InstructLab project’s enhanced models.Dig deeper into AI infrastructure \\nDiscover Red Hat Enterprise Linux AIWhen you’re ready to bring AI to the enterprise, Red Hat® Enterprise Linux® AI brings together the Granite family of open source-licensed LLMs, InstructLab model alignment tools, a bootable image of Red Hat Enterprise Linux, enterprise-grade technical support, and model intellectual property indemnification.Red Hat Enterprise Linux is the world’s leading enterprise Linux platform, certified on hundreds of clouds and with thousands of hardware and software vendors. With the technological foundation of Linux, containers, and automation, Red Hat’s open hybrid cloud strategy gives you the flexibility to run your AI applications anywhere you need them.Red Hat Enterprise Linux AI and the InstructLab project further deliver on this vision, breaking down the cost and resource barriers to experimenting with and building AI models while providing the tools, data, and concepts needed to fuel the next wave of intelligent workloads.Explore Red Hat Enterprise Linux AI \\n\\n\\n\\n\\n\\n\\n\\n\\nHub\\n \\n\\n\\n\\n\\n\\n\\nThe official Red\\xa0Hat blog\\n\\n\\n\\nGet the latest information about our ecosystem of customers, partners, and communities.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\nAll Red\\xa0Hat product trialsOur no-cost product trials help you gain hands-on experience, prepare for a certification, or assess if a product is right for your organization.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nKeep reading\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is Istio?\\n\\n\\n\\n        Find out more about Istio, an open source service mesh that controls how microservices share data with one another.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is CentOS Stream?\\n\\n\\n\\n        CentOS Stream is a Linux® development platform where open source community members can contribute to Red\\xa0Hat® Enterprise Linux in tandem with Red\\xa0Hat developers. \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat is KVM?\\n\\n\\n\\n        Kernel-based virtual machines (KVM) are an open source virtualization technology that turns Linux into a hypervisor.\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRead the article\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOpen source resources\\n\\n\\n\\n\\n\\n\\nRelated content\\n\\n\\n\\n\\n\\n\\nBlog post\\n\\nAny model, any accelerator, any cloud: Unlocking enterprise AI with open source innovation\\n\\n\\n\\nBlog post\\n\\nFrom the now to the next: How Red Hat’s trusted platforms bridge modern and next-generation IT needs\\n\\n\\n\\nBlog post\\n\\nBeyond the hype: Real AI results driven by Red Hat platforms and partners\\n\\n\\n\\nChecklist\\n\\n4 key steps to prepare for post-quantum cryptography\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nRelated articles\\n\\n\\n\\n\\n\\n\\nSLMs vs LLMs: What are small language models?\\n\\n\\nWhat is enterprise AI?\\n\\n\\nWhat is Istio?\\n\\n\\nWhat is parameter-efficient fine-tuning (PEFT)?\\n\\n\\nWhy choose Red Hat Ansible Automation Platform as your AI foundation?\\n\\n\\nLoRA vs. QLoRA\\n\\n\\nWhat is CentOS Stream?\\n\\n\\nWhat is vLLM?\\n\\n\\nWhat is AI inference?\\n\\n\\nPredictive AI vs generative AI\\n\\n\\nWhat is agentic AI?\\n\\n\\nWhat is KVM?\\n\\n\\nWhat are Granite models? \\n\\n\\nRAG vs. fine-tuning\\n\\n\\nWhat is Podman Desktop?\\n\\n\\nUnderstanding AI in telecommunications with Red Hat\\n\\n\\nEdge solutions for real-time decision making\\n\\n\\nWhat is CentOS?\\n\\n\\nWhat are CentOS replacements?\\n\\n\\nWhat are intelligent applications?\\n\\n\\nWhat is Podman?\\n\\n\\nWhat is retrieval-augmented generation?\\n\\n\\nWhat is Helm?\\n\\n\\nWhat is Argo CD?\\n\\n\\nWhat is an AI platform?\\n\\n\\nWhat is LLMops\\n\\n\\nWhat is deep learning?\\n\\n\\nWhat are predictive analytics\\n\\n\\nAI in banking\\n\\n\\nWhat is MicroShift?\\n\\n\\nAI infrastructure explained\\n\\n\\nUnderstanding AI/ML use cases\\n\\n\\nWhat is MLOps?\\n\\n\\nWhat are large language models?\\n\\n\\nWhat are foundation models for AI?\\n\\n\\nWhat is AIOps?\\n\\n\\nHow Kubernetes can help AI/ML\\n\\n\\nWhat is generative AI?\\n\\n\\nWhat is edge AI?\\n\\n\\nOpenJDK versus Oracle JDK\\n\\n\\nWhat is Cloud Foundry?\\n\\n\\nWhat is Kubeflow?\\n\\n\\nwhat is Buildah?\\n\\n\\nAccelerate MLOps with Red Hat OpenShift\\n\\n\\nUnderstanding Ansible, Terraform, Puppet, Chef, and Salt\\n\\n\\nAnsible vs. Chef: What you need to know\\n\\n\\nAnsible vs. Salt: What you need to know\\n\\n\\nWhat is Linux?\\n\\n\\nWhat\\'s the best Linux distro for you?\\n\\n\\nWhat is machine learning?\\n\\n\\nAnsible vs. Puppet: What you need to know\\n\\n\\nRed Hat OpenShift vs. OKD\\n\\n\\nWhat is AI in healthcare? \\n\\n\\nSpring on Kubernetes with Red Hat OpenShift\\n\\n\\nWhy run Apache Kafka on Kubernetes?\\n\\n\\nWhat is Apache Kafka?\\n\\n\\nAnsible vs. Terraform, clarified\\n\\n\\nAnsible vs. Red Hat Ansible Automation Platform\\n\\n\\nWhat is Skopeo?\\n\\n\\nUsing Helm with Red Hat OpenShift\\n\\n\\nWhat is Grafana?\\n\\n\\nWhat is open source software?\\n\\n\\nOpen source vs. proprietary software in vehicles\\n\\n\\nWhat is KubeLinter?\\n\\n\\nWhat is RKT?\\n\\n\\nWhat is Kogito?\\n\\n\\nWhat was CoreOS and CoreOS container Linux\\n\\n\\nWhat is Jaeger?\\n\\n\\nWhat is open source?\\n\\n\\nWhat is Knative?\\n\\n\\nWhat is etcd?\\n\\n\\nWhat is Clair?\\n\\n\\nWhat is Docker?\\n\\n\\n\\n\\n\\n\\n\\nMore about this topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLinkedInYouTubeFacebookXProducts & portfoliosRed Hat AIRed Hat Enterprise LinuxRed Hat OpenShiftRed Hat Ansible Automation PlatformCloud servicesSee all productsToolsTraining and certificationMy accountCustomer supportDeveloper resourcesFind a partnerRed Hat Ecosystem CatalogDocumentationTry, buy, & sellProduct trial centerRed Hat StoreBuy online (Japan)ConsoleCommunicateContact salesContact customer serviceContact trainingSocial\\n    About Red Hat\\n\\n    Red Hat is an open hybrid cloud technology leader, delivering a consistent, comprehensive foundation for transformative IT and artificial intelligence (AI) applications in the enterprise. As a trusted adviser to the Fortune 500, Red Hat offers cloud, developer, Linux, automation, and application platform technologies, as well as award-winning services.\\nOur companyHow we workCustomer success storiesAnalyst relationsNewsroomOpen source commitmentsOur social impactJobsSelect a languageEnglish简体中文EnglishFrançaisDeutschItaliano日本語한국어PortuguêsEspañolRed Hat legal and privacy linksAbout Red HatJobsEventsLocationsContact Red HatRed Hat BlogInclusion at Red HatCool Stuff StoreRed Hat Summit© 2025 Red Hat, Inc.Red Hat legal and privacy linksPrivacy statementTerms of useAll policies and guidelinesDigital accessibility\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader([\"https://www.ibm.com/topics/langchain\", \"https://www.redhat.com/en/topics/ai/what-is-instructlab\"])\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from WORD files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Docx2txtLoader` is utilized to convert Word documents into a document format suitable for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-22 13:47:18--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "200 OKequest sent, awaiting response... \n",
      "Length: 1311881 (1.3M) [application/vnd.openxmlformats-officedocument.wordprocessingml.document]\n",
      "Saving to: ‘file-sample.docx’\n",
      "\n",
      "file-sample.docx    100%[===================>]   1.25M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-05-22 13:47:19 (60.5 MB/s) - ‘file-sample.docx’ saved [1311881/1311881]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/94hiHUNLZdb0bLMkrCh79g/file-sample.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Docx2txtLoader(\"file-sample.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'file-sample.docx'}, page_content='Demonstration of DOCX support in calibre\\n\\nThis document demonstrates the ability of the calibre DOCX Input plugin to convert the various typographic features in a Microsoft Word (2007 and newer) document. Convert this document to a modern ebook format, such as AZW3 for Kindles or EPUB for other ebook readers, to see it in action.\\n\\nThere is support for images, tables, lists, footnotes, endnotes, links, dropcaps and various types of text and paragraph level formatting.\\n\\nTo see the DOCX conversion in action, simply add this file to calibre using the “Add Books” button and then click “Convert”.  Set the output format in the top right corner of the conversion dialog to EPUB or AZW3 and click “OK”.\\n\\n\\n\\nText Formatting\\n\\nInline formatting\\n\\nHere, we demonstrate various types of inline text formatting and the use of embedded fonts.\\n\\nHere is some bold, italic, bold-italic, underlined and struck out  text. Then, we have a superscript and a subscript. Now we see some red, green and blue text. Some text with a yellow highlight. Some text in a box. Some text in inverse video.\\n\\nA paragraph with styled text: subtle emphasis  followed by strong text and intense emphasis. This paragraph uses document wide styles for styling rather than inline text properties as demonstrated in the previous paragraph — calibre can handle both with equal ease.\\n\\nFun with fonts\\n\\nThis document has embedded the Ubuntu font family. The body text is in the Ubuntu typeface, here is some text in the Ubuntu Mono typeface, notice how every letter has the same width, even i and m. Every embedded font will automatically be embedded in the output ebook during conversion. \\n\\nParagraph level formatting\\n\\nYou can do crazy things with paragraphs, if the urge strikes you. For instance this paragraph is right aligned and has a right border. It has also been given a light gray background.\\n\\nFor the lovers of poetry amongst you, paragraphs with hanging indents, like this often come in handy. You can use hanging indents to ensure that a line of poetry retains its individual identity as a line even when the screen is  too narrow to display it as a single line. Not only does this paragraph have a hanging indent, it is also has an extra top margin, setting it apart from the preceding paragraph.\\n\\nTables\\n\\nITEM\\n\\nNEEDED\\n\\nBooks\\n\\n1\\n\\nPens\\n\\n3\\n\\nPencils\\n\\n2\\n\\nHighlighter\\n\\n2 colors\\n\\nScissors\\n\\n1 pair\\n\\nTables in Word can vary from the extremely simple to the extremely complex. calibre tries to do its best when converting tables. While you may run into trouble with the occasional table, the vast majority of common cases should be converted very well, as demonstrated in this section. Note that for optimum results, when creating tables in Word, you should set their widths using percentages, rather than absolute units.  To the left of this paragraph is a floating two column table with a nice green border and header row.\\n\\nNow let’s look at a fancier table—one with alternating row colors and partial borders. This table is stretched out to take 100% of the available width.\\n\\nCity or Town\\n\\nPoint A\\n\\nPoint B\\n\\nPoint C\\n\\nPoint D\\n\\nPoint E\\n\\nPoint A\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPoint B\\n\\n87\\n\\n—\\n\\n\\n\\n\\n\\n\\n\\nPoint C\\n\\n64\\n\\n56\\n\\n—\\n\\n\\n\\n\\n\\nPoint D\\n\\n37\\n\\n32\\n\\n91\\n\\n—\\n\\n\\n\\nPoint E\\n\\n93\\n\\n35\\n\\n54\\n\\n43\\n\\n—\\n\\n\\n\\nNext, we see a table with special formatting in various locations. Notice how the formatting for the header row and sub header rows is preserved.\\n\\nCollege\\n\\nNew students\\n\\nGraduating students\\n\\nChange\\n\\n\\n\\nUndergraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n110\\n\\n103\\n\\n+7\\n\\nOak Institute\\n\\n202\\n\\n210\\n\\n-8\\n\\n\\n\\nGraduate\\n\\n\\n\\n\\n\\nCedar University\\n\\n24\\n\\n20\\n\\n+4\\n\\nElm College\\n\\n43\\n\\n53\\n\\n-10\\n\\nTotal\\n\\n998\\n\\n908\\n\\n90\\n\\nSource: Fictitious data, for illustration purposes only\\n\\nNext, we have something a little more complex, a nested table, i.e. a table inside another table. Additionally, the inner table has some of its cells merged. The table is displayed horizontally centered.\\n\\nOne\\n\\nThree\\n\\nTwo\\n\\n\\n\\nFour\\n\\n\\n\\nTo the left is a table inside a table, with some cells merged.\\n\\n\\n\\nWe end with a fancy calendar, note how much of the original formatting is preserved. Note that this table will only display correctly on relatively wide screens. In general, very wide tables or tables whose cells have fixed width requirements don’t fare well in ebooks.\\n\\nDecember 2007\\n\\nSun\\n\\n\\n\\nMon\\n\\n\\n\\nTue\\n\\n\\n\\nWed\\n\\n\\n\\nThu\\n\\n\\n\\nFri\\n\\n\\n\\nSat\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\n\\n\\n\\n3\\n\\n\\n\\n4\\n\\n\\n\\n5\\n\\n\\n\\n6\\n\\n\\n\\n7\\n\\n\\n\\n8\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n9\\n\\n\\n\\n10\\n\\n\\n\\n11\\n\\n\\n\\n12\\n\\n\\n\\n13\\n\\n\\n\\n14\\n\\n\\n\\n15\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n16\\n\\n\\n\\n17\\n\\n\\n\\n18\\n\\n\\n\\n19\\n\\n\\n\\n20\\n\\n\\n\\n21\\n\\n\\n\\n22\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n23\\n\\n\\n\\n24\\n\\n\\n\\n25\\n\\n\\n\\n26\\n\\n\\n\\n27\\n\\n\\n\\n28\\n\\n\\n\\n29\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n30\\n\\n\\n\\n31\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStructural Elements\\n\\nMiscellaneous structural elements you can add to your document, like footnotes, endnotes, dropcaps and the like. \\n\\nFootnotes & Endnotes\\n\\nFootnotes and endnotes are automatically recognized and both are converted to endnotes, with backlinks for maximum ease of use in ebook devices.\\n\\nDropcaps\\n\\nD\\n\\nrop caps are used to emphasize the leading paragraph at the start of a section. In Word it is possible to specify how many lines of text a drop-cap should use. Because of limitations in ebook technology, this is not possible when converting.  Instead, the converted drop cap will use font size and line height to simulate the effect as well as possible. While not as good as the original, the result is usually tolerable. This paragraph has a “D” dropcap set to occupy three lines of text with a font size of 58.5 pts. Depending on the screen width and capabilities of the device you view the book on, this dropcap can look anything from perfect to ugly.\\n\\nLinks\\n\\nTwo kinds of links are possible, those that refer to an external website and those that refer to locations inside the document itself. Both are supported by calibre. For example, here is a link pointing to the calibre download page. Then we have a link that points back to the section on paragraph level formatting in this document.\\n\\nTable of Contents\\n\\nThere are two approaches that calibre takes when generating a Table of Contents. The first is if the Word document has a Table of Contents itself. Provided that the Table of Contents uses hyperlinks, calibre will automatically use it. The levels of the Table of Contents are identified by their left indent, so if you want the ebook to have a multi-level Table of Contents, make sure you create a properly indented Table of Contents in Word.\\n\\nIf no Table of Contents is found in the document, then a table of contents is automatically generated from the headings in the document. A heading is identified as something that has the Heading 1 or Heading 2, etc. style applied to it. These headings are turned into a Table of Contents with Heading 1 being the topmost level, Heading 2 the second level and so on.\\n\\n You can see the Table of Contents created by calibre by clicking the Table of Contents button in whatever viewer you are using to view the converted ebook. \\n\\n\\tDemonstration of DOCX support in calibre\\t1\\n\\n\\tText Formatting\\t2\\n\\n\\tInline formatting\\t2\\n\\n\\tFun with fonts\\t2\\n\\n\\tParagraph level formatting\\t2\\n\\n\\tTables\\t3\\n\\n\\tStructural Elements\\t5\\n\\n\\tFootnotes & Endnotes\\t5\\n\\n\\tDropcaps\\t5\\n\\n\\tLinks\\t5\\n\\n\\tTable of Contents\\t5\\n\\n\\tImages\\t7\\n\\n\\tLists\\t8\\n\\n\\tBulleted List\\t8\\n\\n\\tNumbered List\\t8\\n\\n\\tMulti-level Lists\\t8\\n\\n\\tContinued Lists\\t8\\n\\n\\n\\n\\n\\nImages\\n\\nImages can be of three main types. Inline images are images that are part of the normal text flow, like this image of a green dot . Inline images do not cause breaks in the text and are usually small in size. The next category of image is a floating image, one that “floats “ on the page and is surrounded by text. Word supports more types of floating images than are possible with current ebook technology, so the conversion maps floating images to simple left and right floats, as you can see with the left and right arrow images on the sides of this paragraph.\\n\\nThe final type of image is a “block” image, one that becomes a paragraph on its own and has no text on either side. Below is a centered green dot.\\n\\nCentered images like this are useful for large pictures that should be a focus of attention. \\n\\nGenerally, it is not possible to translate the exact positioning of images from a Word document to an ebook. That is because in Word, image positioning is specified in absolute units from the page boundaries.  There is no analogous technology in ebooks, so the conversion will usually end up placing the image either centered or floating close to the point in the text where it was inserted, not necessarily where it appears on the page in Word.\\n\\nLists\\n\\nAll types of lists are supported by the conversion, with the exception of lists that use fancy bullets, these get converted to regular bullets.\\n\\nBulleted List\\n\\nOne\\n\\nTwo\\n\\nNumbered List\\n\\nOne, with a very long line to demonstrate that the hanging indent for the list is working correctly\\n\\nTwo\\n\\nMulti-level Lists\\n\\nOne\\n\\nTwo\\n\\nThree\\n\\nFour with a very long line to demonstrate that the hanging indent for the list is working correctly.\\n\\nFive\\n\\nSix\\n\\nA Multi-level list with bullets:\\n\\nOne\\n\\nTwo\\n\\nThis bullet uses an image as the bullet item\\n\\nFour\\n\\nFive\\n\\nContinued Lists\\n\\nOne\\n\\nTwo\\n\\nAn interruption in our regularly scheduled listing, for this essential and very relevant public service announcement.\\n\\nWe now resume our normal programming\\n\\nFour')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from Unstructured Files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we need to load content from various text sources and formats without writing a separate loader for each one. Additionally, when a new file format emerges, we want to save time by not having to write a new loader for it. `UnstructuredFileLoader` addresses this need by supporting the loading of multiple file types. Currently, `UnstructuredFileLoader` can handle text files, PowerPoints, HTML, PDFs, images, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can load `.txt` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'new-Policies.txt'}, page_content=\"1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual's contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.\")]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"new-Policies.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can load `.md` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'markdown-sample.md'}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = UnstructuredFileLoader(\"markdown-sample.md\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple files with different formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even load a list of files with different formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\"markdown-sample.md\", \"new-Policies.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredFileLoader(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': ['markdown-sample.md', 'new-Policies.txt']}, page_content='An h1 header\\n\\nParagraphs are separated by a blank line.\\n\\n2nd paragraph. Italic, bold, and monospace. Itemized lists\\nlook like:\\n\\nthis one\\n\\nthat one\\n\\nthe other one\\n\\nNote that --- not considering the asterisk --- the actual text\\ncontent starts at 4-columns in.\\n\\nBlock quotes are\\nwritten like so.\\n\\nThey can span multiple paragraphs,\\nif you like.\\n\\nUse 3 dashes for an em-dash. Use 2 dashes for ranges (ex., \"it\\'s all\\nin chapters 12--14\"). Three dots ... will be converted to an ellipsis.\\nUnicode is supported. ☺\\n\\nAn h2 header\\n\\nHere\\'s a numbered list:\\n\\nfirst item\\n\\nsecond item\\n\\nthird item\\n\\nNote again how the actual text starts at 4 columns in (4 characters\\nfrom the left side). Here\\'s a code sample:\\n\\nAs you probably guessed, indented 4 spaces. By the way, instead of\\nindenting the block, you can use delimited blocks, if you like:\\n\\n~~~\\ndefine foobar() {\\n    print \"Welcome to flavor country!\";\\n}\\n~~~\\n\\n(which makes copying & pasting easier). You can optionally mark the\\ndelimited block for Pandoc to syntax highlight it:\\n\\n~~~python\\nimport time\\n\\nQuick, count to ten!\\n\\nfor i in range(10):\\n    # (but not too quick)\\n    time.sleep(0.5)\\n    print i\\n~~~\\n\\nAn h3 header\\n\\nNow a nested list:\\n\\nFirst, get these ingredients:\\n\\ncarrots\\ncelery\\nlentils\\n\\nBoil some water.\\n\\nDump everything in the pot and follow\\n    this algorithm:\\nfind wooden spoon\\nuncover pot\\nstir\\ncover pot\\nbalance wooden spoon precariously on pot handle\\nwait 10 minutes\\ngoto first step (or shut off burner when done)\\n\\nDo not bump wooden spoon or it will fall.\\n\\nNotice again how text always lines up on 4-space indents (including\\nthat last line which continues item 3 above).\\n\\nHere\\'s a link to a website, to a local\\ndoc, and to a section heading in the current\\ndoc. Here\\'s a footnote [^1].\\n\\n[^1]: Footnote text goes here.\\n\\nTables can look like this:\\n\\nsize  material      color\\n\\n9     leather       brown\\n10    hemp canvas   natural\\n11    glass         transparent\\n\\nTable: Shoes, their sizes, and what they\\'re made of\\n\\n(The above is the caption for the table.) Pandoc also supports\\nmulti-line tables:\\n\\nkeyword   text\\n\\nred       Sunsets, apples, and\\n          other red or reddish\\n          things.\\n\\ngreen     Leaves, grass, frogs\\n          and other things it\\'s\\n          not easy being.\\n\\nA horizontal rule follows.\\n\\nHere\\'s a definition list:\\n\\napples\\n  : Good for making applesauce.\\noranges\\n  : Citrus!\\ntomatoes\\n  : There\\'s no \"e\" in tomatoe.\\n\\nAgain, text is indented 4 spaces. (Put a blank line between each\\nterm/definition pair to spread things out more.)\\n\\nHere\\'s a \"line block\":\\n\\n| Line one\\n|   Line too\\n| Line tree\\n\\nand images can be specified like so:\\n\\nInline math equations go in like so: $\\\\omega = d\\\\phi / dt$. Display\\nmath should get its own line and be put in in double-dollarsigns:\\n\\n$$I = \\\\int \\\\rho R^{2} dV$$\\n\\nAnd note that you can backslash-escape any punctuation characters\\nwhich you wish to be displayed literally, ex.: `foo`, *bar*, etc.\\n\\n1. Code of Conduct\\n\\nOur Code of Conduct establishes the core values and ethical standards that all members of our organization must adhere to. We are committed to fostering a workplace characterized by integrity, respect, and accountability.\\n\\nIntegrity: We commit to the highest ethical standards by being honest and transparent in all our dealings, whether with colleagues, clients, or the community. We protect sensitive information and avoid conflicts of interest.\\n\\nRespect: We value diversity and every individual\\'s contribution. Discrimination, harassment, or any form of disrespect is not tolerated. We promote an inclusive environment where differences are respected, and everyone is treated with dignity.\\n\\nAccountability: We are responsible for our actions and decisions, complying with all relevant laws and regulations. We aim for continuous improvement and report any breaches of this code, supporting investigations into such matters.\\n\\nSafety: We prioritize the safety of our employees, clients, and the community. We encourage a culture of safety, including reporting any unsafe practices or conditions.\\n\\nEnvironmental Responsibility: We strive to reduce our environmental impact and promote sustainable practices.\\n\\nThis Code of Conduct is the cornerstone of our organizational culture. We expect every employee to uphold these principles and act as role models, ensuring our reputation for ethical conduct, integrity, and social responsibility.\\n\\n2. Recruitment Policy\\n\\nOur Recruitment Policy is dedicated to attracting, selecting, and integrating the most qualified and diverse candidates into our organization. The success of our company depends on the talent, skills, and commitment of our employees.\\n\\nEqual Opportunity: We are an equal opportunity employer and do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status. We actively support diversity and inclusion.\\n\\nTransparency: We maintain a transparent recruitment process. Job vacancies are advertised both internally and externally when appropriate. Job descriptions and requirements are clear and accurately reflect the role.\\n\\nSelection Criteria: We base our selection on qualifications, experience, and skills relevant to the role. Our interviews and assessments are objective, and decisions are made impartially.\\n\\nData Privacy: We are dedicated to protecting candidates\\' personal information and comply with all applicable data protection laws.\\n\\nFeedback: Candidates receive timely and constructive feedback on their applications and interview performance.\\n\\nOnboarding: New hires receive thorough onboarding to help them integrate effectively, including an overview of our culture, policies, and expectations.\\n\\nEmployee Referrals: We welcome employee referrals as they help build a strong and engaged team.\\n\\nThis policy lays the foundation for a diverse, inclusive, and talented workforce. It ensures that we hire candidates who align with our values and contribute to our success. We regularly review and update this policy to incorporate best practices in recruitment.\\n\\n3. Internet and Email Policy\\n\\nOur Internet and Email Policy ensures the responsible and secure use of these tools within our organization, recognizing their importance in daily operations and the need for compliance with security, productivity, and legal standards.\\n\\nAcceptable Use: Company-provided internet and email are primarily for job-related tasks. Limited personal use is permitted during non-work hours as long as it does not interfere with work duties.\\n\\nSecurity: Protect your login credentials and avoid sharing passwords. Be cautious with email attachments and links from unknown sources, and promptly report any unusual online activity or potential security threats.\\n\\nConfidentiality: Use email for confidential information, trade secrets, and sensitive customer data only with encryption. Be careful when discussing company matters on public platforms or social media.\\n\\nHarassment and Inappropriate Content: Internet and email must not be used for harassment, discrimination, or the distribution of offensive content. Always communicate respectfully and sensitively online.\\n\\nCompliance: Adhere to all relevant laws and regulations concerning internet and email use, including copyright and data protection laws.\\n\\nMonitoring: The company reserves the right to monitor internet and email usage for security and compliance purposes.\\n\\nConsequences: Violations of this policy may lead to disciplinary action, including potential termination.\\n\\nThis policy promotes the safe and responsible use of digital communication tools in line with our values and legal obligations. Employees must understand and comply with this policy. Regular reviews will ensure it remains relevant with changing technology and security standards.\\n\\n4. Mobile Phone Policy\\n\\nOur Mobile Phone Policy defines standards for responsible use of mobile devices within the organization to ensure alignment with company values and legal requirements.\\n\\nAcceptable Use: Mobile devices are primarily for work-related tasks. Limited personal use is allowed if it does not disrupt work responsibilities.\\n\\nSecurity: Secure your mobile device and credentials. Be cautious with app downloads and links from unknown sources, and report any security issues promptly.\\n\\nConfidentiality: Avoid sharing sensitive company information via unsecured messaging apps or emails. Exercise caution when discussing company matters in public.\\n\\nCost Management: Personal use of mobile phones should be separate from company accounts, and any personal charges on company-issued phones must be reimbursed.\\n\\nCompliance: Comply with all relevant laws and regulations concerning mobile phone usage, including data protection and privacy laws.\\n\\nLost or Stolen Devices: Immediately report any lost or stolen mobile devices to the IT department or your supervisor.\\n\\nConsequences: Non-compliance with this policy may result in disciplinary actions, including potential loss of mobile phone privileges.\\n\\nThis policy encourages the responsible use of mobile devices in line with legal and ethical standards. Employees are expected to understand and follow these guidelines. The policy is regularly reviewed to stay current with evolving technology and security best practices.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Try to use other PDF loaders\n",
    "\n",
    "There are many other PDF loaders in LangChain, for example, `PyPDFium2Loader`. Can you use this PDF loader to load the PDF and see the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdfium2\n",
      "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: pypdfium2\n",
      "Successfully installed pypdfium2-4.30.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install pypdfium2\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFium2Loader\n",
    "\n",
    "loader = PyPDFium2Loader(pdf_url)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Load from Arxiv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have paper that we want to load from Arxiv, can you load this [paper](https://arxiv.org/abs/1605.08386) using `ArxivLoader`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in /opt/conda/lib/python3.12/site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?2done\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=761c28d97378ed1077fc9820511b436751704f2bf07940fa57693f5edd8106c7\n",
      "  Stored in directory: /home/jupyterlab/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "arXiv:1605.08386v1  [math.CO]  26 May 2016\n",
      "HEAT-BATH RANDOM WALKS WITH MARKOV BASES\n",
      "CAPRICE STANLEY AND TOBIAS WINDISCH\n",
      "Abstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\n",
      "allowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\n",
      "ﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\n",
      "behaviour of heat-bath random walks on these graphs. We also state explicit conditions\n",
      "on the set of moves so that the heat-bath random walk, a generalization of the Glauber\n",
      "dynamics, is an expander in ﬁxed dimension.\n",
      "Contents\n",
      "1.\n",
      "Introduction\n",
      "1\n",
      "2.\n",
      "Graphs and statistics\n",
      "3\n",
      "3.\n",
      "Bounds on the diameter\n",
      "4\n",
      "4.\n",
      "Heat-bath random walks\n",
      "8\n",
      "5.\n",
      "Augmenting Markov bases\n",
      "14\n",
      "References\n",
      "19\n",
      "1. Introduction\n",
      "A ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\n",
      "two lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\n",
      "moves M ⊂Zd. The implicit structure of these graphs \n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for Solution</summary>\n",
    "    \n",
    "```python\n",
    "\n",
    "!pip install arxiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs = ArxivLoader(query=\"1605.08386\", load_max_docs=2).load()\n",
    "\n",
    "print(docs[0].page_content[:1000])\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kang Wang](https://www.linkedin.com/in/kangwang95/)\n",
    "\n",
    "Kang Wang is a Data Scientist in IBM. He is also a PhD Candidate in the University of Waterloo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/)\n",
    "\n",
    "Joseph has a Ph.D. in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n",
    "\n",
    "[Hailey Quach](https://author.skills.network/instructors/hailey_quach)\n",
    "\n",
    "Hailey is a Data Scientist at IBM. She is also an undergraduate student at Concordia University, Montreal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "8ccd87848ab9e79d16c766e68c2292b6bf1eff17098bb52f22c15a7b9da59990"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
